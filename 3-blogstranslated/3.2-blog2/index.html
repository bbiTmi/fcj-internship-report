<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="nguyenlamgiang2198@gmial.com"><link rel=icon href=../../images/favicon.png type=image/png><title>Blog 2 :: Internship Report</title>
<link href=../../css/nucleus.css?1765293643 rel=stylesheet><link href=../../css/fontawesome-all.min.css?1765293643 rel=stylesheet><link href=../../css/hybrid.css?1765293643 rel=stylesheet><link href=../../css/featherlight.min.css?1765293643 rel=stylesheet><link href=../../css/perfect-scrollbar.min.css?1765293643 rel=stylesheet><link href=../../css/auto-complete.css?1765293643 rel=stylesheet><link href=../../css/atom-one-dark-reasonable.css?1765293643 rel=stylesheet><link href=../../css/theme.css?1765293643 rel=stylesheet><link href=../../css/hugo-theme.css?1765293643 rel=stylesheet><link href=../../css/theme-workshop.css?1765293643 rel=stylesheet><script src=../../js/jquery-3.3.1.min.js?1765293643></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=../../3-blogstranslated/3.2-blog2/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=../../><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script src=../../js/lunr.min.js?1765293643 defer></script><script src=../../js/auto-complete.js?1765293643 defer></script><script>var baseurl="https://bbitmi.github.io/fcj-internship-report"</script><script src=../../js/search.js?1765293643 defer></script></div><div class=highlightable><ul class=topics><li data-nav-id=/1-worklog/ title=Worklog class=dd-item><a href=../../1-worklog/><b>1. </b>Worklog
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/1-worklog/1.1-week1/ title="Worklog Week 1" class=dd-item><a href=../../1-worklog/1.1-week1/><b>1.1. </b>Worklog Week 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.2-week2/ title="Worklog Week 2" class=dd-item><a href=../../1-worklog/1.2-week2/><b>1.2. </b>Worklog Week 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.3-week3/ title="Worklog Week 3" class=dd-item><a href=../../1-worklog/1.3-week3/><b>1.3. </b>Worklog Week 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.4-week4/ title="Worklog Week 4" class=dd-item><a href=../../1-worklog/1.4-week4/><b>1.4. </b>Worklog Week 4
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.5-week5/ title="Worklog Week 5" class=dd-item><a href=../../1-worklog/1.5-week5/><b>1.5. </b>Worklog Week 5
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.6-week6/ title="Worklog Week 6" class=dd-item><a href=../../1-worklog/1.6-week6/><b>1.6. </b>Worklog Week 6
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.7-week7/ title="Worklog Week 7" class=dd-item><a href=../../1-worklog/1.7-week7/><b>1.7. </b>Worklog Week 7
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.8-week8/ title="Worklog Week 8" class=dd-item><a href=../../1-worklog/1.8-week8/><b>1.8. </b>Worklog Week 8
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.9-week9/ title="Worklog Week 9" class=dd-item><a href=../../1-worklog/1.9-week9/><b>1.9. </b>Worklog Week 9
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.10-week10/ title="Worklog Week 10" class=dd-item><a href=../../1-worklog/1.10-week10/><b>1.10. </b>Worklog Week 10
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.11-week11/ title="Worklog Week 11" class=dd-item><a href=../../1-worklog/1.11-week11/><b>1.11. </b>Worklog Week 11
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week12/ title="Worklog Week 12" class=dd-item><a href=../../1-worklog/1.12-week12/><b>1.12 </b>Worklog Week 12
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week13/ title="Worklog Week 13" class=dd-item><a href=../../1-worklog/1.12-week13/><b>1.13 </b>Worklog Week 13
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/2-proposal/ title=Proposal class=dd-item><a href=../../2-proposal/><b>2. </b>Proposal
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/ title="Translated Blogs" class="dd-item
parent"><a href=../../3-blogstranslated/><b>3. </b>Translated Blogs
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/3-blogstranslated/3.1-blog1/ title="Blog 1" class=dd-item><a href=../../3-blogstranslated/3.1-blog1/><b>3.1. </b>Blog 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.2-blog2/ title="Blog 2" class="dd-item
active"><a href=../../3-blogstranslated/3.2-blog2/><b>3.2. </b>Blog 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.3-blog3/ title="Blog 3" class=dd-item><a href=../../3-blogstranslated/3.3-blog3/><b>3.3. </b>Blog 3
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/4-eventparticipated/ title="Events Participated" class=dd-item><a href=../../4-eventparticipated/><b>4. </b>Events Participated
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/4-eventparticipated/4.1-event1/ title="Event 1" class=dd-item><a href=../../4-eventparticipated/4.1-event1/><b>4.1. </b>Event 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.2-event2/ title="Event 2" class=dd-item><a href=../../4-eventparticipated/4.2-event2/><b>4.2. </b>Event 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.3-event3/ title="Event 3" class=dd-item><a href=../../4-eventparticipated/4.3-event3/><b>4.3. </b>Event 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.4-event4/ title="Event 4" class=dd-item><a href=../../4-eventparticipated/4.4-event4/><b>4.4. </b>Event 4
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.5-event5/ title="Event 5" class=dd-item><a href=../../4-eventparticipated/4.5-event5/><b>4.5. </b>Event 5
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/ title=Workshop class=dd-item><a href=../../5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.1-workshop-overview/ title="Workshop Overview" class=dd-item><a href=../../5-workshop/5.1-workshop-overview/><b>5.1. </b>Workshop Overview
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.2-prerequiste/ title=Preparation class=dd-item><a href=../../5-workshop/5.2-prerequiste/><b>5.2. </b>Preparation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.3-s3-vpc/ title="VPC & Amazon RDS" class=dd-item><a href=../../5-workshop/5.3-s3-vpc/><b>5.3. </b>VPC & Amazon RDS
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/ title="Create VPC" class=dd-item><a href=../../5-workshop/5.3-s3-vpc/5.3.1-create-gwe/><b>5.3.1. </b>Create VPC
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/ title="Create Amazon RDS" class=dd-item><a href=../../5-workshop/5.3-s3-vpc/5.3.2-test-gwe/><b>5.3.2. </b>Create Amazon RDS
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/5.4-s3-onprem/ title="Lambda & API Gateway" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/><b>5.4. </b>Lambda & API Gateway
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.1-prepare/ title="Create Lambda Functions" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.1-prepare/><b>5.4.1. </b>Create Lambda Functions
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/ title="Create API Gateway" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/><b>5.4.2. </b>Create API Gateway
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/ title="Test API Endpoints" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/><b>5.4.3. </b>Test API Endpoints
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/ title="Configure CORS & Security" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/><b>5.4.4. </b>Configure CORS & Security
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/5.5-policy/ title="S3, CloudFront & Amplify" class=dd-item><a href=../../5-workshop/5.5-policy/><b>5.5. </b>S3, CloudFront & Amplify
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.6-cleanup/ title="CI/CD, CloudWatch & Cleanup" class=dd-item><a href=../../5-workshop/5.6-cleanup/><b>5.6. </b>CI/CD, CloudWatch & Cleanup
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/6-self-evaluation/ title=Self-Assessment class=dd-item><a href=../../6-self-evaluation/><b>6. </b>Self-Assessment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/7-feedback/ title="Sharing and Feedback" class=dd-item><a href=../../7-feedback/><b>7. </b>Sharing and Feedback
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://bbitmi.github.io/fcj-internship-report/3-blogstranslated/3.2-blog2/ selected>English</option><option id=vi value=https://bbitmi.github.io/fcj-internship-report/vi/3-blogstranslated/3.2-blog2/>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate=today.toLocaleDateString("en-GB");document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=../../>Internship Report</a> > <a href=../../3-blogstranslated/>Translated Blogs</a> > Blog 2</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#key-features-that-make-eks-auto-mode-ideal-for-aiml-workloads>Key features that make EKS Auto Mode ideal for AI/ML workloads</a></li><li><a href=#walkthrough>Walkthrough</a><ul><li><a href=#prerequisites><strong>Prerequisites</strong></a></li><li><a href=#set-up-environment-variables><strong>Set up environment variables</strong></a></li><li><a href=#set-up-eks-auto-mode-cluster-and-run-a-model><strong>Set up EKS Auto Mode cluster and run a model</strong></a></li></ul></li><li><a href=#reducing-model-cold-start-time-in-ai-inference-workloads><strong>Reducing model cold start time in AI inference workloads</strong></a></li><li><a href=#conclusion><strong>Conclusion</strong></a><ul><li><a href=#about-the-authors><strong>About the authors</strong></a></li></ul></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Blog 2</h1><h1 id=how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode>How to run AI model inference with GPUs on Amazon EKS Auto Mode</h1><p>AI model inference using GPUs is becoming a core part of modern applications, powering real-time recommendations, intelligent assistants, content generation, and other latency-sensitive AI features. Kubernetes has become the orchestrator of choice for running inference workloads, and organizations want to use its capabilities while still maintaining a strong focus on rapid innovation and time-to-market. But here’s the challenge: while teams see the value of Kubernetes for its dynamic scaling and efficient resource management, they often get slowed down by the need to learn Kubernetes concepts, manage cluster configurations, and handle security updates. This shifts focus away from what matters most: deploying and optimizing AI models. That is where <a href=https://docs.aws.amazon.com/eks/latest/userguide/automode.html>Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode</a> comes in. EKS Auto Mode Automates node creation, manages <a href=https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html#addon-consider-auto>core capabilities</a>, and handles upgrades and security patching. In turn, this enables to run your inference workloads without the operational overhead.</p><p>In this post, we show you how to swiftly deploy inference workloads on EKS Auto Mode. We also demonstrate key features that streamline GPU management, show best practices for model deployment, and walk through a practical example by deploying <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html>open weight models from OpenAI</a> using <a href=https://docs.vllm.ai/en/latest/>vLLM</a>. Whether you’re building a new AI/machine learning (ML) platform or optimizing existing workflows, these patterns help you accelerate development while maintaining operational efficiency.</p><h2 id=key-features-that-make-eks-auto-mode-ideal-for-aiml-workloads>Key features that make EKS Auto Mode ideal for AI/ML workloads</h2><p>In this section, we take a closer look at the GPU-specific features that come pre-configured and ready to use with an EKS Auto Mode cluster. These capabilities are also available in self-managed Amazon EKS environments, but they typically need manual setup and tuning. However, EKS Auto Mode has them enabled and configured out of the box.</p><p>Dynamic autoscaling with Karpenter: EKS Auto Mode includes a managed version of open source conformant <a href=https://karpenter.sh/>Karpenter</a> that provisions right-sized <a href=https://aws.amazon.com/ec2/>Amazon Elastic Compute Cloud (Amazon EC2)</a> instances, such as GPU‑accelerated options, based on pod requirements. It supports just-in-time scaling and allows you to configure provisioning behavior so that you can optimize for cost, performance, or instance placement. EKS Auto Mode supports a predefined set of <a href=https://docs.aws.amazon.com/eks/latest/userguide/automode-learn-instances.html>instance types and sizes</a>, along with <a href=https://docs.aws.amazon.com/eks/latest/userguide/create-node-pool.html#auto-supported-labels>node labels and taints</a> for scheduling control.</p><p>Automatic GPU failure handling: EKS Auto Mode includes <a href=https://aws.amazon.com/blogs/containers/amazon-eks-introduces-node-monitoring-and-auto-repair-capabilities/>Node Monitoring Agent (NMA) and Node Auto Repair</a>, which detect GPU failures and initiate automated recovery 10 minutes after detection. The repair process cordons the affected node and either reboots or replaces it, while respecting Pod Disruption Budgets. GPU telemetry tools, either <a href=https://github.com/NVIDIA/dcgm-exporter>DCGM-Exporter</a> for NVIDIA or <a href=https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html>Neuron Monitor</a> for Amazon Web Services (AWS) <a href=https://aws.amazon.com/ai/machine-learning/inferentia/>Inferentia</a> and AWS <a href=https://aws.amazon.com/ai/machine-learning/trainium/>Trainium</a>, are pre-installed and integrated with NMA for device-level health monitoring.</p><p>Amazon EKS-optimized AMIs for accelerated instances: EKS Auto Mode allows you to create a <a href=https://karpenter.sh/docs/concepts/nodepools/>Karpenter NodePool</a> using GPU instance types. Furthermore, when a workload requests a GPU, it automatically launches the appropriate <a href=https://aws.amazon.com/bottlerocket/>Bottlerocket</a> Accelerated Amazon Machine Image (AMI)—with no need to configure AMI IDs, launch templates, or software components. These AMIs come pre-installed with the necessary drivers, runtimes, and plugins, whether you’re using NVIDIA GPUs or AWS Inferentia and Trainium, so that your AI workloads are ready to run by default.</p><p>Together, these features remove the heavy lifting of configuring and operating GPU infrastructure, so that teams can focus on building, scaling, and running AI/ML workloads without becoming Kubernetes experts.</p><h2 id=walkthrough>Walkthrough</h2><p>In this section, you’ll walk through deploying an open-source large language model (LLM) on a GPU-enabled EKS Auto Mode cluster. You’ll create the cluster, configure a GPU NodePool, deploy the model, and send a test prompt, all with minimal setup.</p><h3 id=prerequisites><strong>Prerequisites</strong></h3><p>To get started, make sure that you have the following prerequisites installed and configured:</p><ul><li><a href=https://aws.amazon.com/cli/>AWS Command Line Interface (AWS CLI</a>) (v2.27.11 or later)</li><li><a href=https://kubernetes.io/docs/reference/kubectl/>kubectl</a></li><li><a href=https://eksctl.io/>eksctl</a> (v0.195.0 or later)</li><li><a href=https://jqlang.org/>jq</a></li></ul><h3 id=set-up-environment-variables><strong>Set up environment variables</strong></h3><p>Configure the following environment variables, replacing the placeholder values as appropriate for your setup:</p><pre tabindex=0><code>export CLUSTER_NAME=automode-gpu-blog-cluster
export AWS_REGION=us-west-2
</code></pre><h3 id=set-up-eks-auto-mode-cluster-and-run-a-model><strong>Set up EKS Auto Mode cluster and run a model</strong></h3><p><strong>Step 1:</strong> Create an EKS Auto Mode using <code>eksctl</code></p><p>Begin by creating your EKS cluster with Auto Mode enabled by running the following command:</p><pre tabindex=0><code>eksctl create cluster --name=$CLUSTER_NAME --region=$AWS_REGION --enable-auto-mode
</code></pre><p>This process takes a few minutes to complete. After completion, <code>eksctl</code> automatically updates your kubeconfig and targets your newly created cluster. To verify that the cluster is operational, use the following:</p><pre tabindex=0><code>kubectl get pods --all-namespaces
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   metrics-server-6d67d68f67-7x4tg       1/1     Running   0          3m
kube-system   metrics-server-6d67d68f67-l4xv6       1/1     Running   0          3m
</code></pre><p>You won’t see components such as VPC CNI, <code>kube-proxy</code>, Karpenter, and <code>CoreDNS</code> in the pod list. In EKS Auto Mode, AWS runs these components on the fully managed infrastructure layer, alongside the Amazon EKS control plane.</p><p><strong>Step 2:</strong> Create a GPU NodePool with Karpenter</p><p>Deploy a GPU NodePool tailored to run ML models. Apply the following NodePool manifest:</p><pre tabindex=0><code>cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-node-pool
spec:
  template:
    metadata:
      labels:
        type: karpenter
        NodeGroupType: gpu-node-pool
    spec:
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      taints:
        - key: nvidia.com/gpu
          value: Exists
          effect: NoSchedule
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: [&#34;spot&#34;, &#34;on-demand&#34;]
        - key: eks.amazonaws.com/instance-category
          operator: In
          values: [&#34;g&#34;]
        - key: eks.amazonaws.com/instance-generation
          operator: Gt
          values: [&#34;4&#34;]
        - key: kubernetes.io/arch
          operator: In
          values: [&#34;amd64&#34;]
  limits:
    cpu: 100
    memory: 100Gi
EOF
</code></pre><p>This NodePool targets GPU-based EC2 instances in the <code>g</code> category with a generation greater than four, such as G5 and G6e instances. These instance families offer powerful NVIDIA GPUs and high-bandwidth networking, making them well-suited for demanding ML inference and generative AI workloads. The applied taint makes sure that only GPU-eligible pods are scheduled on these nodes, maintaining efficient resource isolation. Allowing both On-Demand and Spot capacity types gives EKS Auto Mode the flexibility to optimize for cost while maintaining performance.</p><p><strong>Validate the NodePool:</strong></p><pre tabindex=0><code>kubectl get nodepools
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>NAME              NODECLASS   NODES   READY   AGE
general-purpose   default     0       True    15m
gpu-node-pool     default     0       True    8s
system            default     2       True    15m
</code></pre><p>The <code>gpu-node-pool</code> is created with zero nodes initially. To inspect available nodes, use:</p><pre tabindex=0><code>kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:&#34;status.conditions[?(@.type==&#39;Ready&#39;)].status&#34;,OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.&#39;node\.kubernetes\.io/instance-type&#39;,LIFECYCLE:.metadata.labels.&#39;karpenter\.sh/capacity-type&#39;
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>NAME                  READY     OS-IMAGE                                                              INSTANCE-TYPE   LIFECYCLE
i-0319343e8ad4c5f14   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
i-0a3ff5bfd7be551e2   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
</code></pre><p>EKS Auto Mode runs two <code>c6g</code> instances using the non-accelerated Bottlerocket AMI variant (<code>aws-k8s-1.32-standard</code>), which are CPU-only and used for running metrics server.</p><p><strong>Step 3.</strong> Deploy the gpt-oss-20b model using vLLM</p><p><a href=https://docs.vllm.ai/en/latest/>vLLM</a> is a high-throughput, open source inference engine optimized for large language models (LLMs). The following YAML deploys the <code>vllm</code> container image, which is model-agnostic. In this example, we specify <code>openai/gpt-oss-20b</code> as the model for vLLM to serve.</p><pre tabindex=0><code>cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt-oss-20b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-gptoss-20b
  template:
    metadata:
      labels:
        app: vllm-gptoss-20b
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
      - name: inference-server
        image: public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
        command: [ &#34;vllm&#34;, &#34;serve&#34; ]
        args:
        - openai/gpt-oss-20b
        - --gpu-memory-utilization=0.90
        - --tensor-parallel-size=1
        - --max-model-len=20000
        env:
        - name: VLLM_ATTENTION_BACKEND
          value: &#34;TRITON_ATTN&#34;
        - name: PORT
          value: &#34;8000&#34;
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
---
apiVersion: v1
kind: Service
metadata:
  name: gptoss-service
spec:
  selector:
    app: vllm-gptoss-20b
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP
EOF
</code></pre><p>This deployment uses a toleration for <code>nvidia.com/gpu</code>, matching the taint on your GPU NodePool. Initially, no GPU nodes are present, so the pod enters the <code>Pending</code> state. Karpenter detects the unschedulable pod and automatically provision a GPU node. When the instance is ready, the pod is scheduled and transitions to the <code>ContainerCreating</code> state, at which point it begins pulling the <code>vllm</code> container image. When the container image is pulled and unpacked, the container enters the <code>Running</code> state.</p><p>Wait for the pod to show <code>Running</code>. To monitor pod events, use the following:</p><pre tabindex=0><code>kubectl get pods -l app=vllm-gptoss-20b -w
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>NAME                            READY   STATUS    RESTARTS   AGE
gpt-oss-20b-7dc7f7658d-8xsbm    1/1     Running   0          5m
</code></pre><p>To check the pod events use the following:</p><pre tabindex=0><code>kubectl describe pod -l app=vllm-gptoss-20b
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>Events:
  Type     Reason            Age    From                   Message
  ----     ------            ----   ----                   -------
  Warning  FailedScheduling  2m33s  default-scheduler      0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Nominated         2m32s  eks-auto-mode/compute  Pod should schedule on: nodeclaim/gpu-node-pool-c4bb5
  Normal   Scheduled         95s    default-scheduler      Successfully assigned default/gpt-oss-20b-68b49c7b44-2r7gr to i-05a572a3bbed6669f
  Normal   Pulling           89s    kubelet                Pulling image &#34;public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci&#34;
  Normal   Pulled            1s     kubelet                Successfully pulled image &#34;public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci&#34; in 1m27.666s (1m27.666s including waiting). Image size: 14221606791 bytes.
  Normal   Created           1s     kubelet                Created container: inference-server
  Normal   Started           1s     kubelet                Started container inference-server
</code></pre><p>It may take a few minutes for the pod to be in the <code>Running</code> state. In the preceding example, Karpenter provisioned the instance and scheduled the pod in under a minute. The remaining time was spent downloading the <code>vllm</code> image over the internet, which is roughly 14 GB in size.</p><p>When the container is <code>Running</code>, the model weights start loading into GPU memory, which takes a few minutes. View logs to track the loading progress:</p><pre tabindex=0><code>kubectl logs -l app=vllm-gptoss-20b -f
</code></pre><p>When the model has loaded, you should see output similar to the following:</p><pre tabindex=0><code>INFO 08-22 22:26:52 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
</code></pre><p>After applying the manifest, Karpenter provisioned a GPU instance that satisfies the constraints defined in the NodePool. To see which instance was launched, run the following command:</p><pre tabindex=0><code>kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:&#34;status.conditions[?(@.type==&#39;Ready&#39;)].status&#34;,OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.&#39;node\.kubernetes\.io/instance-type&#39;,LIFECYCLE:.metadata.labels.&#39;karpenter\.sh/capacity-type&#39;
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>NAME                  READY     OS-IMAGE                                                              INSTANCE-TYPE   LIFECYCLE
i-0319343e8ad4c5f14   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
i-0a3ff5bfd7be551e2   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
i-029d33a1259f77564   True      Bottlerocket (EKS Auto, Nvidia) 2025.7.25 (aws-k8s-1.32-nvidia)       g6e.xlarge      spot
</code></pre><p>In this case Karpenter determined that the <a href=https://aws.amazon.com/ec2/instance-types/g6e/>G6e</a> xlarge spot instance is the most cost-efficient instance type that adheres to the constraints defined in the NodePool.</p><p><strong>Step 5.</strong> Test the model endpoint</p><p>First, execute a port forward to the <code>gptoss-service</code> service using kubectl:</p><pre tabindex=0><code>kubectl port-forward service/gptoss-service 8000:8000
</code></pre><p>In another terminal, send a test prompt using <code>curl</code>:</p><pre tabindex=0><code>curl http://localhost:8000/v1/chat/completions \
  -H &#34;Content-Type: application/json&#34; \
  -d &#39;{
    &#34;model&#34;: &#34;openai/gpt-oss-20b&#34;,
    &#34;messages&#34;: [
      {
        &#34;role&#34;: &#34;user&#34;,
        &#34;content&#34;: &#34;What is machine learning?&#34;
      }
    ],
    &#34;temperature&#34;: 0.7,
    &#34;max_tokens&#34;: 100
  }&#39; | jq -r &#39;.choices[0].message.content&#39;
</code></pre><p><strong>Sample output:</strong></p><pre tabindex=0><code>**Machine learning (ML)** is a branch of computer science that gives computers the ability to learn from data, identify patterns, and make decisions or predictions without being explicitly programmed to perform each specific task.
</code></pre><p>This setup allows you to test and interact with your inference server without exposing it externally. To make it accessible to other applications or users, you can update the service type to <code>LoadBalancer</code>, for either external access or within your VPC. If exposing the service, then make sure to implement appropriate access controls such as authentication, authorization, and rate limiting.</p><p><strong>Step 6.</strong> Cleaning up</p><p>When you have finished your experiments, you must clean up the resources you created to avoid incurring ongoing charges. To delete the cluster and all associated resources managed by EKS Auto Mode, run the following command:</p><pre tabindex=0><code>eksctl delete cluster --name=$CLUSTER_NAME --region=$AWS_REGION
</code></pre><p>This command removes the entire EKS cluster along with its control plane, data plane nodes, NodePools, and all resources managed by EKS Auto Mode.</p><h2 id=reducing-model-cold-start-time-in-ai-inference-workloads><strong>Reducing model cold start time in AI inference workloads</strong></h2><p>As you saw in the preceding section, it took a few minutes for the container image to download, the model to be fetched, and the weights to load into GPU memory. This delay is often caused by large container images (over 14 GB in this case), model downloads from external sources, and the time needed to load the model into memory, adding latency to pod startup and scaling events. In production scenarios, especially when running inference at scale, you must use Kubernetes autoscaling and minimize this startup time to make sure of fast, responsive scaling. In this section, we walk through techniques to optimize model startup time and reduce cold start delays.</p><p>Store vLLM container image in Amazon ECR and use a VPC endpoint: Pulling container images from public registries over the internet introduces latency during pod startup, especially when images are large or network bandwidth is constrained. To reduce this overhead:</p><ul><li>Store your container image in <a href=https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html>Amazon Elastic Container Registry (Amazon ECR)</a>, a fully managed container registry that is regionally available and optimized for use with Amazon EKS.</li><li>Configure an <a href=https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ecr.html>Amazon ECR VPC endpoint </a>so that nodes pull images over the AWS backbone rather than the public internet.</li></ul><p>Prefetch model artifacts using AWS Storage options: To reduce the startup time introduced by model downloads and loading from <a href=https://huggingface.co/>Hugging Face</a>, store the model artifacts in an AWS storage option that supports concurrent access and high-throughput reads across multiple nodes and <a href=https://aws.amazon.com/about-aws/global-infrastructure/regions_az/>AWS Availability Zones (AZs).</a> This is essential when multiple replicas of your inference service, possibly running across different nodes or AZs, need to read the same model weights simultaneously. Shared storage avoids the need to download and store duplicate copies of the model per pod or per node.</p><ul><li><a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html>Amazon S3</a> with <a href=https://github.com/awslabs/mountpoint-s3>Mountpoint</a> and <a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-high-performance.html#s3-express-one-zone>S3 Express One Zone</a>: Express One Zone stores data in a single AZ, although it can be accessed from other AZs in the same Region. This is the lowest-cost storage option and most direct to set up. It is ideal for general-purpose inference workloads where performance requirements are moderate and directness is key. For best results, configure a <a href=https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html>VPC endpoint</a> to make sure that traffic stays within the AWS network.</li><li><a href=https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html>Amazon Elastic File System (Amazon EFS)</a>: A natively multi-AZ service that automatically replicates data across AZs. Amazon EFS is an easy-to-use shared file system that offers a good balance of cost, latency, and throughput. It is suitable for workloads that need consistent access to models from multiple AZs with built-in high availability.</li><li><a href=https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html>Amazon FSx for Lustre</a>: Deployed in a single AZ and accessible from other AZs within the same VPC. It delivers the highest-performance option for shared storage. Although FSx for Lustre may have a higher storage cost, its speed in loading model weights can reduce overall GPU idle time, often balancing out the cost while providing the fastest model loading performance.</li></ul><p>Separating model artifacts from container images, storing your containers in Amazon ECR, and choosing the right storage backend for your models, such as Amazon S3 Mountpoint, Amazon EFS, or Amazon FSx for Lustre allows you to significantly reduce startup time and improve the responsiveness of your inference workloads. To explore more strategies for optimizing container and model startup time on Amazon EKS, refer to the <a href=https://awslabs.github.io/ai-on-eks/docs/guidance/container-startup-time>AI on Amazon EKS guidance</a>.</p><h2 id=conclusion><strong>Conclusion</strong></h2><p>Amazon EKS Auto Mode streamlines running GPU-powered AI inference workloads by handling cluster provisioning, node scaling, and GPU configuration for you. Dynamic autoscaling through Karpenter, pre-configured AMIs, and built-in GPU monitoring and recovery enable you to deploy models faster—without needing to configure or maintain the underlying infrastructure.</p><p>To further explore running inference workloads on EKS Auto Mode, the following are a few next steps:</p><ul><li>Learn more: Visit the <a href=https://docs.aws.amazon.com/eks/latest/userguide/automode.html>EKS Auto Mode documentation</a> for full capabilities, supported instance types, and configuration options. You can also check out the <a href=https://aws.amazon.com/blogs/containers/getting-started-with-amazon-eks-auto-mode/>EKS Auto Mode post</a> for a hands-on introduction.</li><li>Get hands-on experience: Join an instructor-led AWS virtual workshops from the <a href=https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks>Amazon EKS series</a>, featuring dedicated sessions on Auto Mode and AI inference.</li><li>Explore best practices: Review the Amazon <a href=https://docs.aws.amazon.com/eks/latest/best-practices/aiml.html>EKS best practices guide for AI/ML workloads</a>.</li><li>Plan for scale and costs: If you’re running LLMs or other high-demand GPU workloads, then connect with your AWS account team for pricing guidance, right-sizing recommendations, and planning support. AWS recently <a href=https://aws.amazon.com/blogs/aws/announcing-up-to-45-price-reduction-for-amazon-ec2-nvidia-gpu-accelerated-instances/>announced up to a 45% price reduction</a> on NVIDIA GPU-accelerated instances, including the P4d, P4de, and P5, so it’s a good time to evaluate your options.</li></ul><p>These tools and guidance allow you to run inference workloads at scale with less effort and more confidence.</p><hr><h3 id=about-the-authors><strong>About the authors</strong></h3><img src=../../images/3-Blog/3.2-Blog2/shivam-bio.png width=100 style=float:left;margin-right:15px;margin-top:-1px><p><strong>Shivam Dubey</strong> is a Specialist Solutions Architect at AWS, where he helps customers build scalable, AI-powered solutions on Amazon EKS. He is passionate about open-source technologies and their role in modern cloud-native architectures. Outside of work, Shivam enjoys hiking, visiting national parks, and exploring new genres of music.</p><div style=clear:both></div><img src=../../images/3-Blog/3.2-Blog2/Bharath-Bio.jpg width=100 style=float:left;margin-right:15px;margin-top:-1px><p><strong>Bharath Gajendran</strong> is a Technical Account Manager at AWS, where he empowers customers to design and operate highly scalable, cost-effective, and fault-tolerant workloads utilizing AWS. He is passionate about Amazon EKS and open-source technologies, and specializes in enabling organizations to run and scale AI workloads on EKS.</p><div style=clear:both></div><img src=../../images/3-Blog/3.2-Blog2/Christina-Andonov.png width=100 style=float:left;margin-right:15px;margin-top:-10px><p><strong>Christina Andonov</strong> is a Sr. Specialist Solutions Architect at AWS, helping customers run AI workloads on Amazon EKS with open source tools. She’s passionate about Kubernetes and known for making complex concepts easy to understand.</p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=../../3-blogstranslated/3.1-blog1/ title="Blog 1"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=../../3-blogstranslated/3.3-blog3/ title="Blog 3" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../../js/clipboard.min.js?1765293643></script><script src=../../js/perfect-scrollbar.min.js?1765293643></script><script src=../../js/perfect-scrollbar.jquery.min.js?1765293643></script><script src=../../js/jquery.sticky.js?1765293643></script><script src=../../js/featherlight.min.js?1765293643></script><script src=../../js/highlight.pack.js?1765293643></script><script>hljs.initHighlightingOnLoad()</script><script src=../../js/modernizr.custom-3.6.0.js?1765293643></script><script src=../../js/learn.js?1765293643></script><script src=../../js/hugo-learn.js?1765293643></script><link href=../../mermaid/mermaid.css?1765293643 rel=stylesheet><script src=../../mermaid/mermaid.js?1765293643></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>