<!doctype html><html lang=vi class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="nguyenlamgiang2198@gmial.com"><link rel=icon href=../../../images/favicon.png type=image/png><title>Blog 2 :: Báo cáo thực tập</title>
<link href=../../../css/nucleus.css?1765293643 rel=stylesheet><link href=../../../css/fontawesome-all.min.css?1765293643 rel=stylesheet><link href=../../../css/hybrid.css?1765293643 rel=stylesheet><link href=../../../css/featherlight.min.css?1765293643 rel=stylesheet><link href=../../../css/perfect-scrollbar.min.css?1765293643 rel=stylesheet><link href=../../../css/auto-complete.css?1765293643 rel=stylesheet><link href=../../../css/atom-one-dark-reasonable.css?1765293643 rel=stylesheet><link href=../../../css/theme.css?1765293643 rel=stylesheet><link href=../../../css/hugo-theme.css?1765293643 rel=stylesheet><link href=../../../css/theme-workshop.css?1765293643 rel=stylesheet><script src=../../../js/jquery-3.3.1.min.js?1765293643></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=../../../vi/3-blogstranslated/3.2-blog2/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=../../../><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script src=../../../js/lunr.min.js?1765293643 defer></script><script src=../../../js/auto-complete.js?1765293643 defer></script><script>var baseurl="https://bbitmi.github.io/fcj-internship-report/vi"</script><script src=../../../js/search.js?1765293643 defer></script></div><div class=highlightable><ul class=topics><li data-nav-id=/vi/1-worklog/ title="Nhật ký công việc" class=dd-item><a href=../../../vi/1-worklog/><b>1. </b>Nhật ký công việc
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/vi/1-worklog/1.1-week1/ title="Worklog Tuần 1" class=dd-item><a href=../../../vi/1-worklog/1.1-week1/><b>1.1. </b>Worklog Tuần 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.2-week2/ title="Worklog Tuần 2" class=dd-item><a href=../../../vi/1-worklog/1.2-week2/><b>1.2. </b>Worklog Tuần 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.3-week3/ title="Worklog Tuần 3" class=dd-item><a href=../../../vi/1-worklog/1.3-week3/><b>1.3. </b>Worklog Tuần 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.4-week4/ title="Worklog Tuần 4" class=dd-item><a href=../../../vi/1-worklog/1.4-week4/><b>1.4. </b>Worklog Tuần 4
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.5-week5/ title="Worklog Tuần 5" class=dd-item><a href=../../../vi/1-worklog/1.5-week5/><b>1.5. </b>Worklog Tuần 5
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.6-week6/ title="Worklog Tuần 6" class=dd-item><a href=../../../vi/1-worklog/1.6-week6/><b>1.6. </b>Worklog Tuần 6
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.7-week7/ title="Worklog Tuần 7" class=dd-item><a href=../../../vi/1-worklog/1.7-week7/><b>1.7. </b>Worklog Tuần 7
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.8-week8/ title="Worklog Tuần 8" class=dd-item><a href=../../../vi/1-worklog/1.8-week8/><b>1.8. </b>Worklog Tuần 8
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.9-week9/ title="Worklog Tuần 9" class=dd-item><a href=../../../vi/1-worklog/1.9-week9/><b>1.9. </b>Worklog Tuần 9
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.10-week10/ title="Worklog Tuần 10" class=dd-item><a href=../../../vi/1-worklog/1.10-week10/><b>1.10. </b>Worklog Tuần 10
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.11-week11/ title="Worklog Tuần 11" class=dd-item><a href=../../../vi/1-worklog/1.11-week11/><b>1.11. </b>Worklog Tuần 11
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.12-week12/ title="Worklog Tuần 12" class=dd-item><a href=../../../vi/1-worklog/1.12-week12/><b>1.12 </b>Worklog Tuần 12
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/1-worklog/1.12-week13/ title="Worklog Tuần 13" class=dd-item><a href=../../../vi/1-worklog/1.12-week13/><b>1.13 </b>Worklog Tuần 13
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/vi/2-proposal/ title="Bản đề xuất" class=dd-item><a href=../../../vi/2-proposal/><b>2. </b>Bản đề xuất
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/3-blogstranslated/ title="Các bài blogs đã dịch" class="dd-item
parent"><a href=../../../vi/3-blogstranslated/><b>3. </b>Các bài blogs đã dịch
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/vi/3-blogstranslated/3.1-blog1/ title="Blog 1" class=dd-item><a href=../../../vi/3-blogstranslated/3.1-blog1/><b>3.1. </b>Blog 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/3-blogstranslated/3.2-blog2/ title="Blog 2" class="dd-item
active"><a href=../../../vi/3-blogstranslated/3.2-blog2/><b>3.2. </b>Blog 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/3-blogstranslated/3.3-blog3/ title="Blog 3" class=dd-item><a href=../../../vi/3-blogstranslated/3.3-blog3/><b>3.3. </b>Blog 3
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/vi/4-eventparticipated/ title="Các events đã tham gia" class=dd-item><a href=../../../vi/4-eventparticipated/><b>4. </b>Các events đã tham gia
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/vi/4-eventparticipated/4.1-event1/ title="Sự kiện 1" class=dd-item><a href=../../../vi/4-eventparticipated/4.1-event1/><b>4.1. </b>Sự kiện 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/4-eventparticipated/4.2-event2/ title="Sự kiện 2" class=dd-item><a href=../../../vi/4-eventparticipated/4.2-event2/><b>4.2. </b>Sự kiện 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/4-eventparticipated/4.3-event3/ title="Sự kiện 3" class=dd-item><a href=../../../vi/4-eventparticipated/4.3-event3/><b>4.3. </b>Sự kiện 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/4-eventparticipated/4.4-event4/ title="Sự kiện 4" class=dd-item><a href=../../../vi/4-eventparticipated/4.4-event4/><b>4.4. </b>Sự kiện 4
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/4-eventparticipated/4.5-event5/ title="Sự kiện 5" class=dd-item><a href=../../../vi/4-eventparticipated/4.5-event5/><b>4.5. </b>Sự kiện 5
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/vi/5-workshop/ title=Workshop class=dd-item><a href=../../../vi/5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/vi/5-workshop/5.1-workshop-overview/ title="Tổng quan Workshop" class=dd-item><a href=../../../vi/5-workshop/5.1-workshop-overview/><b>5.1. </b>Tổng quan Workshop
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.2-prerequiste/ title="Chuẩn bị" class=dd-item><a href=../../../vi/5-workshop/5.2-prerequiste/><b>5.2. </b>Chuẩn bị
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.3-s3-vpc/ title="VPC & Amazon RDS" class=dd-item><a href=../../../vi/5-workshop/5.3-s3-vpc/><b>5.3. </b>VPC & Amazon RDS
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/vi/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/ title="Tạo VPC" class=dd-item><a href=../../../vi/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/><b>5.3.1. </b>Tạo VPC
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/ title="Tạo Amazon RDS" class=dd-item><a href=../../../vi/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/><b>5.3.2. </b>Tạo Amazon RDS
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/vi/5-workshop/5.4-s3-onprem/ title="Lambda & API Gateway" class=dd-item><a href=../../../vi/5-workshop/5.4-s3-onprem/><b>5.4. </b>Lambda & API Gateway
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/vi/5-workshop/5.4-s3-onprem/5.4.1-prepare/ title="Tạo Lambda Functions" class=dd-item><a href=../../../vi/5-workshop/5.4-s3-onprem/5.4.1-prepare/><b>5.4.1. </b>Tạo Lambda Functions
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/ title="Tạo API Gateway" class=dd-item><a href=../../../vi/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/><b>5.4.2. </b>Tạo API Gateway
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/ title="Test API Endpoints" class=dd-item><a href=../../../vi/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/><b>5.4.3. </b>Test API Endpoints
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/ title="Cấu hình CORS & Security" class=dd-item><a href=../../../vi/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/><b>5.4.4. </b>Cấu hình CORS & Security
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/vi/5-workshop/5.5-policy/ title="S3, CloudFront & Amplify" class=dd-item><a href=../../../vi/5-workshop/5.5-policy/><b>5.5. </b>S3, CloudFront & Amplify
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/5-workshop/5.6-cleanup/ title="CI/CD, CloudWatch & Dọn dẹp" class=dd-item><a href=../../../vi/5-workshop/5.6-cleanup/><b>5.6. </b>CI/CD, CloudWatch & Dọn dẹp
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/vi/6-self-evaluation/ title="Tự đánh giá" class=dd-item><a href=../../../vi/6-self-evaluation/><b>6. </b>Tự đánh giá
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/vi/7-feedback/ title="Chia sẻ, đóng góp ý kiến" class=dd-item><a href=../../../vi/7-feedback/><b>7. </b>Chia sẻ, đóng góp ý kiến
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://bbitmi.github.io/fcj-internship-report/3-blogstranslated/3.2-blog2/>English</option><option id=vi value=https://bbitmi.github.io/fcj-internship-report/vi/3-blogstranslated/3.2-blog2/ selected>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate=today.toLocaleDateString("en-GB");document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=../../../vi/>Báo cáo thực tập</a> > <a href=../../../vi/3-blogstranslated/>Các bài blogs đã dịch</a> > Blog 2</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#tính-năng-chính-khiến-eks-auto-mode-lý-tưởng-cho-aiml-workloads>Tính năng chính khiến EKS Auto Mode lý tưởng cho AI/ML workloads</a></li><li><a href=#hướng-dẫn-thực-hành>Hướng dẫn thực hành</a><ul><li><a href=#yêu-cầu>Yêu cầu</a></li><li><a href=#cài-đặt-biến-môi-trường>Cài đặt biến môi trường</a></li><li><a href=#cài-đặt-eks-auto-mode-cluster-và-chạy-model>Cài đặt EKS Auto Mode cluster và chạy model</a></li></ul></li><li><a href=#cách-giảm-thời-gian-khởi-động-cold-start-cho-ai-inference-workloads>Cách giảm thời gian khởi động “cold start” cho AI inference workloads</a></li><li><a href=#kết-luận>Kết luận</a></li><li><a href=#các-tác-giả>Các tác giả</a></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Blog 2</h1><h1 id=cách-thực-thi-ai-model-inference-với-gpus-trên-amazon-eks-auto-mode>Cách thực thi AI model inference với GPUs trên Amazon EKS Auto Mode</h1><p>Việc suy luận mô hình AI (AI model inference) bằng GPU đang trở thành thành phần cốt lõi trong các ứng dụng hiện đại, thúc đẩy các hệ thống gợi ý theo thời gian thực, trợ lý thông minh, tạo nội dung tự động, và các tính năng AI yêu cầu độ trễ thấp khác.Kubernetes đã trở thành nền tảng điều phối (orchestrator) được ưa chuộng để chạy các tác vụ suy luận, và các tổ chức muốn tận dụng những khả năng này trong khi vẫn tập trung hoàn toàn vào duy trì tốc độ đổi mới và thời gian ra mắt sản phẩm nhanh. Tuy nhiên thách thức đặt ra là: trong khi nhóm kỹ thuật nhận thấy giá trị của Kubernetes trong khả năng mở rộng linh hoạt và quản lý tài nguyên hiệu quả, họ thường bị chậm lại do phải học các Kubernetes concepts, quản lý cấu hình cụm (cluster configurations) và xử lý các bản cập nhật bảo mật. Điều này khiến họ phân tán khỏi mục tiêu chính: triển khai và tối ưu hóa mô hình AI. Đó chính là lý do <a href=https://docs.aws.amazon.com/eks/latest/userguide/automode.html>Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode</a> ra đời. EKS Auto Mode tự động hóa việc tạo node, quản lý các <a href=https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html#addon-consider-auto>chức năng cốt lõi</a>, và xử lý nâng cấp và vá bảo mật, giúp bạn có thể chạy các tác vụ suy luận AI mà không phải gánh chịu chi phí vận hành phức tạp.</p><p>Trong bài viết này, chúng tôi sẽ hướng dẫn bạn triển khai nhanh chóng các tác vụ suy luận trên EKS Auto Mode. Chúng tôi cũng sẽ giới thiệu các tính năng quan trọng giúp đơn giản hóa việc quản lý GPU, trình bày các phương pháp triển khai mô hình tối ưu, và minh họa bằng một ví dụ thực tế thông qua việc triển khai các <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html>mô hình mã nguồn mở của OpenAI</a> thông qua <a href=https://docs.vllm.ai/en/latest/>vLLM</a>. Dù bạn đang xây dựng một nền tảng AI/machine learning (ML) mới hay tối ưu hóa quy trình hiện có, những mẫu triển khai này sẽ giúp bạn tăng tốc quá trình phát triển trong khi vẫn duy trì hiệu quả vận hành.</p><h2 id=tính-năng-chính-khiến-eks-auto-mode-lý-tưởng-cho-aiml-workloads>Tính năng chính khiến EKS Auto Mode lý tưởng cho AI/ML workloads</h2><p>Trong phần này, chúng ta sẽ tìm hiểu chi tiết hơn về các tính năng chuyên biệt cho GPU được cấu hình sẵn và sẵn sàng sử dụng trong EKS Auto Mode clusters. Những khả năng này cũng có sẵn trong môi trường Amazon EKS tự quản lý, nhưng thường cần phải thiết lập và tinh chỉnh. Tuy nhiên, với EKS Auto Mode các tính năng này được kích hoạt và cấu hình sẵn ngay từ đầu.</p><p><strong>Tự động mở rộng linh hoạt với Karpenter:</strong> EKS Auto Mode bao gồm một phiên bản được quản lý của <a href=https://karpenter.sh/>Karpenter </a>cung cấp một công cụ mã nguồn mở tương thích với<a href=https://aws.amazon.com/ec2/> Amazon Elastic Compute Cloud (Amazon EC2)</a> có kích thước phù hợp, bao gồm cả các tùy chọn tăng tốc bằng GPU, dựa trên yêu cầu của pod. Giải pháp này hỗ trợ cơ chế mở rộng tức thời và cho phép bạn tùy chỉnh hành vi cấp phát tài nguyên để tối ưu theo chi phí, hiệu năng hoặc vị trí triển khai của instance. EKS Auto Mode hỗ trợ sẵn<a href=https://docs.aws.amazon.com/eks/latest/userguide/automode-learn-instances.html> </a><a href=https://docs.aws.amazon.com/eks/latest/userguide/automode-learn-instances.html>các loại và kích thước instance</a> được định nghĩa trước, đồng thời cung cấp <a href=https://docs.aws.amazon.com/eks/latest/userguide/create-node-pool.html#auto-supported-labels>các nhãn và thuộc tính hạn chế (taints)</a> để kiểm soát lịch trình triển khai một cách chính xác.</p><p><strong>Tự động xử lý sự cố GPU:</strong> EKS Auto Mode bao gồm <a href=https://aws.amazon.com/blogs/containers/amazon-eks-introduces-node-monitoring-and-auto-repair-capabilities/>Node Monitoring Agent (NMA) và Node Auto Repair</a>, cho phép phát hiện lỗi GPU và tự động khởi động quá trình khôi phục sau 10 phút kể từ khi phát hiện sự cố. Hệ thống sửa chữa sẽ cô lập node bị ảnh hưởng và thực hiện khởi động lại hoặc thay thế node đó, trong khi vẫn tuân thủ Pod Disruption Budgets. Các công cụ giám sát GPU, như <a href=https://github.com/NVIDIA/dcgm-exporter>DCGM-Exporter</a> cho NVIDIA hoặc <a href=https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html>Neuron Monitor</a> cho Amazon Web Services (AWS) <a href=https://aws.amazon.com/ai/machine-learning/inferentia/>Inferentia</a> và AWS <a href=https://aws.amazon.com/ai/machine-learning/trainium/>Trainium</a>, được cài đặt sẵn và tích hợp với NMA giúp giám sát tình trạng phần cứng ở cấp độ thiết bị.</p><p><strong>Amazon EKS-optimized AMIs cho instances tăng tốc (accelerated instances):</strong> EKS Auto Mode cho phép bạn tạo một <a href=https://karpenter.sh/docs/concepts/nodepools/>Karpenter NodePool</a> sử dụng GPU instance. Hơn nữa, khi workload yêu cầu GPU, hệ thống sẽ tự động khởi tạo Amazon Machine Image (AMI) <a href=https://aws.amazon.com/bottlerocket/>Bottlerocket</a> Accelerated - mà không cần cấu hình thủ công AMI IDs, mẫu khởi chạy (launch templates), hoặc software components. Các AMIs này được cài đặt sẵn driver cần thiết, runtimes, và plugins, dù bạn đang sử dụng GPU của NVIDIA hay bộ tăng tốc AWS Inferentia và Trainium, nhờ vậy AI workloads của bạn sẵn sàng chạy ngay mặc định.</p><p>Tổng hợp lại, các tính năng này loại bỏ phần lớn công việc phức tạp trong việc cấu hình và vận hành hạ tầng GPU, giúp các nhóm kỹ thuật tập trung vào việc xây dựng, mở rộng và triển khai các tác vụ AI/ML mà không cần phải trở thành chuyên gia Kubernetes.</p><h2 id=hướng-dẫn-thực-hành>Hướng dẫn thực hành</h2><p>Trong phần này, bạn sẽ thực hành triển khai một mô hình LLM mã nguồn mở trên EKS Auto Mode có hỗ trợ GPU. Bạn sẽ tạo cluster, cấu hình một GPU NodePool, triển khai mô hình, và gửi một truy vấn thử nghiệm (test prompt), tất cả chỉ với cấu hình đơn giản.</p><h3 id=yêu-cầu>Yêu cầu</h3><p>Để bắt đầu, hãy đảm bảo rằng bạn đã tải và cài đặt cấu hình sau:</p><ul><li><p><a href=https://aws.amazon.com/cli/>AWS Command Line Interface (AWS CLI</a> (v2.27.11 hoặc phiên bản gần nhất)</p></li><li><p><a href=https://kubernetes.io/docs/reference/kubectl/>kubectl</a></p></li><li><p><a href=https://eksctl.io/>eksctl</a> (v0.195.0 hoặc phiên bản gần nhất)</p></li><li><p><a href=https://jqlang.org/>jq</a></p></li></ul><h3 id=cài-đặt-biến-môi-trường>Cài đặt biến môi trường</h3><p>Cấu hình các biến môi trường, thay thế các giá trị mẫu bằng thông tin của bạn:</p><pre tabindex=0><code>export CLUSTER_NAME=automode-gpu-blog-cluster
export AWS_REGION=us-west-2
</code></pre><h3 id=cài-đặt-eks-auto-mode-cluster-và-chạy-model>Cài đặt EKS Auto Mode cluster và chạy model</h3><p><strong>Bước 1:</strong> Tạo một EKS Auto Mode bằng <code>eksctl</code></p><p>Bắt đầu tạo EKS cluster của bạn với Auto Mode có sẵn bằng cách chạy dòng dưới trong command:</p><pre tabindex=0><code>eksctl create cluster --name=$CLUSTER_NAME --region=$AWS_REGION --enable-auto-mode
</code></pre><p>Quá trình này có thể mất vài phút để hoàn thánh. Sau khi hoàn tất, <code>eksctl</code> tự động cập nhật kubeconfig và chuyển sang cluster vừa được tạo. Để xác minh rằng cluster đang hoạt động, sử dụng lệnh sau:</p><pre tabindex=0><code>kubectl get pods --all-namespaces
</code></pre><p>Ví dụ về output:</p><pre tabindex=0><code>NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   metrics-server-6d67d68f67-7x4tg       1/1     Running   0          3m
kube-system   metrics-server-6d67d68f67-l4xv6       1/1     Running   0          3m
</code></pre><p>Bạn sẽ không thấy các thành phần như VPC CNI, <code>kube-proxy</code>, Karpenter và <code>CoreDNS</code> trong danh sách pod. Trong EKS Auto Mode, AWS vận hành các thành phần này trên lớp hạ tầng được quản lý hoàn toàn, cùng với control plane của Amazon EKS.</p><p><strong>Bước 2:</strong> Tạo một GPU NodePool với Karpenter</p><p>Triển khai một GPU NodePool được thiết kế để chạy ML models. Áp dụng NodePool manifest:</p><pre tabindex=0><code>cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu-node-pool
spec:
  template:
    metadata:
      labels:
        type: karpenter
        NodeGroupType: gpu-node-pool
    spec:
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      taints:
        - key: nvidia.com/gpu
          value: Exists
          effect: NoSchedule
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: [&#34;spot&#34;, &#34;on-demand&#34;]
        - key: eks.amazonaws.com/instance-category
          operator: In
          values: [&#34;g&#34;]
        - key: eks.amazonaws.com/instance-generation
          operator: Gt
          values: [&#34;4&#34;]
        - key: kubernetes.io/arch
          operator: In
          values: [&#34;amd64&#34;]
  limits:
    cpu: 100
    memory: 100Gi
EOF
</code></pre><p>NodePool này nhắm tới các instance EC2 dùng GPU thuộc họ <code>g </code>với thế hệ từ 4 trở đi, chẳng hạn G5 và G6e. Những instance này này cung cấp GPU NVIDIA mạnh cùng kết nối mạng băng thông cao, nên rất phù hợp cho các workload suy luận ML đòi hỏi cao và generative AI. Taint được áp dụng đảm bảo chỉ các pod đủ điều kiện dùng GPU mới được lên lịch trên các node này, duy trì cô lập tài nguyên hiệu quả. Việc cho phép đồng thời hai loại On-Demand và Spot mang lại cho EKS Auto Mode tính linh hoạt để tối ưu chi phí trong khi vẫn đảm bảo hiệu năng.</p><p><strong>Kiểm tra NodePool:</strong></p><pre tabindex=0><code>kubectl get nodepools
</code></pre><p><strong>Mẫu output:</strong></p><pre tabindex=0><code>NAME              NODECLASS   NODES   READY   AGE
general-purpose   default     0       True    15m
gpu-node-pool     default     0       True    8s
system            default     2       True    15m
</code></pre><p><code>gpu-node-pool</code> được khởi tạo với 0 node. Để kiểm tra, chạy lệnh sau:</p><pre tabindex=0><code>kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:&#34;status.conditions[?(@.type==&#39;Ready&#39;)].status&#34;,OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.&#39;node\.kubernetes\.io/instance-type&#39;,LIFECYCLE:.metadata.labels.&#39;karpenter\.sh/capacity-type&#39;
</code></pre><p><strong>Mẫu output:</strong></p><pre tabindex=0><code>NAME                  READY     OS-IMAGE                                                              INSTANCE-TYPE   LIFECYCLE
i-0319343e8ad4c5f14   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
i-0a3ff5bfd7be551e2   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
</code></pre><p>EKS Auto Mode khởi chạy hai <code>c6g</code> instances sử dụng Bottlerocket AMI không tăng tốc (non-accelerated) (<code>aws-k8s-1.32-standard</code>), các instance này chỉ sử dụng CPU (CPU-only) và được dùng để chạy dịch vụ metrics server.</p><p><strong>Bước 3.</strong> Triển khai mô hình gpt-oss-20b sử dụng vLLM</p><p><a href=https://docs.vllm.ai/en/latest/>vLLM</a><a href=https://docs.vllm.ai/en/latest/> </a>là công cụ suy luận (inference engine) mã nguồn mở hiệu năng cao được tối ưu hóa cho các mô hình ngôn ngữ lớn (LLMs). Tệp YAML triển khai container image độc lập (model-agnostic) <code>vllm``/``vllm``-openai:``gptoss</code>. Trong ví dụ này, ta chỉ định <code>openai/gpt-oss-20b</code> làm mô hình mà vLLM sẽ phục vụ.</p><pre tabindex=0><code>cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt-oss-20b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-gptoss-20b
  template:
    metadata:
      labels:
        app: vllm-gptoss-20b
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
      - name: inference-server
`        image: ``vllm``/``vllm``-openai:``gptoss`

        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
        command: [ &#34;vllm&#34;, &#34;serve&#34; ]
        args:
        - openai/gpt-oss-20b
        - --gpu-memory-utilization=0.90
        - --tensor-parallel-size=1
        - --max-model-len=20000
        env:
        - name: VLLM_ATTENTION_BACKEND
          value: &#34;TRITON_ATTN_VLLM_V1&#34;
        - name: PORT
          value: &#34;8000&#34;
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
---
apiVersion: v1
kind: Service
metadata:
  name: gptoss-service
spec:
  selector:
    app: vllm-gptoss-20b
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP
EOF
</code></pre><p>Triển khai này sử dụng toleration (miễn nhiễm) <code>nvidia.com/gpu</code>, tương ứng với taint trên GPU NodePool. Ban đầu, chưa có node GPU nào tồn tại, vì vậy pod sẽ ở trạng thái <code>Pending</code>. Karpenter sẽ phát hiện pod không thể được lên lịch và tự động khởi tạo một node GPU mới. Khi instance sẵn sàng, pod được lên lịch và chuyển sang trạng thái <code>ContainerCreating</code>, tại thời điểm đó hệ thống bắt đầu tải (pull) container image của <code>vllm</code>. Sau khi container image được tải và giải nén hoàn tất, container chuyển sang trạng thái <code>Running</code>.</p><p>Chờ đến khi pod chuyển sang trạng thái <code>Running</code>. Để theo dõi các sự kiện của pod, dùng lệnh sau:</p><pre tabindex=0><code>kubectl get pods -l app=vllm-gptoss-20b -w
</code></pre><p><strong>Mẫu output:</strong></p><pre tabindex=0><code>NAME                            READY   STATUS    RESTARTS   AGE
gpt-oss-20b-7dc7f7658d-8xsbm    1/1     Running   0          5m
</code></pre><p>Để kiểm tra pod event dùng lệnh:</p><pre tabindex=0><code>kubectl describe pod -l app=vllm-gptoss-20b
</code></pre><p><strong>Mẫu output:</strong></p><pre tabindex=0><code>Events:
  Type     Reason            Age    From                   Message
  ----     ------            ----   ----                   -------
  Warning  FailedScheduling  2m33s  default-scheduler      0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Nominated         2m32s  eks-auto-mode/compute  Pod should schedule on: nodeclaim/gpu-node-pool-c4bb5
  Normal   Scheduled         95s    default-scheduler      Successfully assigned default/gpt-oss-20b-68b49c7b44-2r7gr to i-05a572a3bbed6669f
  Normal   Pulling           89s    kubelet                Pulling image &#34;public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci&#34;
  Normal   Pulled            1s     kubelet                Successfully pulled image &#34;public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci&#34; in 1m27.666s (1m27.666s including waiting). Image size: 14221606791 bytes.
  Normal   Created           1s     kubelet                Created container: inference-server
  Normal   Started           1s     kubelet                Started container inference-server
</code></pre><p>Mất vài phút để pod chuyển sang trạng thái <code>Running</code>. Trong ví dụ trên, Karpenter đã khởi tạo instance và lên lịch cho pod chưa đầy một phút. Thời gian còn lại dùng để tải image <code>vllm</code> từ internet, có dung lượng khoảng 14 GB.</p><p>Khi container chuyển sang trạng thái <code>Running</code>, trọng số của mô hình (model weights) bắt đầu được nạp vào bộ nhớ GPU, quá trình này mất vài phút để hoàn tất. Xem logs để theo dõi tiến trình:</p><pre tabindex=0><code>kubectl logs -l app=vllm-gptoss-20b -f
</code></pre><p>Khi mô hình được tải xong, bạn sẽ thấy output giống như sau:</p><pre tabindex=0><code>INFO 08-22 22:26:52 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-22 22:26:52 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
</code></pre><p>Sau khi áp dụng manifest, Karpenter đã khởi tạo một GPU instance đáp ứng các ràng buộc (constraints) được định nghĩa trong NodePool. Để xem instance nào đã được khởi tạo, chạy lệnh sau:</p><pre tabindex=0><code>kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:&#34;status.conditions[?(@.type==&#39;Ready&#39;)].status&#34;,OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.&#39;node\.kubernetes\.io/instance-type&#39;,LIFECYCLE:.metadata.labels.&#39;karpenter\.sh/capacity-type&#39;
</code></pre><p><strong>Mẫu output:</strong></p><pre tabindex=0><code>NAME                  READY     OS-IMAGE                                                              INSTANCE-TYPE   LIFECYCLE
i-0319343e8ad4c5f14   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
i-0a3ff5bfd7be551e2   True      Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard)   c6g.large       on-demand
i-029d33a1259f77564   True      Bottlerocket (EKS Auto, Nvidia) 2025.7.25 (aws-k8s-1.32-nvidia)       g6e.xlarge      spot
</code></pre><p>Trong trường hợp này Karpenter xác định rằng <a href=https://aws.amazon.com/ec2/instance-types/g6e/>G6e</a> xlarge spot instance là loại instance có chi phí hiệu quả và tuân thủ các ràng buộc được định nghĩa trong NodePool.</p><p><strong>Bước 5.</strong> Kiểm tra model endpoints</p><p>Đầu tiên, thực thi một port forward đến <code>gptoss``-service</code> service sử dụng kubectl:</p><pre tabindex=0><code>kubectl port-forward service/gptoss-service 8000:8000
</code></pre><p>Trong một terminal khác, gửi một test prompt bằng cách sử dụng <code>curl</code>:</p><pre tabindex=0><code>curl http://localhost:8000/v1/chat/completions \
  -H &#34;Content-Type: application/json&#34; \
  -d &#39;{
    &#34;model&#34;: &#34;openai/gpt-oss-20b&#34;,
    &#34;messages&#34;: [
      {
        &#34;role&#34;: &#34;user&#34;,
        &#34;content&#34;: &#34;What is machine learning?&#34;
      }
    ],
    &#34;temperature&#34;: 0.7,
    &#34;max_tokens&#34;: 100
  }&#39; | jq -r &#39;.choices[0].message.content&#39;
</code></pre><p><strong>Mẫu output:</strong></p><pre tabindex=0><code>**Machine learning (ML)** is a branch of computer science that gives computers the ability to learn from data, identify patterns, and make decisions or predictions without being explicitly programmed to perform each specific task.
</code></pre><p>Thiết lập này cho phép bạn test và tương tác với inference server của mình mà không cần công khai nó ra bên ngoài. Để làm cho nó có thể truy cập được bởi các ứng dụng hoặc người dùng khác, bạn có thể cập nhật service type thành <code>LoadBalancer</code>, cho phép truy cập từ bên ngoài hoặc trong VPC của bạn. Nếu công khai service, hãy đảm bảo triển khai các kiểm soát truy cập phù hợp như authentication, authorization, và rate limiting.</p><p><strong>Bước 5.</strong> Dọn dẹp tài nguyên</p><p>Khi bạn đã hoàn thành các thử nghiệm của mình, bạn phải dọn dẹp các tài nguyên đã tạo để tránh phát sinh chi phí liên tục. Để xóa cluster và tất cả các tài nguyên liên quan được quản lý bởi EKS Auto Mode, hãy chạy lệnh sau:</p><pre tabindex=0><code>eksctl delete cluster --name=$CLUSTER_NAME --region=$AWS_REGION
</code></pre><p>Lệnh này sẽ xóa toàn bộ EKS cluster cùng với control plane, data plane nodes, NodePools, và tất cả các tài nguyên được quản lý bởi EKS Auto Mode.</p><h2 id=cách-giảm-thời-gian-khởi-động-cold-start-cho-ai-inference-workloads>Cách giảm thời gian khởi động “cold start” cho AI inference workloads</h2><p>Như bạn đã thấy trong phần trước, quá trình này mất vài phút để container image được tải xuống, model được lấy về, và weights được nạp vào GPU memory. Độ trễ này thường được gây ra bởi container image có kích thước lớn (trên 17GB trong trường hợp này), việc tải model từ nguồn bên ngoài, và thời gian cần thiết để nạp model vào bộ nhớ, tất cả đều làm tăng độ trễ khi pod khởi động và trong các sự kiện mở rộng. Trong môi trường sản xuất, đặc biệt khi chạy inference ở quy mô lớn, bạn phải sử dụng Kubernetes autoscaling và giảm thiểu thời gian khởi động này để đảm bảo mở rộng nhanh và phản hồi kịp thời. Trong phần này, chúng ta sẽ đi qua các kỹ thuật nhằm tối ưu hóa thời gian khởi động model và giảm độ trễ khởi động lạnh (cold start delays).</p><p><strong>Lưu trữ vLLM container image trong Amazon ECR và sử dụng VPC endpoint:</strong> Pulling container images từ các public registries trên internet gây ra độ trễ trong quá trình pod khởi động, đặc biệt khi image có kích thước lớn hoặc băng thông mạng bị giới hạn. Để giảm chi phí xử lý này:</p><ul><li><p><strong>Lưu trữ container image của bạn trong</strong> <a href=https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html>Amazon Elastic Container Registry (Amazon ECR)</a>, một container registry được quản lý hoàn toàn khả dụng theo khu vực và được tối ưu hóa để dùng với Amazon EKS.</p></li><li><p><strong>Cấu hình</strong> <a href=https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ecr.html>Amazon ECR VPC endpoint </a>để các nodes pull images thông qua AWS backbone thay vì public internet.</p></li></ul><p><strong>Các mô hình được tải sẵn (prefetch model artifacts) bằng cách sử dụng các tùy chọn của AWS Storage:</strong> Để giảm thời gian khởi động do việc tải xuống và load model từ <a href=https://huggingface.co/>Hugging Face</a> gây ra, hãy lưu trữ các model artifacts trong một tùy chọn lưu trữ AWS hỗ trợ truy cập đồng thời và đọc với thông lượng cao trên nhiều nodes và <a href=https://aws.amazon.com/about-aws/global-infrastructure/regions_az/>AWS Availability Zones (</a><a href=https://aws.amazon.com/about-aws/global-infrastructure/regions_az/>AZs</a><a href=https://aws.amazon.com/about-aws/global-infrastructure/regions_az/>)</a>. Điều này là thiết yếu khi nhiều bản sao của inference service của bạn,có thể đang chạy trên các nodes hoặc AZs khác nhau, cần đọc cùng một model weights đồng thời. Việc sử dụng shared storage giúp tránh phải tải xuống và lưu trữ các bản sao trùng lặp của model cho mỗi pod hoặc node.</p><ul><li><p><a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html>Amazon S3</a> **với **<a href=https://github.com/awslabs/mountpoint-s3>Mountpoint</a> **và **<a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-high-performance.html#s3-express-one-zone>S3 Express One Zone</a>: Express One Zone lưu trữ dữ liệu trong một AZ, mặc dù dữ liệu đó có thể được truy cập từ các AZ khác trong cùng một Region. Đây là tùy chọn lưu trữ có chi phí thấp nhất và dễ thiết lập nhất. Giải pháp này lý tưởng cho các công việc suy luận tổng quát nơi yêu cầu hiệu năng ở mức vừa phải và độ trực tiếp là yếu tố chính. Để đạt kết quả tốt nhất, hãy cấu hình một <a href=https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html>VPC endpoint</a> nhằm đảm bảo rằng lưu lượng luôn nằm trong mạng AWS.</p></li><li><p><a href=https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html>Amazon Elastic File System (Amazon EFS)</a>: Là một dịch vụ đa vùng sẵn có (natively multi-AZ service), tự động sao chép dữ liệu giữa các AZs. Amazon EFS là một hệ thống chia sẻ tệp dễ sử dụng, mang lại sự cân bằng tốt giữa chi phí, độ trễ và thông lượng. Phù hợp cho các tải công việc cần truy cập mô hình ổn định từ nhiều AZs, với tính sẵn sàng cao được tích hợp sẵn.</p></li><li><p><a href=https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html>Amazon FSx for Lustre</a>: Được triển khai trong một AZ duy nhất và có thể truy cập từ các AZ khác trong cùng một VPC. Dịch vụ này cung cấp tùy chọn hiệu năng cao nhất cho shared storage.Mặc dù FSx for Lustre có thể có chi phí lưu trữ cao hơn, nhưng tốc độ tải model weights của nó có thể giảm thời gian trống của GPU (GPU idle time), thường cân bằng lại chi phí đồng thời mang lại hiệu suất tải mô hình nhanh nhất.</p></li></ul><p>Tách biệt model artifacts khỏi container images, lưu trữ containers của bạn trong Amazon ECR, và lựa chọn storage backend phù hợp cho models của bạn,chẳng hạn như Amazon S3 Mountpoint, Amazon EFS, hoặc Amazon FSx for Lustre cho phép bạn giảm đáng kể thời gian khởi động và cải thiện khả năng phản hồi của các tải công việc suy luận. Để khám phá thêm các chiến lược nhằm tối ưu hóa thời gian khởi động của container và model trên Amazon EKS, hãy tham khảo tài liệu <a href=https://awslabs.github.io/ai-on-eks/docs/guidance/container-startup-time>AI on Amazon EKS guidance</a>.</p><h2 id=kết-luận>Kết luận</h2><p>Amazon EKS Auto Mode đơn giản hóa việc chạy các công việc suy luận AI sử dụng GPU bằng cách xử lý các tác vụ như khởi tạo cluster, node scaling, và cấu hình GPU thay cho bạn. Tự động mở rộng động thông qua Karpenter, AMIs được cấu hình sẵn, cùng với giám sát và khôi phục GPU tích hợp cho phép bạn triển khai model nhanh hơn - mà không cần phải cấu hình hoặc duy trì hạ tầng nền tảng.</p><p>Để tìm hiểu thêm về việc chạy các inference workloads trên EKS Auto Mode, dưới đây là một vài bước tiếp theo:</p><ul><li><p><strong>Tìm hiểu thêm:</strong> Truy cập <a href=https://docs.aws.amazon.com/eks/latest/userguide/automode.html>EKS Auto Mode documentation</a> để xem đầy đủ capabilities, các loại instance được hỗ trợ, và tùy chọn cấu hình. Bạn cũng có thể xem <a href=https://aws.amazon.com/blogs/containers/getting-started-with-amazon-eks-auto-mode/>EKS Auto Mode post</a> để giới thiệu thực hành.</p></li><li><p><strong>Trải nghiệm thực tế:</strong> Tham gia các buổi hội thảo ảo có giảng viên hướng dẫn thuộc chuỗi Amazon EKS series, trong đó có các session chuyên biệt về Auto Mode và AI inference.</p></li><li><p><strong>Khám phá các phương pháp tốt nhất:</strong> Xem Amazon <a href=https://docs.aws.amazon.com/eks/latest/best-practices/aiml.html>EKS best practices guide for AI/ML workloads</a>.</p></li><li><p>**Lập kế hoạch về khả năng mở rộng và chi phí: **Nếu bạn đang chạy LLMs hoặc GPU workloads có nhu cầu cao, hãy liên hệ với AWS account team của bạn để được hướng dẫn về định giá, khuyến nghị tối ưu kích thước, và hỗ trợ lập kế hoạch. AWS gần đây <a href=https://aws.amazon.com/blogs/aws/announcing-up-to-45-price-reduction-for-amazon-ec2-nvidia-gpu-accelerated-instances/>đã thông báo giảm giá tới 45%</a> cho các instance được tăng tốc bằng NVIDIA GPU, bao gồm P4d, P4de, và P5, vì vậy đây là thời điểm thích hợp để đánh giá các tùy chọn của bạn.</p></li></ul><p>Các công cụ và hướng dẫn này cho phép bạn chạy các tải công việc suy luận ở quy mô lớn với ít nỗ lực hơn và tự tin hơn.</p><h2 id=các-tác-giả>Các tác giả</h2><img src=../../../images/3-Blog/3.2-Blog2/shivam-bio.png width=100 style=float:left;margin-right:15px;margin-top:-1px><p><strong>Shivam Dubey</strong> là Kiến trúc sư Giải pháp Chuyên biệt (Specialist Solutions Architect) tại Amazon Web Services (AWS), nơi anh hỗ trợ khách hàng xây dựng các giải pháp mở rộng quy mô và tích hợp Trí tuệ nhân tạo trên Amazon EKS. Anh có niềm đam mê với các công nghệ mã nguồn mở và vai trò của chúng trong kiến trúc điện toán đám mây hiện đại. Ngoài công việc, Shivam yêu thích leo núi, tham quan các công viên quốc gia và khám phá những thể loại âm nhạc mới.</p><div style=clear:both></div><img src=../../../images/3-Blog/3.2-Blog2/Bharath-Bio.jpg width=100 style=float:left;margin-right:15px;margin-top:-1px><p><strong>Bharath Gajendran</strong> là Quản lý Tài khoản Kỹ thuật tại Amazon Web Services (AWS), nơi anh hỗ trợ khách hàng thiết kế và vận hành các khối lượng công việc có khả năng mở rộng cao, tối ưu chi phí và chịu lỗi hiệu quả bằng cách tận dụng các dịch vụ của AWS. Anh có niềm đam mê với Amazon EKS và các công nghệ mã nguồn mở, đồng thời chuyên về việc giúp các tổ chức triển khai và mở rộng quy mô các khối lượng công việc AI trên EKS.</p><div style=clear:both></div><img src=../../../images/3-Blog/3.2-Blog2/Christina-Andonov.png width=100 style=float:left;margin-right:15px;margin-top:-1px><p><strong>Christina Andonov</strong> là Kiến trúc sư Giải pháp Chuyên biệt Cấp cao tại Amazon Web Services (AWS), nơi cô hỗ trợ khách hàng triển khai các khối lượng công việc AI trên Amazon EKS bằng cách ứng dụng các công cụ mã nguồn mở. Cô có niềm đam mê đặc biệt với Kubernetes và được biết đến với khả năng truyền đạt những khái niệm phức tạp một cách dễ hiểu và trực quan.</p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=../../../vi/3-blogstranslated/3.1-blog1/ title="Blog 1"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=../../../vi/3-blogstranslated/3.3-blog3/ title="Blog 3" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../../../js/clipboard.min.js?1765293643></script><script src=../../../js/perfect-scrollbar.min.js?1765293643></script><script src=../../../js/perfect-scrollbar.jquery.min.js?1765293643></script><script src=../../../js/jquery.sticky.js?1765293643></script><script src=../../../js/featherlight.min.js?1765293643></script><script src=../../../js/highlight.pack.js?1765293643></script><script>hljs.initHighlightingOnLoad()</script><script src=../../../js/modernizr.custom-3.6.0.js?1765293643></script><script src=../../../js/learn.js?1765293643></script><script src=../../../js/hugo-learn.js?1765293643></script><link href=../../../mermaid/mermaid.css?1765293643 rel=stylesheet><script src=../../../mermaid/mermaid.js?1765293643></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>