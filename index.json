[{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Create Lambda Functions","tags":[],"description":"","content":"Step 1: Create IAM Role for Lambda Go to IAM Console → Roles → Create role\nTrusted entity type:\nAWS service Use case: Lambda Add permissions:\nAWSLambdaVPCAccessExecutionRole AWSLambdaBasicExecutionRole Role details:\nRole name: daivietblood-lambda-role Description: IAM role for DaiVietBlood Lambda functions Click Create role\nStep 2: Create Lambda Layer for Dependencies Create a folder for dependencies: mkdir -p nodejs cd nodejs npm init -y npm install mysql2 Create zip file: cd .. zip -r mysql2-layer.zip nodejs Go to Lambda Console → Layers → Create layer\nConfigure:\nName: mysql2-layer Upload: Select mysql2-layer.zip Compatible runtimes: Node.js 18.x, Node.js 20.x Click Create\nStep 3: Create Lambda Function - Get Users Go to Lambda Console → Functions → Create function\nBasic information:\nFunction name: daivietblood-get-users Runtime: Node.js 20.x Architecture: x86_64 Execution role: Use existing role → daivietblood-lambda-role Click Create function\nAdd Layer:\nScroll to Layers → Add a layer Custom layers → Select mysql2-layer Click Add Configure VPC:\nGo to Configuration → VPC → Edit VPC: daivietblood-vpc Subnets: Select both Private Subnets Security groups: daivietblood-lambda-sg Click Save Add Environment Variables:\nGo to Configuration → Environment variables → Edit Add: DB_HOST = daivietblood-db.xxxx.ap-southeast-1.rds.amazonaws.com DB_PORT = 3306 DB_NAME = daivietblood DB_USER = admin DB_PASSWORD = YourSecurePassword123! Click Save Add code in Code tab:\nconst mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const conn = await getConnection(); const [rows] = await conn.execute(\u0026#39;SELECT * FROM users\u0026#39;); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Click Deploy Step 4: Create Lambda Function - Create User Create new function: daivietblood-create-user Same configuration as above (VPC, Layer, Environment Variables) Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const body = JSON.parse(event.body); const { email, name, blood_type, phone } = body; if (!email || !name || !blood_type) { return { statusCode: 400, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Missing required fields\u0026#39; }) }; } const conn = await getConnection(); const [result] = await conn.execute( \u0026#39;INSERT INTO users (email, name, blood_type, phone) VALUES (?, ?, ?, ?)\u0026#39;, [email, name, blood_type, phone || null] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, email, name, blood_type, phone }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); if (error.code === \u0026#39;ER_DUP_ENTRY\u0026#39;) { return { statusCode: 409, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Email already exists\u0026#39; }) }; } return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Step 5: Create Lambda Function - Emergency Requests Create function: daivietblood-emergency-requests Same configuration Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { const conn = await getConnection(); const method = event.httpMethod; try { if (method === \u0026#39;GET\u0026#39;) { const [rows] = await conn.execute( \u0026#39;SELECT * FROM emergency_requests WHERE status = \u0026#34;open\u0026#34; ORDER BY urgency DESC, created_at DESC\u0026#39; ); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } if (method === \u0026#39;POST\u0026#39;) { const body = JSON.parse(event.body); const { requester_name, blood_type, units_needed, hospital, urgency } = body; const [result] = await conn.execute( \u0026#39;INSERT INTO emergency_requests (requester_name, blood_type, units_needed, hospital, urgency) VALUES (?, ?, ?, ?, ?)\u0026#39;, [requester_name, blood_type, units_needed, hospital, urgency || \u0026#39;normal\u0026#39;] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, message: \u0026#39;Emergency request created\u0026#39; }) }; } return { statusCode: 405, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Method not allowed\u0026#39; }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Verification Checklist IAM Role created with VPC and Basic execution permissions Lambda Layer created with mysql2 package Lambda functions created and deployed: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests All functions configured with VPC (Private Subnets) Environment variables set correctly Functions deployed successfully "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create VPC","tags":[],"description":"","content":"Step 1: Create VPC Go to VPC Console → Your VPCs → Create VPC\nConfigure VPC:\nResources to create: VPC and more Name tag auto-generation: daivietblood IPv4 CIDR block: 10.0.0.0/16 IPv6 CIDR block: No IPv6 CIDR block Tenancy: Default Configure Subnets:\nNumber of Availability Zones: 2 Number of public subnets: 2 Number of private subnets: 2 Customize subnets CIDR blocks: Public subnet CIDR block in ap-southeast-1a: 10.0.1.0/24 Public subnet CIDR block in ap-southeast-1b: 10.0.2.0/24 Private subnet CIDR block in ap-southeast-1a: 10.0.3.0/24 Private subnet CIDR block in ap-southeast-1b: 10.0.4.0/24 Configure NAT Gateway:\nNAT gateways: In 1 AZ Configure VPC Endpoints:\nVPC endpoints: None (we\u0026rsquo;ll create later if needed) Click Create VPC\nℹ️ VPC creation takes 2-3 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 2: Verify VPC Resources After creation, verify the following resources were created:\nResource Name Details VPC daivietblood-vpc 10.0.0.0/16 Public Subnet 1 daivietblood-subnet-public1-ap-southeast-1a 10.0.1.0/24 Public Subnet 2 daivietblood-subnet-public2-ap-southeast-1b 10.0.2.0/24 Private Subnet 1 daivietblood-subnet-private1-ap-southeast-1a 10.0.3.0/24 Private Subnet 2 daivietblood-subnet-private2-ap-southeast-1b 10.0.4.0/24 Internet Gateway daivietblood-igw Attached to VPC NAT Gateway daivietblood-nat-public1-ap-southeast-1a In Public Subnet 1 Route Table (Public) daivietblood-rtb-public Routes to IGW Route Table (Private) daivietblood-rtb-private1-ap-southeast-1a Routes to NAT Step 3: Create Security Groups 3.1. Security Group for Lambda\nGo to VPC Console → Security Groups → Create security group\nConfigure:\nSecurity group name: daivietblood-lambda-sg Description: Security group for Lambda functions VPC: Select daivietblood-vpc Inbound rules: (Leave empty - Lambda initiates connections)\nOutbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n3.2. Security Group for RDS\nGo to VPC Console → Security Groups → Create security group\nConfigure:\nSecurity group name: daivietblood-rds-sg Description: Security group for RDS MySQL VPC: Select daivietblood-vpc Inbound rules:\nType Protocol Port Source Description MySQL/Aurora TCP 3306 daivietblood-lambda-sg Allow Lambda access Outbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n⚠️ Security Best Practice: Only allow access from Lambda Security Group to RDS. Never open port 3306 to 0.0.0.0/0.\nStep 4: Create DB Subnet Group Go to RDS Console → Subnet groups → Create DB subnet group\nConfigure:\nName: daivietblood-db-subnet-group Description: Subnet group for DaiVietBlood RDS VPC: Select daivietblood-vpc Add subnets:\nAvailability Zones: Select ap-southeast-1a and ap-southeast-1b Subnets: Select both Private Subnets (10.0.3.0/24 and 10.0.4.0/24) Click Create\nVerification Checklist VPC created with CIDR 10.0.0.0/16 2 Public Subnets created 2 Private Subnets created Internet Gateway attached to VPC NAT Gateway created in Public Subnet Route tables configured correctly Lambda Security Group created RDS Security Group created with inbound rule from Lambda SG DB Subnet Group created with Private Subnets "},{"uri":"https://bbitmi.github.io/fcj-internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Quach Lam Giang\nPhone Number: 0899197017\nEmail: nguyenlamgiang2198@gmail.com\nUniversity: FPT University Ho Chi Minh City\nMajor: Artificial Intelligent\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"System Architecture The DaiVietBlood system uses a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nArchitecture Components 1. Network Infrastructure (VPC)\nComponent Description VPC Virtual Private Cloud with CIDR 10.0.0.0/16 Public Subnet Contains NAT Gateway, allows Internet access Private Subnet Contains Lambda, RDS - isolated from Internet NAT Gateway Allows Private Subnet resources to access Internet Internet Gateway Allows Public Subnet to communicate with Internet 2. Application \u0026amp; Data Layer\nService Role AWS Lambda Process business logic (CRUD operations, emergency requests) API Gateway Receive HTTP requests, route to Lambda Amazon RDS MySQL database storing user data, blood inventory Amazon S3 Store static files (images, documents) 3. Frontend \u0026amp; Distribution\nService Role AWS Amplify Host React application CloudFront CDN distributes content globally with low latency 4. DevOps \u0026amp; Monitoring\nService Role CodePipeline Automate CI/CD process CodeBuild Build and test source code CloudWatch Collect logs, metrics, set up alarms Data Flow User Request → CloudFront → Amplify (Frontend) ↓ API Gateway ↓ AWS Lambda (Private Subnet) ↓ Amazon RDS (Private Subnet) Security Model Network Isolation: RDS and Lambda in Private Subnet, no direct Internet access IAM Roles: Each service has minimum required permissions (Least Privilege) Data Encryption: At-rest (RDS, S3) and In-transit (HTTPS) Security Groups: Control inbound/outbound traffic for each resource Workshop Objectives After completing this workshop, you will be able to:\n✅ Create VPC with proper network segmentation ✅ Deploy RDS MySQL in Private Subnet ✅ Build Lambda functions and expose via API Gateway ✅ Configure S3 and CloudFront for static content ✅ Deploy React app with Amplify ✅ Set up CI/CD pipeline ✅ Monitor with CloudWatch "},{"uri":"https://bbitmi.github.io/fcj-internship-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Building geolocation verification for iGaming and sports betting on AWS Geolocation verification in sports betting and iGaming serves two primary purposes: compliance and fraud prevention. Online sports betting and iGaming operators may need to ensure that only players from certain regions of the world are able to view content due to in-market licensing restrictions. Or gaming regulations may require that access is restricted to persons within allowed jurisdictions while blocking unauthorized traffic at geopolitical boundaries.\nIn the United States, the Federal Wire Act, which prohibits sports betting across state lines, begins with the following text:\nWhoever being engaged in the business of betting or wagering knowingly uses a wire communication facility for the transmission in interstate or foreign commerce of bets or wagers or information assisting in the placing of bets or wagers on any sporting event or contest, or for the transmission of a wire communication which entitles the recipient to receive money or credit as a result of bets or wagers, or for information assisting in the placing of bets or wagers, shall be fined under this title or imprisoned not more than two years, or both.\nIGT and the New Hampshire State lottery have prevailed in a court case against the United States (US) Department of Justice regarding the non-applicability of the Wire Act to iGaming and lottery operations. However, some states still require that players and wagering infrastructure be located within the same state. In these states, operators must also put in place mechanisms to detect virtual private networks (VPNs), software, virtualization, and other methods capable of circumventing player location detection. Usually this means an SDK must be compiled into an app or other software (a sidecar) must run alongside it.\nIn addition to allowing an operator to comply with regulations, geographic access controls deliver additional business benefits. Blocking traffic at the edge reduces infrastructure costs by eliminating unauthorized traffic before it reaches core systems. These controls can also be used to thwart fraud, for example, so-called proxy betting where a player in an allowed geographic zone places wagers for multiple people outside that zone.\nThe technology choice for geolocation verification depends on the specific use case of the company employing it. In the US, Brazil, and some other regions, sports betting and iGaming operators utilize compliance-tested, licensed geolocation systems, with SDKs accessing mobile OS directly to satisfy anti-spoofing requirements. However, there are alternatives which may be appropriate for content providers or operators in other regions.\nThese methods may be used alone or in conjunction with one another to provide a satisfactory solution, depending on compliance and business needs. Let’s explore five distinct approaches companies can use to build and deploy geolocation verification using Amazon Web Service (AWS).\n1. Geolocation blocking using Amazon Route 53 The geolocation routing capabilities of Amazon Route 53 can restrict access to content by country, continent, or US state domain name system server (DNS) level. This DNS-based approach processes location decisions before traffic reaches your infrastructure, reducing unnecessary load and associated costs. A key benefit of this solution is that it requires no modifications to existing server or client code bases. The implementation uses DNS records with geolocation routing policies, which determine a user’s location and respond according to the rules configured for that geography. Following is an example Route 53 geolocation traffic routing policy:\n{ \u0026#34;RuleType\u0026#34;: \u0026#34;geo\u0026#34;, \u0026#34;Locations\u0026#34;: [ {\u0026#34;EvaluateTargetHealth\u0026#34;: true, \u0026#34;Country\u0026#34;: \u0026#34;SE\u0026#34;, \u0026#34;EndpointReference\u0026#34;: ENDPOINT_IF_TRAFFIC_ORINGINATES_FROM_SWEDEN }, { \u0026#34;Country\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;IsDefault\u0026#34;: true, \u0026#34;EvaluateTargetHealth\u0026#34;: true, \u0026#34;EndpointReference\u0026#34;: ENDPOINT_FOR_ALL_OTHER_TRAFFIC } ] } The example policy routes traffic to endpoints based on whether the country of origin is Sweden. In this example, traffic originating from Sweden will be sent to an application load balancer, while traffic originating from elsewhere will be directed to an error page hosted on Amazon CloudFront. Route 53 geolocation routing integrates directly with other AWS services and includes fallback rules for unmatched locations.\nDNS-based geolocation does have limitations. The solution relies on the accuracy of IP-to-location mapping, which can be circumvented by VPNs or proxy servers. DNS responses may be cached by intermediate resolvers, potentially allowing users to retain access from a blocked location after moving. Additionally, the solution cannot detect device tampering, location spoofing, or prevent proxy betting. It has not been approved for use by licensed US online gambling operators for the purpose of meeting their compliance needs.\n2. Amazon Location Service with JavaScript Customers can use Amazon Location Service combined with a client-side JavaScript to build geolocation capabilities directly inside any app which uses JavaScript (web apps, React Native, Swift, and so on). This approach retrieves GPS coordinates from the user’s environment, potentially offering a more precise location determination than IP-based methods. Developers must modify their applications to include location request code, integrate the AWS SDK for JavaScipt (version 3), and create API endpoints for location verification. For a basic implementation, following is an example JavaScript implementation with error handling:\n// use API Key id for credentials const authHelper = await withAPIKey(\u0026#34;api-key-id\u0026#34;); // Initialize the Location client const locationClient = new LocationClient({ region: \u0026#34;us-east-1\u0026#34;, ...authHelper.getLocationClientConfig() }); async function updateDeviceLocation() { try { // Get current position from browser const position = await new Promise((resolve, reject) =\u0026gt; { navigator.geolocation.getCurrentPosition(resolve, reject, { enableHighAccuracy: true, timeout: 10000 }); }); // Prepare the update command const command = new BatchUpdateDevicePositionCommand({ TrackerName: \u0026#34;MyDeviceTracker\u0026#34;, Updates: [{ DeviceId: \u0026#34;mobile-device-123\u0026#34;, Position: [position.coords.longitude, position.coords.latitude], SampleTime: new Date() }] }); // Send location update to Amazon Location Service const response = await locationClient.send(command); console.log(\u0026#34;Location updated successfully:\u0026#34;, response); } catch (error) { console.error(\u0026#34;Failed to update location:\u0026#34;, error); } } Amazon Location Service provides a JavaScript authentication helper to streamline authentication when making API calls. This way you can avoid hardcoding credentials in your JavaScript. You can use either Amazon Cognito or API keys as your authentication method. This solution requires user interaction. Browsers will display a permission prompt requesting access to location services. Once granted, the code retrieves GPS coordinates and emits events when geofence borders are crossed.\nIf using the ForecastGeofenceEvents API call, Amazon Location Service can validate them against defined geographic boundaries. For Android WebView implementations, developers must enable JavaScript in their WebView settings by requesting ACCESS_FINE_LOCATION permission in the app manifest. They must additionally implement a custom WebChromeClient to handle geolocation permission requests. Android applications also require a local device cache for storing geolocation permissions and positions, configured through the WebView’s geolocation database path setting.\nFor iOS WebView implementations using WKWebView, geolocation is allowed by default, but developers must add the NSLocationWhenInUseUsageDescription key to their Info.plist file with a message explaining the location access requirement. The application must also request location authorization through the iOS location services framework.\nThis solution is only viable for apps utilizing JavaScript. The implementation introduces latency as the system waits for user permission and GPS acquisition. User experience can be impacted when location access is denied, or GPS signals are unavailable in certain environments. This method cannot detect location spoofing or device tampering. While this method provides more precise location data than IP-based solutions, it still falls short of satisfying more stringent requirements for regulated gaming operations, particularly when detecting location spoofing is required.\n3. Blocking or allowing through Amazon CloudFront The geographic restrictions Amazon CloudFront offers is content access control at AWS edge locations. This solution blocks or allows requests before they reach your origin servers, reducing both infrastructure costs and potential security risks.\nFollowing is a CloudFront geographic restriction configuration used in the Distribution Settings:\n{ \u0026#34;GeoRestriction\u0026#34;: { \u0026#34;RestrictionType\u0026#34;: \u0026#34;allowlist\u0026#34;, // Alternative: \u0026#34;blocklist\u0026#34; \u0026#34;Locations\u0026#34;: [ \u0026#34;GB\u0026#34;, // United Kingdom \u0026#34;IE\u0026#34;, // Ireland \u0026#34;MT\u0026#34; // Malta ] } } It’s also possible for you to configure CloudFront to add location headers to the requests that CloudFront receives from your users and forward them to your app. In this way, you can create app logic that can take other actions besides allowing or denying users’ access. The geographic restrictions of CloudFront operate at the country level only—they cannot differentiate between US states or regions within countries.\nFor operators in markets requiring country-level blocking, CloudFront provides a cost-effective first line of defense. CloudFront geo-blocking requires minimal development effort. It can be configured through the AWS Console, command line interface (CLI), or Infrastructure as Code (IaC) tools and services without modifying application code. This makes it an attractive option for quick deployment of basic geographic controls.\nUsing the geographic restrictions of CloudFront does not incur additional charges beyond standard CloudFront usage fees. Blocked requests are denied at the edge location, resulting in minimal data transfer costs. When a request is blocked, CloudFront returns an HTTP 403 (Forbidden) error response, consuming only a few bytes of data transfer. This makes CloudFront geo-blocking particularly economical for applications receiving high volumes of traffic from restricted regions.\nBlocking requests at CloudFront edge locations instead of your origin servers could save significant bandwidth costs and reduce the load on your backend infrastructure. Additionally, since distributed denial-of-service (DDoS) attacks often originate from specific geographic regions, country-level blocking can reduce your exposure to these attacks and associated costs.\nHowever, CloudFront geo-blocking has limitations beyond its country-level granularity. The service relies on IP address mapping to determine location, which can be circumvented by VPNs or proxy servers. It relies on cache-control headers and invalidation strategies to verify data freshness for dynamic APIs—meaning location data may not always be up-to-date. Additionally, no device integrity checking or anti-spoofing measures are available through this method, which may invalidate it as an option for iGaming or sports betting operators with more stringent geolocation requirements.\n4. AWS WAF geo match statements AWS WAF provides geographic access control through geo match statements, offering more granular control than CloudFront restrictions. The service can filter traffic based on both country and US state locations, making it useful for operators who need regional precision in their access controls.\nFollowing is an AWS WAF rule with a geo match statement:\n{ \u0026#34;Name\u0026#34;: \u0026#34;RegionalAccessControl\u0026#34;, \u0026#34;Statement\u0026#34;: { \u0026#34;GeoMatchStatement\u0026#34;: { \u0026#34;CountryCodes\u0026#34;: [\u0026#34;SG\u0026#34;], \u0026#34;ForwardedIPConfig\u0026#34;: { \u0026#34;HeaderName\u0026#34;: \u0026#34;X-Forwarded-For\u0026#34;, \u0026#34;FallbackBehavior\u0026#34;: \u0026#34;MATCH\u0026#34; } } }, \u0026#34;Action\u0026#34;: { \u0026#34;Allow\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;RegionalAccessMetric\u0026#34; } } A key advantage of AWS WAF geo matching is its integration with Amazon CloudWatch metrics, enabling detailed monitoring of blocked requests and traffic patterns. This visibility helps operators understand access patterns and adjust their blocking strategies accordingly. AWS WAF also handles IPv6 traffic natively and can process requests based on the X-Forwarded-For header, important for applications behind proxy servers or load balancers.\nHowever, AWS WAF geo matching cannot detect sophisticated location spoofing attempts or verify device integrity. While it can identify and block traffic from known proxy services and VPN endpoints, determined users may still circumvent these controls.\n5. Licensed geolocation verification suppliers There are many jurisdictions with stringent requirements for geolocation verification systems (for example, US and Brazil). For regulated gaming operators, these markets generally require geolocation verification systems that can detect and prevent location spoofing by verifying integrity at the device level. Licensed geolocation verification suppliers such as GeoComply, OpenBet, and Xpoint provide these capabilities.\nThese company’s solutions differ fundamentally from the previously described solutions in their approach to location verification. Rather than relying solely on IP addresses or GPS coordinates, they retrieve location information from the device itself (using Wi-Fi trilateration, GPS, and so on). They verify device integrity by using a device-specific SDK to examine the operating system for signs of tampering, VPNs, or location spoofing software.\nGeofence management systems determine whether the device is within or near a geofence border, which may be a state border or other political boundary. Real-time monitoring detects sudden location changes that might indicate proxy betting or other nefarious action. Building these systems typically requires integration at multiple levels, including:\nNative SDK (iOS and Android) integration for mobile apps Browser plugin or sidecar application for web apps Amazon Location Services API integration Compliance reporting endpoints Licensed geolocation suppliers are a good option when:\nRegulations require precise geofencing and location verification (such as the US Federal Wire Act or Brazil’s gaming laws) Jurisdictional requirements mandate anti-spoofing and device integrity verification Operators need to prevent proxy betting through sophisticated device-level checks Conclusion The choice of geolocation method depends on various factors, including regulatory requirements for the target geography, risk profile, and cost. While many operators implement multiple layers of protection, it’s crucial to understand which solutions are appropriate, based on the risks and requirements at issue. If stringent criteria are not required, the less costly solutions outlined previously can provide helpful geolocation capabilities while maintaining performance and the user experience.\nContact an AWS Representative to know how we can help accelerate your business.\nFurther reading Guidance for Building Geolocation Systems for the Betting \u0026amp; Gaming Industry on AWS Amazon Location Service launches Enhanced Location Integrity features OpenBet Delivers Geolocation Integrity for Betting and Gaming Using Solutions on AWS Winning the Cat-and-Mouse Race: Staying One Step Ahead of Streaming Geopiracy with GeoGuard and AWS Dr. Mike Reaves Dr. Mike Reaves is Principal Solutions Architect, Betting \u0026amp; Gaming at Amazon Web Services (AWS). He works with customers to develop technical strategy, and leverages AWS technologies to develop purpose-built solutions for customers. An industry veteran, Dr. Reaves is an AWS certified Solutions Architect with over 25 years of software development and IT experience. Dr. Reaves holds a Ph.D. in Physics from the University of Connecticut.\nDr. Haowen You Dr. Haowen You is Software Engineering Manager, Geospatial at Amazon Web Services (AWS). He leads a software development team for Amazon Location Service. With 12+ years of experience managing software teams and projects, Dr. You is currently focusing his efforts on AWS applied AI products. Dr. You holds a Ph.D. in Systems Engineering from the University of Virginia.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"How to run AI model inference with GPUs on Amazon EKS Auto Mode AI model inference using GPUs is becoming a core part of modern applications, powering real-time recommendations, intelligent assistants, content generation, and other latency-sensitive AI features. Kubernetes has become the orchestrator of choice for running inference workloads, and organizations want to use its capabilities while still maintaining a strong focus on rapid innovation and time-to-market. But here’s the challenge: while teams see the value of Kubernetes for its dynamic scaling and efficient resource management, they often get slowed down by the need to learn Kubernetes concepts, manage cluster configurations, and handle security updates. This shifts focus away from what matters most: deploying and optimizing AI models. That is where Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode comes in. EKS Auto Mode Automates node creation, manages core capabilities, and handles upgrades and security patching. In turn, this enables to run your inference workloads without the operational overhead.\nIn this post, we show you how to swiftly deploy inference workloads on EKS Auto Mode. We also demonstrate key features that streamline GPU management, show best practices for model deployment, and walk through a practical example by deploying open weight models from OpenAI using vLLM. Whether you’re building a new AI/machine learning (ML) platform or optimizing existing workflows, these patterns help you accelerate development while maintaining operational efficiency.\nKey features that make EKS Auto Mode ideal for AI/ML workloads In this section, we take a closer look at the GPU-specific features that come pre-configured and ready to use with an EKS Auto Mode cluster. These capabilities are also available in self-managed Amazon EKS environments, but they typically need manual setup and tuning. However, EKS Auto Mode has them enabled and configured out of the box.\nDynamic autoscaling with Karpenter: EKS Auto Mode includes a managed version of open source conformant Karpenter that provisions right-sized Amazon Elastic Compute Cloud (Amazon EC2) instances, such as GPU‑accelerated options, based on pod requirements. It supports just-in-time scaling and allows you to configure provisioning behavior so that you can optimize for cost, performance, or instance placement. EKS Auto Mode supports a predefined set of instance types and sizes, along with node labels and taints for scheduling control.\nAutomatic GPU failure handling: EKS Auto Mode includes Node Monitoring Agent (NMA) and Node Auto Repair, which detect GPU failures and initiate automated recovery 10 minutes after detection. The repair process cordons the affected node and either reboots or replaces it, while respecting Pod Disruption Budgets. GPU telemetry tools, either DCGM-Exporter for NVIDIA or Neuron Monitor for Amazon Web Services (AWS) Inferentia and AWS Trainium, are pre-installed and integrated with NMA for device-level health monitoring.\nAmazon EKS-optimized AMIs for accelerated instances: EKS Auto Mode allows you to create a Karpenter NodePool using GPU instance types. Furthermore, when a workload requests a GPU, it automatically launches the appropriate Bottlerocket Accelerated Amazon Machine Image (AMI)—with no need to configure AMI IDs, launch templates, or software components. These AMIs come pre-installed with the necessary drivers, runtimes, and plugins, whether you’re using NVIDIA GPUs or AWS Inferentia and Trainium, so that your AI workloads are ready to run by default.\nTogether, these features remove the heavy lifting of configuring and operating GPU infrastructure, so that teams can focus on building, scaling, and running AI/ML workloads without becoming Kubernetes experts.\nWalkthrough In this section, you’ll walk through deploying an open-source large language model (LLM) on a GPU-enabled EKS Auto Mode cluster. You’ll create the cluster, configure a GPU NodePool, deploy the model, and send a test prompt, all with minimal setup.\nPrerequisites To get started, make sure that you have the following prerequisites installed and configured:\nAWS Command Line Interface (AWS CLI) (v2.27.11 or later) kubectl eksctl (v0.195.0 or later) jq Set up environment variables Configure the following environment variables, replacing the placeholder values as appropriate for your setup:\nexport CLUSTER_NAME=automode-gpu-blog-cluster export AWS_REGION=us-west-2 Set up EKS Auto Mode cluster and run a model Step 1: Create an EKS Auto Mode using eksctl\nBegin by creating your EKS cluster with Auto Mode enabled by running the following command:\neksctl create cluster --name=$CLUSTER_NAME --region=$AWS_REGION --enable-auto-mode This process takes a few minutes to complete. After completion, eksctl automatically updates your kubeconfig and targets your newly created cluster. To verify that the cluster is operational, use the following:\nkubectl get pods --all-namespaces Sample output:\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system metrics-server-6d67d68f67-7x4tg 1/1 Running 0 3m kube-system metrics-server-6d67d68f67-l4xv6 1/1 Running 0 3m You won’t see components such as VPC CNI, kube-proxy, Karpenter, and CoreDNS in the pod list. In EKS Auto Mode, AWS runs these components on the fully managed infrastructure layer, alongside the Amazon EKS control plane.\nStep 2: Create a GPU NodePool with Karpenter\nDeploy a GPU NodePool tailored to run ML models. Apply the following NodePool manifest:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: gpu-node-pool spec: template: metadata: labels: type: karpenter NodeGroupType: gpu-node-pool spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: default taints: - key: nvidia.com/gpu value: Exists effect: NoSchedule requirements: - key: karpenter.sh/capacity-type operator: In values: [\u0026#34;spot\u0026#34;, \u0026#34;on-demand\u0026#34;] - key: eks.amazonaws.com/instance-category operator: In values: [\u0026#34;g\u0026#34;] - key: eks.amazonaws.com/instance-generation operator: Gt values: [\u0026#34;4\u0026#34;] - key: kubernetes.io/arch operator: In values: [\u0026#34;amd64\u0026#34;] limits: cpu: 100 memory: 100Gi EOF This NodePool targets GPU-based EC2 instances in the g category with a generation greater than four, such as G5 and G6e instances. These instance families offer powerful NVIDIA GPUs and high-bandwidth networking, making them well-suited for demanding ML inference and generative AI workloads. The applied taint makes sure that only GPU-eligible pods are scheduled on these nodes, maintaining efficient resource isolation. Allowing both On-Demand and Spot capacity types gives EKS Auto Mode the flexibility to optimize for cost while maintaining performance.\nValidate the NodePool:\nkubectl get nodepools Sample output:\nNAME NODECLASS NODES READY AGE general-purpose default 0 True 15m gpu-node-pool default 0 True 8s system default 2 True 15m The gpu-node-pool is created with zero nodes initially. To inspect available nodes, use:\nkubectl get nodes -o custom-columns=NAME:.metadata.name,READY:\u0026#34;status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].status\u0026#34;,OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.\u0026#39;node\\.kubernetes\\.io/instance-type\u0026#39;,LIFECYCLE:.metadata.labels.\u0026#39;karpenter\\.sh/capacity-type\u0026#39; Sample output:\nNAME READY OS-IMAGE INSTANCE-TYPE LIFECYCLE i-0319343e8ad4c5f14 True Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard) c6g.large on-demand i-0a3ff5bfd7be551e2 True Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard) c6g.large on-demand EKS Auto Mode runs two c6g instances using the non-accelerated Bottlerocket AMI variant (aws-k8s-1.32-standard), which are CPU-only and used for running metrics server.\nStep 3. Deploy the gpt-oss-20b model using vLLM\nvLLM is a high-throughput, open source inference engine optimized for large language models (LLMs). The following YAML deploys the vllm container image, which is model-agnostic. In this example, we specify openai/gpt-oss-20b as the model for vLLM to serve.\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: gpt-oss-20b spec: replicas: 1 selector: matchLabels: app: vllm-gptoss-20b template: metadata: labels: app: vllm-gptoss-20b spec: tolerations: - key: nvidia.com/gpu operator: Exists effect: NoSchedule containers: - name: inference-server image: public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci ports: - containerPort: 8000 resources: limits: nvidia.com/gpu: 1 command: [ \u0026#34;vllm\u0026#34;, \u0026#34;serve\u0026#34; ] args: - openai/gpt-oss-20b - --gpu-memory-utilization=0.90 - --tensor-parallel-size=1 - --max-model-len=20000 env: - name: VLLM_ATTENTION_BACKEND value: \u0026#34;TRITON_ATTN\u0026#34; - name: PORT value: \u0026#34;8000\u0026#34; volumeMounts: - mountPath: /dev/shm name: dshm volumes: - name: dshm emptyDir: medium: Memory --- apiVersion: v1 kind: Service metadata: name: gptoss-service spec: selector: app: vllm-gptoss-20b ports: - port: 8000 targetPort: 8000 type: ClusterIP EOF This deployment uses a toleration for nvidia.com/gpu, matching the taint on your GPU NodePool. Initially, no GPU nodes are present, so the pod enters the Pending state. Karpenter detects the unschedulable pod and automatically provision a GPU node. When the instance is ready, the pod is scheduled and transitions to the ContainerCreating state, at which point it begins pulling the vllm container image. When the container image is pulled and unpacked, the container enters the Running state.\nWait for the pod to show Running. To monitor pod events, use the following:\nkubectl get pods -l app=vllm-gptoss-20b -w Sample output:\nNAME READY STATUS RESTARTS AGE gpt-oss-20b-7dc7f7658d-8xsbm 1/1 Running 0 5m To check the pod events use the following:\nkubectl describe pod -l app=vllm-gptoss-20b Sample output:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 2m33s default-scheduler 0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling. Normal Nominated 2m32s eks-auto-mode/compute Pod should schedule on: nodeclaim/gpu-node-pool-c4bb5 Normal Scheduled 95s default-scheduler Successfully assigned default/gpt-oss-20b-68b49c7b44-2r7gr to i-05a572a3bbed6669f Normal Pulling 89s kubelet Pulling image \u0026#34;public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci\u0026#34; Normal Pulled 1s kubelet Successfully pulled image \u0026#34;public.ecr.aws/deep-learning-containers/vllm:0.11.2-gpu-py312-ec2-soci\u0026#34; in 1m27.666s (1m27.666s including waiting). Image size: 14221606791 bytes. Normal Created 1s kubelet Created container: inference-server Normal Started 1s kubelet Started container inference-server It may take a few minutes for the pod to be in the Running state. In the preceding example, Karpenter provisioned the instance and scheduled the pod in under a minute. The remaining time was spent downloading the vllm image over the internet, which is roughly 14 GB in size.\nWhen the container is Running, the model weights start loading into GPU memory, which takes a few minutes. View logs to track the loading progress:\nkubectl logs -l app=vllm-gptoss-20b -f When the model has loaded, you should see output similar to the following:\nINFO 08-22 22:26:52 [launcher.py:37] Route: /rerank, Methods: POST INFO 08-22 22:26:52 [launcher.py:37] Route: /v1/rerank, Methods: POST INFO 08-22 22:26:52 [launcher.py:37] Route: /v2/rerank, Methods: POST INFO 08-22 22:26:52 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST INFO 08-22 22:26:52 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST INFO 08-22 22:26:52 [launcher.py:37] Route: /invocations, Methods: POST INFO 08-22 22:26:52 [launcher.py:37] Route: /metrics, Methods: GET INFO: Started server process [1] INFO: Waiting for application startup. INFO: Application startup complete. After applying the manifest, Karpenter provisioned a GPU instance that satisfies the constraints defined in the NodePool. To see which instance was launched, run the following command:\nkubectl get nodes -o custom-columns=NAME:.metadata.name,READY:\u0026#34;status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].status\u0026#34;,OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.\u0026#39;node\\.kubernetes\\.io/instance-type\u0026#39;,LIFECYCLE:.metadata.labels.\u0026#39;karpenter\\.sh/capacity-type\u0026#39; Sample output:\nNAME READY OS-IMAGE INSTANCE-TYPE LIFECYCLE i-0319343e8ad4c5f14 True Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard) c6g.large on-demand i-0a3ff5bfd7be551e2 True Bottlerocket (EKS Auto, Standard) 2025.7.18 (aws-k8s-1.32-standard) c6g.large on-demand i-029d33a1259f77564 True Bottlerocket (EKS Auto, Nvidia) 2025.7.25 (aws-k8s-1.32-nvidia) g6e.xlarge spot In this case Karpenter determined that the G6e xlarge spot instance is the most cost-efficient instance type that adheres to the constraints defined in the NodePool.\nStep 5. Test the model endpoint\nFirst, execute a port forward to the gptoss-service service using kubectl:\nkubectl port-forward service/gptoss-service 8000:8000 In another terminal, send a test prompt using curl:\ncurl http://localhost:8000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;openai/gpt-oss-20b\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is machine learning?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: 100 }\u0026#39; | jq -r \u0026#39;.choices[0].message.content\u0026#39; Sample output:\n**Machine learning (ML)** is a branch of computer science that gives computers the ability to learn from data, identify patterns, and make decisions or predictions without being explicitly programmed to perform each specific task. This setup allows you to test and interact with your inference server without exposing it externally. To make it accessible to other applications or users, you can update the service type to LoadBalancer, for either external access or within your VPC. If exposing the service, then make sure to implement appropriate access controls such as authentication, authorization, and rate limiting.\nStep 6. Cleaning up\nWhen you have finished your experiments, you must clean up the resources you created to avoid incurring ongoing charges. To delete the cluster and all associated resources managed by EKS Auto Mode, run the following command:\neksctl delete cluster --name=$CLUSTER_NAME --region=$AWS_REGION This command removes the entire EKS cluster along with its control plane, data plane nodes, NodePools, and all resources managed by EKS Auto Mode.\nReducing model cold start time in AI inference workloads As you saw in the preceding section, it took a few minutes for the container image to download, the model to be fetched, and the weights to load into GPU memory. This delay is often caused by large container images (over 14 GB in this case), model downloads from external sources, and the time needed to load the model into memory, adding latency to pod startup and scaling events. In production scenarios, especially when running inference at scale, you must use Kubernetes autoscaling and minimize this startup time to make sure of fast, responsive scaling. In this section, we walk through techniques to optimize model startup time and reduce cold start delays.\nStore vLLM container image in Amazon ECR and use a VPC endpoint: Pulling container images from public registries over the internet introduces latency during pod startup, especially when images are large or network bandwidth is constrained. To reduce this overhead:\nStore your container image in Amazon Elastic Container Registry (Amazon ECR), a fully managed container registry that is regionally available and optimized for use with Amazon EKS. Configure an Amazon ECR VPC endpoint so that nodes pull images over the AWS backbone rather than the public internet. Prefetch model artifacts using AWS Storage options: To reduce the startup time introduced by model downloads and loading from Hugging Face, store the model artifacts in an AWS storage option that supports concurrent access and high-throughput reads across multiple nodes and AWS Availability Zones (AZs). This is essential when multiple replicas of your inference service, possibly running across different nodes or AZs, need to read the same model weights simultaneously. Shared storage avoids the need to download and store duplicate copies of the model per pod or per node.\nAmazon S3 with Mountpoint and S3 Express One Zone: Express One Zone stores data in a single AZ, although it can be accessed from other AZs in the same Region. This is the lowest-cost storage option and most direct to set up. It is ideal for general-purpose inference workloads where performance requirements are moderate and directness is key. For best results, configure a VPC endpoint to make sure that traffic stays within the AWS network. Amazon Elastic File System (Amazon EFS): A natively multi-AZ service that automatically replicates data across AZs. Amazon EFS is an easy-to-use shared file system that offers a good balance of cost, latency, and throughput. It is suitable for workloads that need consistent access to models from multiple AZs with built-in high availability. Amazon FSx for Lustre: Deployed in a single AZ and accessible from other AZs within the same VPC. It delivers the highest-performance option for shared storage. Although FSx for Lustre may have a higher storage cost, its speed in loading model weights can reduce overall GPU idle time, often balancing out the cost while providing the fastest model loading performance. Separating model artifacts from container images, storing your containers in Amazon ECR, and choosing the right storage backend for your models, such as Amazon S3 Mountpoint, Amazon EFS, or Amazon FSx for Lustre allows you to significantly reduce startup time and improve the responsiveness of your inference workloads. To explore more strategies for optimizing container and model startup time on Amazon EKS, refer to the AI on Amazon EKS guidance.\nConclusion Amazon EKS Auto Mode streamlines running GPU-powered AI inference workloads by handling cluster provisioning, node scaling, and GPU configuration for you. Dynamic autoscaling through Karpenter, pre-configured AMIs, and built-in GPU monitoring and recovery enable you to deploy models faster—without needing to configure or maintain the underlying infrastructure.\nTo further explore running inference workloads on EKS Auto Mode, the following are a few next steps:\nLearn more: Visit the EKS Auto Mode documentation for full capabilities, supported instance types, and configuration options. You can also check out the EKS Auto Mode post for a hands-on introduction. Get hands-on experience: Join an instructor-led AWS virtual workshops from the Amazon EKS series, featuring dedicated sessions on Auto Mode and AI inference. Explore best practices: Review the Amazon EKS best practices guide for AI/ML workloads. Plan for scale and costs: If you’re running LLMs or other high-demand GPU workloads, then connect with your AWS account team for pricing guidance, right-sizing recommendations, and planning support. AWS recently announced up to a 45% price reduction on NVIDIA GPU-accelerated instances, including the P4d, P4de, and P5, so it’s a good time to evaluate your options. These tools and guidance allow you to run inference workloads at scale with less effort and more confidence.\nAbout the authors Shivam Dubey is a Specialist Solutions Architect at AWS, where he helps customers build scalable, AI-powered solutions on Amazon EKS. He is passionate about open-source technologies and their role in modern cloud-native architectures. Outside of work, Shivam enjoys hiking, visiting national parks, and exploring new genres of music.\nBharath Gajendran is a Technical Account Manager at AWS, where he empowers customers to design and operate highly scalable, cost-effective, and fault-tolerant workloads utilizing AWS. He is passionate about Amazon EKS and open-source technologies, and specializes in enabling organizations to run and scale AI workloads on EKS.\nChristina Andonov is a Sr. Specialist Solutions Architect at AWS, helping customers run AI workloads on Amazon EKS with open source tools. She’s passionate about Kubernetes and known for making complex concepts easy to understand.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Enforcing organization-wide Amazon S3 bucket-tagging policies In today’s complex cloud environments, maintaining consistent resource tagging is a critical challenge faced by organizations of all sizes. Proper resource tagging is essential for cost allocation, security compliance, operational management, and maintaining governance at scale. However, enforcing tagging standards across distributed teams and numerous resources can be difficult, especially when dealing with rapid deployment cycles and multiple stakeholder requirements. Organizations often struggle to maintain consistent tagging practices, which leads to poor resource visibility, inaccurate cost attribution, and challenges in maintaining compliance with internal policies.\nAWS provides several capabilities to help organizations implement and maintain effective tagging governance for Amazon S3, which offers cloud storage that serves as the foundation for countless workloads across development, analytics, backup, content delivery use cases, and more. As storage deployments grow and become more distributed across teams and projects, they often become a prime example tagging challenges. Through a combination of proactive and reactive controls, organizations can establish automated mechanisms to make sure that resources meet tagging requirements. These controls can be implemented across an entire AWS Organization, providing centralized oversight while maintaining the flexibility needed for different business units and workloads.\nIn this post, I demonstrate how to implement a reactive automated tagging governance solution for S3 buckets using AWS Config, AWS Lambda, and Amazon EventBridge. You can learn how to deploy a solution that automatically restricts object uploads to non-compliant buckets and removes these restrictions when the necessary tags are applied. This approach enables organizations to enforce tagging standards without manual intervention, make sure of consistent resource categorization, and maintain accurate cost allocation—all while minimizing impact on development teams and existing workflows.\nSolution overview This solution takes a reactive approach to tagging governance by using AWS Config compliance service and using the “required-tags” AWS Config managed rule. This rule monitors and identifies resources where necessary tags are missing. Then, this rule applies a resource policy on buckets that are non-compliant to block new object uploads. When the necessary tags are applied by the bucket owners, the solution automatically removes the policy, lifting the object upload restriction. With minimal adjustment to the configurations, this solution could be extended to be used to enforce tagging for other types of AWS resources.\nThe solution uses a hub and spoke model. The compliance monitoring is done at the account and AWS Region level. Then, compliance status changes are routed to a centralized account that is responsible for taking action when an S3 bucket goes from compliant to non-compliant state or the reverse. This solution uses AWS Config to evaluate the compliance status and to detect changes, EventBridge to communicate the changes, and Lambda to take automated remediation action.\nThe following diagram represents the components that are deployed as part of this solution:\nFigure 1: Architecture diagram of the AWS resources deployed by the CloudFormation templates into management and linked AWS accounts to support this solution\nEach linked account where this solution is deployed has an AWS Config rule deployed that monitors the tagging compliance of the S3 buckets. Each linked account/Region also has an EventBridge rule that forwards the notification about compliance change to the solution management (or designated orchestration) account. The solution management account has the centralized EventBridge bus, a rule, and a Lambda function that update the S3 bucket policy according to the tagging compliance status. If you are looking to enforce tagging on the management (or designated orchestration) account along with using it for orchestration, then make sure to deploy both CFN templates in that account. In that scenario, all components represented in the preceding figure are deployed in that account.\nThis solution also recommends that you use our proactive governance capability with AWS Organization tag policies (optional). Tag policies allow you to enforce consistency in how tag keys are named and which values are allowed to be assigned. AWS CloudFormation templates provided with this post do not deploy tag policies, and you must configure them separately.\nMonitoring: An AWS Config rule in each account and AWS Region within your organization monitors S3 buckets for adherence to the tagging policy that you define in the rule. When the compliance status changes, the rule sends an event to the Default EventBridge bus in that account. Event forwarding: The EventBridge rule in each individual account picks up that event and forwards it to a centralized custom EventBridge bus hosted in the management(or designated orchestration) account. Processing: A rule set up on the custom EventBridge bus in the management (or designated orchestration) account picks up the event and sends it for processing to a Lambda function. The rule also records the event in Amazon CloudWatch Logs for record keeping. Policy application: The Lambda function checks if the S3 bucket compliance status is “Non-compliant”, and updates the bucket resource policy to prevent any objects from being uploaded to it with the following policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;BlockFileUploadToBucket\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::\u0026lt;bucket name\u0026gt;/*\u0026#34; } ] } The Lambda function in the management (or designated orchestration) account updates the bucket policy in the linked account. It does this by assuming a role that is deployed through StackSets to each target account.\nRemediation: When the bucket becomes “Compliant” again, when the necessary tags are applied the S3 bucket, the Lambda function reverses the restriction on uploading objects by removing the resource bucket policy. Deployment process To deploy the solution, use two CloudFormation templates provided in this GitHub project. One of the templates is intended to be deployed in the management account or another account designated to act as a centralized account for the solution. The second CFN template is to be deployed in each account and AWS Region where tagging enforcement is needed. This could be done directly using CloudFormation Stacks or using CloudFormation StackSets from the Organization management account.\nThe s3-tagging-governance-mngt.yaml CloudFormation template deploys the components intended to run in your management (or designated orchestration) account.\nYou can deploy this template in your management account or any other account that serves as the designated orchestration account and is responsible for managing non-compliance notifications and taking remediation actions. Deploy in a single Region where most of your resources are deployed. Use CloudFormation Stacks to deploy this template. As part of deployment, you must provide the OrgID for your Organization. Obtain this by going to the Organizations console -\u0026gt; Settings. The Organization ID is listed under Organization Detail. When it is successfully deployed, go to the Outputs tab of the deployed CFN Stack, and note the following values (you must specify them when deploying the s3-tagging-governance-linked.yaml template: a. CentralEventBusArn: contains the Amazon Resource Name (ARN) of the deployed central event bus. b. LambdaAssumeRole: contains thename of the role that Lambda assumes while executing to deploy resource policy update in each linked account. c. LambdaMngtExecutionRoleArn: contains the ARN of the Lambda execution role deployed in the management (or designated orchestration) account. The s3-tagging-governance-linked.yaml CloudFormation template is to be deployed into accounts and regions where you want to implement tagging governance.\nDeploy using CloudFormation StackSets or regular CFN Stacks. Deploy in each account and AWS Region where you want to enforce tagging. When deploying this template, use the values that you captured when deploying s3-tagging-governance-mngt.yaml template: a. ExecLambdaRoleName: use the output parameter value that contains the name of the role that Lambda assumes while executing to deploy the resource policy update in each linked account b. MngtEventBusArn: use the output parameter that contains the ARN of the event bus deployed in the management (or designated orchestration) account. c. MngtLambdaRoleArn: use the output parameter that contains the ARN of the Lambda execution role deployed in the management (or designated orchestration) account Make sure to deploy this template in the solution management (or designated orchestration) account (along with s3-tagging-governance-mngt.yaml) if you are also looking to enforce tagging compliance for S3 buckets in that account. CloudFormation templates do not deploy tag policies, but you can define them by following the instructions here: AWS Organizations – Tag Policies.\nA few important considerations The solution as it is packaged uses tags to determine which objects are in scope for the AWS Config rule. The tag that the Config rule looks for has a “TagsRequired-PLEASE-UPDATE” key and a value of “Yes”. Prior to deploying the solution, make sure to update CloudFormation templates to change the tag to something that is used to determine if the bucket should be governed by this policy or use a different technique to define scope for the AWS Config Rule (resource for instance). CAUTION: When deploying this solution, carefully consider which S3 buckets to target (consider the preceding bullet point) and how this impacts existing workloads that are using those buckets. When the solution applies the resource policy on non-compliant buckets, the workloads can no longer upload files to those buckets, which might have a negative effect on your workloads. If your S3 buckets are already using bucket resource polices and a bucket that is non-compliant (does not have the necessary tags) already has the existing policy, then it is updated to include the statement to block the file uploads that this solution applies. Your existing statements aren’t impacted. Lambda code reviewsall statements using statement ID (SID) to determine if the statement with ID “BlockFileUploadToBucket” already exists and adds it if it is not there. It uses the same SID to remove the statement from the policy when the bucket is compliant. To prevent users from manually updating S3 bucket policies, use Service Control Policies (SCPs) or user AWS Identity and Access Management (IAM) polices to lock down the user’s permissions by explicitly denying actions such as “s3:PutBucketPolicy” and “s3:DeleteBucketPolicy”. Customer spotlight: Marriott International The Marriott International Cloud FinOps team relies on resource tags for their financial reporting. These tags are used to identify resource owners and attribute the costs to respective workloads and business units. Although they had a policy in place that needed the teams to use tagging, the compliance rate was very low and a lot of costs were being reported as “uncategorized.” To enforce the policy, the Marriott FinOps team embarked on the project to create tagging governance across most used AWS services used across their teams.\nThey successfully employed Service Control Policies to block the creation of any Amazon Elastic Compute Cloud (Amazon EC2) instance or Amazon Elastic Block Store (Amazon EBS) volumes that did not have the necessary tags. However, when they tried to employ this approach for S3 buckets, it presented challenges. First, they started to come up against the character size limits described earlier in this post, and they discovered that the tag condition is not supported in SCP polices for Amazon S3.\nAt that point, the Marriott FinOps team partnered with their AWS Technical Account Manager and Solution Architect to develop the solution described in this post. After deploying this solution across their Organization, they were able to reduce the footprint of untagged S3 buckets by 80%. This allowed them to properly report the costs associated with Amazon S3 usage to respective resource owners and to the leadership.\nConclusion This solution demonstrates a comprehensive approach to enforcing Amazon S3 bucket tagging compliance using the AWS Config, Amazon EventBridge, and AWS Lambda functions in a hub-and-spoke model. The solution monitors S3 buckets across your AWS Organization, automatically restricts object uploads to non-compliant buckets through resource policies, and removes these restrictions when the necessary tags are applied. Key components of the solution include AWS Config rules for monitoring, EventBridge for event routing, and Lambda functions for automated policy enforcement. All of these are deployable through the provided AWS CloudFormation templates.\nThe benefits of implementing this solution extend beyond tag enforcement. Organizations can achieve better cost allocation and resource management through consistent tagging, as demonstrated by Marriott’s 80% reduction in untagged S3 buckets. This approach overcomes the limitations of SCPs, offering a scalable way to enforce tagging governance without running into SCP character limits or service-specific constraints. Furthermore, the solution’s automated remediation capabilities minimize administrative overhead while making sure of continuous compliance.\nTo get started with implementing this tagging governance solution, download the provided CloudFormation templates from the GitHub repository and follow the detailed deployment instructions. Remember to carefully consider your bucket targeting strategy and existing workload requirements before deployment. For more guidance or customization needs, consult the AWS documentation or reach out to your AWS account team to make sure of a successful implementation that aligns with your organization’s specific requirements.\nTAGS: Amazon EventBridge, Amazon Simple Storage Service (Amazon S3), AWS Cloud Storage, AWS Config, AWS Lambda\nPavel Rabinovich Pavel Rabinovich is a Senior Technical Account Manager at AWS, based in Washington, D.C., with over 25 years of experience in IT and software engineering. He advises enterprise customers in the hospitality, healthcare, and insurance industries on cloud strategy and security. As a member of the AWS Security Technical Field Community, Pavel focuses on identity and application security.\nAvinash Pala Avinash Pala is a Cloud Solution Architect and Data Architect. He is AWS Certified.\nReema Bali Reema Bali is \u0026ldquo;Director - Enterprise FinOps\u0026rdquo; at Marriott International responsible for bridging the gap between Engineering and Finance, ensuring cloud spending is optimized without sacrificing speed or innovation. She is a seasoned IT leader with over 25 years of experience and a proven track record in driving large-scale, high-impact programs that align with strategic business goals.\nRuby Siddiqui Ruby Siddiqui is Senior Director of FinOps Engineering and Technology Strategy at Marriott International, leading enterprise-wide cloud financial operations and optimization initiatives. With expertise in cloud economics, Application development, observability, and governance, Ruby drives scalable FinOps practices that deliver cost transparency and business value across multi-cloud environments.\nSri Gudavalli Sri Gudavalli is a Solutions Architect with AWS helping Enterprise customers with their cloud migration and modernization journey. He works with Enterprise customers from the US-East Region to build leading edge cloud applications and services on AWS.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Event Report: “AWS Cloud Day Vietnam – AI Edition 2025” Event Name: Vietnam Cloud Day 2025\nDate: 18/09/2025\nLocation: 2 Hai Trieu Street, District 1, Ho Chi Minh City\nRole: Attendee\n1. Event Objectives The event focused on accelerating Vietnam’s digital transformation through the combined power of Cloud Computing and Artificial Intelligence, centered around four key directions:\nDemocratizing GenAI for enterprises: Transforming generative AI from a trend into real-world applications, emphasizing the importance of a comprehensive data strategy.\nBridging business and IT: Especially in the financial sector, cloud is seen as a foundation for business value creation.\nIndustry-specific modernization: Sharing success stories from Honda Vietnam, Xanh SM, Masterise Group.\nEnhancing security \u0026amp; resilience: Encouraging a “security by design” mindset throughout the application lifecycle.\n2. Speakers The event gathered a full lineup of 24 speakers, ranging from high-ranking government officials to C-level executives and technical experts.\nSpeaker Name Title Organization H.E. Pham Duc Long Vice Minister of Science and Technology Ministry of Science and Technology H.E. Marc E. Knapper U.S. Ambassador to Vietnam U.S. Embassy in Vietnam Jaime Valles VP, Managing Director APJ AWS Jeff Johnson Managing Director, ASEAN AWS Dr. Jens Lottner CEO Techcombank Dieter Botha CEO TymeX Trang Phung CEO U2U Network Vu Van Co-founder \u0026amp; CEO ELSA Corp Nguyen Hoa Binh Chairman Nexttech Group Gia Hieu Dinh CIO F88 Nguyen Hong Phuong Huy Head of Cloud Infrastructure \u0026amp; Cybersecurity Masterise Group Nguyen Vu Hoang Head of Technology VTV Digital Ha Anh Van Head of IT Solutions Honda Vietnam Nguyen Tuan Huy Director of Digital Transformation Mobifone Minh Hoang Chief Data Officer Techcom Securities Vincent Nguyen Managing Director Nam Long Commercial Property Seunghoon Chae CEO MegazoneCloud Vietnam Uy Tran Co-founder \u0026amp; COO Katalon Thai Huy Chuong Head of Application Development Bao Viet Holdings Tran Dinh Khiem Director of Digital Banking Techcombank Christopher Bennett CTO TymeX Selma Belhadjamor Principal Data Scientist Onebyzero Ngo Manh Ha Co-CEO, CTO TechX Corp Nguyen Thanh Binh Head of DevOps Renova Cloud 3. Key Content Highlights 3.1 Policies \u0026amp; Leadership Vietnam and the U.S. reaffirmed their commitment to supporting digital infrastructure. Panel discussions emphasized the importance of people and culture in corporate innovation. 3.2 Financial Sector – New Banking Models Techcombank and Bao Viet presented Ecosystem Banking strategies. TechX introduced XGenAI, a platform enhancing customer experience in financial services. 3.3 Cross-Industry Modernization Honda Vietnam shared its SAP migration journey to AWS. VTV Digital and Mobifone discussed their digital transformation pathways from vision to execution. Masterise Group explained its strategy for migrating VMware workloads to AWS. 3.4 Data, AI \u0026amp; DevOps Data experts emphasized the need for high-quality data in GenAI initiatives. Katalon and Renova Cloud demonstrated how GenAI accelerates QA and DevOps automation. 4. Key Lessons Learned 4.1 Mindset Orientation Technology must serve business goals. Resilience must be built from the foundation. 4.2 Technical Architecture Generative AI is only effective when supported by a clear enterprise data strategy. Flexible modernization paths: microservices, serverless, VMware replatforming, etc. 4.3 Operational Strategy The focus is not only “moving to the cloud” but “operating with continuous innovation.” Financial institutions are shifting toward Open Banking and API-driven ecosystems. 5. Practical Applications Conduct data quality audits before initiating GenAI projects. Experiment with GenAI in DevOps to accelerate development cycles. Apply migration learnings from Honda and Masterise to similar system modernization projects. Integrate security throughout the application lifecycle. 6. Personal Experience The event provided a comprehensive view of Vietnam’s digital economy future, integrating strategic vision, technical expertise, and real-world lessons. Insights from Techcombank, Honda, TechX, and Masterise illuminated how enterprises operationalize Cloud and AI at scale. Meanwhile, technical sessions on GenAI in DevOps introduced new approaches to automation and process optimization.\nLessons learned:\nData is the decisive factor for GenAI success. Modernization is a long-term journey. Security must be embedded from the initial design phase. Conclusion:\nThe event highlighted the pivotal role of AWS Cloud and Generative AI in shaping the next phase of Vietnam’s digital economic growth, while offering a clear roadmap for organizations pursuing modernization and sustainable innovation.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding Network Architecture and Building a Basic VPC\nWeek 3: Working with EC2, Security Configuration, and Deploying Web Servers\nWeek 4: Hosting Static Websites with S3 \u0026amp; CloudFront and Introduction to RDS\nWeek 5: Deep-Diving AWS Technical Blogs and Refining the Project Proposal\nWeek 6: Reviewing AWS Security, Load Balancing, and Disaster Recovery Concepts\nWeek 7: Learning DynamoDB, Route53, and Resilient Architecture Patterns\nWeek 8: Performance \u0026amp; Cost Optimization\nWeek 9: Practicing DynamoDB via CLI and Finalizing System Architecture\nWeek 10: Enhancing Cognito Data Attributes and Testing with LocalStack\nWeek 11: Building Data Pipelines and Integrating Cognito with the Database\nWeek 12: Training Personalize Models and Developing the Data Dashboard\nWeek 13: Deploying the Website, Updating the Proposal, and Configuring AWS Infrastructure\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.1-week1/","title":"Worklog Week 1","tags":[],"description":"","content":"Week 1 Objectives: Familiarize with the AWS account and set up basic configurations to ensure cost management, security, and technical support readiness. Begin learning foundational services: AWS Account, AWS Budget, IAM, AWS Support. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Create an AWS account 08/09/2025 08/09/2025 Create a new AWS account. 3 Create Budget 09/09/2025 09/09/2025 COST MANAGEMENT WITH AWS BUDGETS 4 - Learn and create IAM - Create an IAM Admin account for daily tasks 09/09/2025 10/09/2025 AWS Identity and Access Management (IAM) Access Control 5 - Learn AWS Support and submit a Support Case 10/09/2025 11/09/2025 AWS Support Week 1 Achievements: Create AWS Account Understood the structure of an AWS account: root user, IAM users, billing, and security. Root user should only be used for sensitive tasks (such as Billing setup, enabling MFA, financial support). Recognized the importance of MFA based on AWS security principles (Least Privilege \u0026amp; Multi-Factor Authentication). Create AWS Budget Learned how to configure a Cost Budget to monitor expenses and receive email alerts when exceeding thresholds. Understood and distinguished the concepts of Actual cost (real incurred cost) vs Forecasted cost (projected end-of-month cost). Learn IAM and create IAM Admin Understood IAM structure: User - Group - Role - Policy. Distinguished the concepts of IAM User (human/application) and IAM Role (permissions assumed temporarily by services or users). Created an IAM Admin user to replace root for daily operations. Learn AWS Support and submit Support Case Learned the 4 levels of Support: Basic, Developer, Business, Enterprise. Under Free Tier, only Basic Support is available, including: Documentation, forums, basic Trusted Advisor checks Ability to submit Support Cases related to billing Learned how to create a ticket to get familiar with the technical support workflow. Difficulties Encountered Difficulty understanding the structure of IAM Policy JSON (Action, Resource, Effect). Solutions \u0026amp; Lessons Learned Reviewed sample policies in AWS Documentation and used the Visual Editor interface to simplify IAM Policy creation. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.2-week2/","title":"Worklog Week 2","tags":[],"description":"","content":"Week 2 Objectives: Understand core concepts of Amazon VPC to build a secure and best-practice network foundation. Practice creating VPC, Subnets, Route Tables, and Gateways on AWS to understand how networking components operate in practice. Learn how to draw network architecture diagrams using Draw.io. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 4 Learn VPC concepts - Understand Public Cloud/Private Cloud - Distinguish Public Subnet, Private Subnet, and VPN-only Subnet - Route table, Destination/Target - Internet Gateway vs NAT Gateway 16/09/2025 17/09/2025 YouTube 4 Practice creating a VPC on AWS 17/09/2025 17/09/2025 Amazon VPC and AWS Site-to-Site VPN Workshop 6 Draw architecture diagrams with Draw.io 19/09/2025 19/09/2025 How to draw AWS architecture on Draw.io Week 2 Achievements: VPC and the difference between Public Cloud and Private Cloud Public Cloud refers to infrastructure owned and operated by AWS; Private Cloud relates to isolated and secure network environments tailored for enterprise needs. AWS VPC is a Virtual Private Cloud — logically private but still runs on AWS’s public infrastructure. Distinguishing Public Subnet - Private Subnet - VPN-only Subnet Public Subnet: Route Table points to an Internet Gateway (IGW) → resources inside subnet can access the internet directly. Private Subnet: No IGW connection → outbound internet access requires a NAT Gateway or NAT Instance. VPN-only Subnet: Used only for connecting to on-premises environments via a VPN Gateway, without internet access. Route Table - Destination - Target Route Tables define how traffic flows within a VPC. Destination: the network address traffic is directed to. Target: the gateway or component responsible for forwarding traffic to the Destination. Route Table for Public Subnet\nDestination Target 0.0.0.0/0 igw-12345 Explanation:\n→ All outbound internet traffic (0.0.0.0/0) must pass through the Internet Gateway.\nRoute Table for Private Subnet\nDestination Target 0.0.0.0/0 nat-67890 Explanation:\n→ Instances in the Private Subnet must route outbound internet traffic through a NAT Gateway (which does not accept inbound connections).\nInternet Gateway vs NAT Gateway Component Purpose Used in Internet Gateway Allows instances in a public subnet to access the internet and receive inbound traffic Public subnet NAT Gateway Allows instances in a private subnet to initiate outbound internet traffic but blocks inbound traffic Private subnet Drawing architecture with Draw.io Learned how to represent VPC, subnet, AZ, IGW, NAT Gateway, and EC2 using AWS icons. Understood AWS architectural diagram standards. Clear diagrams help convey architecture more effectively in reports and presentations. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.3-week3/","title":"Worklog Week 3","tags":[],"description":"","content":"Week 3 Objectives: Understand the concept of Amazon EC2 and the differences between instance types. Configure basic network security using Security Groups to ensure safe connectivity. Practice deploying EC2 on Linux \u0026amp; Windows, connecting via SSH and RDP. Get familiar with installing Web Servers (LAMP/XAMPP) on EC2 to prepare for upcoming lessons on application architecture. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn EC2 concepts and instance types 22/09/2025 22/09/2025 Module 02-Lab03-04.1 - Create EC2 Instances in Subnets Configure Security Groups for the VPC — set proper inbound/outbound rules for SSH connectivity 22/09/2025 22/09/2025 Practice: Launch and connect to an EC2 instance on AWS — basic operations with EC2 22/09/2025 23/09/2025 Introduction to Amazon EC2 4 Deploy an EC2 Linux instance: - Connect via SSH - Recover access when losing the Key Pair - Install LAMP Web Server 24/08/2025 24/08/2025 5 Deploy an EC2 Windows instance: - Connect via Remote Desktop Protocol (RDP) - Install XAMPP on the Windows instance 24/08/2025 25/08/2025 Week 3 Achievements: EC2 Concepts and Architecture EC2 is a compute service delivered under the IaaS model that allows creating virtual machines (instances) on demand. The EC2 ecosystem includes: Instance Types AMI EBS Volume Key Pair Security Group Elastic IP User Data (bootstrapping scripts) Security Group — the first firewall layer Security Group is a stateful firewall. Inbound rules: control traffic entering the instance. Outbound rules: control traffic leaving the instance. Deploying EC2 Linux Connect via SSH Recovering access when losing the key pair: Create a new key pair Use PuTTYgen to extract and copy the full Public Key from the newly created key pair Stop the instance Edit the instance’s User Data and paste the full cloud-config script with the copied Public Key into the ssh-authorized-keys section for the appropriate user (e.g., ec2-user). The Public Key must start with ssh-rsa Restart the instance Install the LAMP stack (Apache, MariaDB, PHP) Deploying EC2 Windows Connect via RDP (mstsc) Retrieve the Administrator password using the key pair Install and configure XAMPP Access the web server through a browser Difficulties Encountered SSH initially timed out because the Security Group did not allow the correct port. When installing LAMP, Apache failed to start due to changes in Amazon Linux 2023, where “amazon-linux-extras” has been replaced by “distro-stream repositories”. Solutions \u0026amp; Lessons Learned Double-check Security Group rules to ensure necessary ports are open. Stop EC2 instances when not in use to avoid unnecessary charges. Verify the Amazon Linux version and run appropriate commands. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.4-week4/","title":"Worklog Week 4","tags":[],"description":"","content":"Week 4 Objectives: Understand and practice deploying static website hosting using Amazon S3 combined with CloudFront. Become familiar with using AWS CLI via VSCode to interact with AWS services more efficiently and professionally. Practice creating and managing Amazon RDS (MySQL). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Understand the basic concepts of S3 and CloudFront 29/09/2025 29/09/2025 Practice: - Turn an S3 bucket into a static website - Configure access for the website - Create CloudFront distribution - Practice Bucket Versioning 29/09/2025 30/09/2025 with Amazon S3 3 Connect AWS with VSCode and use CLI to work with AWS services 30/09/2025 30/09/2025 4 Practice: Create RDS instances and work with MySQL on AWS 01/10/2025 01/10/2025 Amazon Relational Database Service (Amazon RDS) Week 4 Achievements: Basic understanding of S3 and static website hosting mechanism S3 is an object storage service suitable for hosting static websites (HTML/CSS/JS). Static Website Hosting requires enabling the feature and specifying index.html \u0026amp; error.html. The S3 bucket must be configured for public access or served through CloudFront for secure distribution. Practicing static website deployment on S3 Upload web source files to S3 Enable static hosting Add bucket policy to allow public read access Test access via the S3 endpoint URL Creating and configuring CloudFront Distribution Origin = S3 bucket Behavior: caching settings, TTL, HTTP/HTTPS Distribution domain name Use Invalidations when updating website content CloudFront provides:\nImproved content delivery speed Hiding direct S3 access from end users HTTPS support via ACM Certificate Connecting AWS with VSCode + AWS CLI Setup steps:\nInstall AWS Toolkit for VSCode Configure credentials (aws configure) Operate S3, EC2, RDS using CLI commands Practicing Amazon RDS (MySQL) Create an RDS MySQL instance Configure subnet group and security group Connect using MySQL Workbench Create databases \u0026amp; run basic SQL commands "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.5-week5/","title":"Worklog Week 5","tags":[],"description":"","content":"Week 5 Objectives: Deepen understanding of AWS services through blog translation. Define the group project topic and assign tasks to team members. Learn how to use the AWS Pricing Calculator to estimate project costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Translate Blog 1: Building geolocation verification for iGaming and sports betting on AWS Translate Blog 2: How to run AI model inference with GPUs on Amazon EKS Auto Mode Translate Blog 3: Enforcing organization-wide Amazon S3 bucket-tagging policies 06/10/2025 08/10/2025 Geolocation Verification for iGaming \u0026amp; Sports Betting Running AI Model Inference with GPUs on Amazon EKS Auto Mode Organization-wide S3 Tagging Governance 3 Finalize project topic and assign tasks to team members 07/10/2025 07/10/2025 5 Use AWS Pricing Calculator to estimate project cost 09/10/2025 09/10/2025 7 Write the project Proposal 11/10/2025 13/10/2025 Week 5 Achievements: Blog 1 – Geolocation Verification for iGaming \u0026amp; Sports Betting on AWS Key knowledge learned:\nUnderstood how to enforce location-based access control using Route 53 Geolocation and CloudFront geo-restriction. Learned about risks such as VPN or proxy spoofing and mitigation methods using SDKs and device-level location verification. Recognized the importance of compliance requirements for industries operating under strict legal geographic boundaries. Blog 2 – How to run AI model inference with GPUs on Amazon EKS Auto Mode Understood how EKS Auto Mode automatically provisions GPU nodes based on workload demand (powered by Karpenter). EKS Auto Mode supports optimized GPU AMIs, automatic node repair, NMA monitoring, and DCGM. The workflow for deploying AI inference workloads (e.g., vLLM) on GPU nodes becomes significantly simpler compared to self-managed EKS clusters. Blog 3 – Organization-wide S3 Tagging Governance Learned how to use the AWS Config “required-tags” rule for organization-wide compliance enforcement. When a bucket is non-compliant, Lambda automatically applies a deny-upload policy; once compliant, the restriction is removed. Understood EventBridge forwarding from member accounts to the management account for centralized governance. AWS Pricing Calculator Estimated the cost of EC2, Lambda, API Gateway, RDS, CloudFront, and S3. Identified the services contributing the highest costs within the architecture. Learned how to adjust instance types, regions, and storage options to optimize spending. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Objectives: Strengthen understanding of Secure Architecture and AWS core security mechanisms. Understand how Elastic Load Balancing works and the different routing methods. Master Disaster Recovery strategies in AWS and distinguish RTO/RPO. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Review: Secure Architecture - Understand IAM (User, Group, Policy, Role, Least Privilege) - Types of MFA for account protection - Compare Security Group vs NACL 13/10/2025 13/10/2025 3 Learn Elastic Load Balancing concepts: - Application Load Balancer (ALB) - Network Load Balancer (NLB) - Open Systems Interconnections - Routing types of ALB and NLB - Sticky Session / Session Affinity 14/10/2025 14/10/2025 4 Study Key Management Service, AWS Certificate Management, and encryption/key storage 15/10/2025 15/10/2025 5 Learn Disaster Recovery Strategies: - Backup \u0026amp; Restore - Pilot Light - Warm Standby - Multi-Site Active-Active Distinguish RTO and RPO 16/10/2025 16/10/2025 Week 6 Achievements: Secure Architecture Key points consolidated:\nIAM includes User, Group, Policy, Role; apply the Least Privilege principle. MFA protects accounts with methods like Virtual MFA, Hardware MFA, and SMS MFA. Security Group: stateful firewall (automatically allows response traffic). NACL: stateless, applied at subnet level, suitable for deny rules. SG controls instance-level traffic → more granular; NACL protects at subnet layer → more general. Elastic Load Balancing – ALB, NLB, and routing ALB operates at Layer 7, supports HTTP/HTTPS, and routing by path, host, header, or query string. NLB operates at Layer 4, optimized for extremely high throughput and millions of requests per second, suitable for TCP/UDP. OSI model clarifies ALB (L7) vs NLB (L4). Understood routing mechanisms: weighted, host-based, path-based (ALB) and TCP/UDP routing (NLB). Sticky Session / Session Affinity uses cookies to maintain user connections to the same target. KMS \u0026amp; ACM – Encryption and key management AWS KMS manages creation, storage, and lifecycle of encryption keys (CMK). Supports encryption across S3, EBS, RDS, and many services via envelope encryption. Distinguish AWS-managed keys vs Customer-managed keys (CMK). AWS Certificate Manager (ACM) provides free SSL/TLS certificates with automatic renewal. Combining ACM with CloudFront/ALB ensures end-to-end HTTPS. Disaster Recovery Strategies Strategy Description Recovery Level Backup \u0026amp; Restore Backup data and restore during disaster Low Pilot Light Keep the minimal core system running Medium Warm Standby A smaller-scale version running in parallel High Multi-Site Active-Active Multiple active sites simultaneously Very high RTO (Recovery Time Objective): acceptable downtime duration. RPO (Recovery Point Objective): maximum acceptable data loss during an incident. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Objectives: Understand and practice using DynamoDB. Learn how to use Route 53 to host websites and manage DNS. Deepen knowledge of Resilient Architecture and disaster recovery strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn about DynamoDB and its components 20/10/2025 20/10/2025 Amazon DynamoDB 3 Practice creating DynamoDB on AWS and learn how to host a website using Route 53 21/10/2025 21/10/2025 4 Review: Resilient Architecture - Multi AZ, Multi Region - Disaster Recovery Strategies (Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site Active-Active) - Health Check - Load Balancing - AWS services that help recover systems quickly 22/10/2025 25/10/2025 7 Draw system architecture for the project and estimate expected traffic usage 25/10/2025 26/10/2025 Week 7 Achievements: DynamoDB DynamoDB is a NoSQL key-value database with near-infinite scalability. Key components include: Partition Key / Sort Key GSI (Global Secondary Index) LSI (Local Secondary Index) Provisioned / On-Demand Capacity Hosting a website with Route 53 How to purchase and configure a domain with Route 53. Create Hosted Zones and Records: A, CNAME. Pointing domain to: S3 Static Website CloudFront Distribution EC2 Elastic IP Understanding DNS propagation and TTL. Resilient Architecture Concepts Multi-AZ: Automatically fails over when an AZ encounters issues. Multi-Region: Backup strategy for systems requiring extremely high uptime or global distribution. Disaster Recovery Strategies Strategy Complexity Cost Recovery Time Backup \u0026amp; Restore Low Low Longest Pilot Light Medium Medium Moderate Warm Standby High High Fast Multi-Site Active/Active Very high Very high Near-zero downtime RTO \u0026amp; RPO:\nRTO: Maximum allowable downtime. RPO: Maximum acceptable data loss. Health Check: Route 53 monitors endpoint health → fails over to backup site if failures occur.\nLoad Balancing: ALB/NLB distribute traffic and improve fault tolerance.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.8-week8/","title":"Worklog Week 8","tags":[],"description":"","content":"Week 8 Objectives: Reinforce knowledge of High-Performing Architectures to optimize system performance. Master cost optimization techniques in AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Review: High Performing Architectures - Compute Scaling - Storage Optimization - Caching - Network Optimization 27/10/2025 27/10/2025 3 Review: Cost Optimized Architecture - Budget - Saving Plans - Reserved Instance 28/10/2025 28/10/2025 4 Consolidate knowledge, solve AI-generated practice questions, and attempt SSA mock tests 29/10/2025 30/10/2025 6 Review and take the midterm exam 31/10/2025 31/10/2025 Week 8 Achievements: High-Performing Architectures Compute Scaling:\nDistinguished vertical scaling vs horizontal scaling. Understood Auto Scaling Groups, scaling policies, and the mechanism to increase/decrease EC2 capacity based on system load. Storage Optimization\nUnderstood S3 tiering mechanisms (S3 Standard → IA → Glacier) to balance performance and cost. Caching\nCaching reduces backend load and improves application performance. Leveraged Amazon CloudFront and ElastiCache (Redis/Memcached) to reduce latency. Network Optimization\nUnderstood the importance of VPC Endpoints, enhanced networking, and data path optimization. Used CloudFront to reduce global latency. Cost-Optimized Architectures AWS Budget: Configure alerts based on actual or forecasted spending.\nSaving Plans:\nCompute Saving Plans are flexible (support EC2, Fargate, Lambda). EC2 Instance Saving Plans offer deeper discounts but with less flexibility. Reserved Instances (RI):\nCommit 1–3 years to reduce costs for EC2/RDS. Suitable for stable and predictable workloads. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives: Become proficient in working with DynamoDB using AWS CLI and understand NoSQL table structures. Generate test data using AI to validate the system. Finalize the overall project architecture, select appropriate AWS services, and estimate deployment costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice writing CLI commands to create DynamoDB tables - Use AI to generate test data for the system 03/11/2025 03/11/2025 3 - Discuss and finalize AWS services for the project, redraw the architecture, estimate cost with Pricing Calculator, assign tasks and roles for team members 04/11/2025 04/11/2025 4 - Create mock data for the project, refine features to align with AWS Personalize requirements 05/11/2025 07/11/2025 Week 9 Achievements: Working with DynamoDB using AWS CLI Learned how to create DynamoDB tables with CLI, defining Partition Key and Sort Key. Practiced using commands to add items (put-item), retrieve items (get-item), query data (query), and scan tables (scan). Gained a clear understanding of JSON structure and how to map data in a NoSQL environment. Difficulties Encountered DynamoDB CLI syntax is lengthy and JSON formatting is easy to get wrong. Initial mock data lacked logical consistency, leading to poor results when training with Amazon Personalize. Solutions \u0026amp; Lessons Learned Used JSON template files to avoid missing fields or incorrect formatting. Adjusted mock data to follow realistic user behavior patterns. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Create Amazon RDS","tags":[],"description":"","content":"Step 1: Create RDS MySQL Instance Go to RDS Console → Databases → Create database\nChoose database creation method:\nStandard create Engine options:\nEngine type: MySQL Engine version: MySQL 8.0.35 (or latest 8.0.x) Templates:\nFree tier (for workshop/testing) Settings:\nDB instance identifier: daivietblood-db Master username: admin Credentials management: Self managed Master password: YourSecurePassword123! Confirm password: YourSecurePassword123! ⚠️ Important: Save your password securely. You will need it to connect from Lambda.\nStep 2: Instance Configuration Instance configuration: DB instance class: db.t3.micro (Free tier eligible) Storage type: General Purpose SSD (gp2) Allocated storage: 20 GiB Storage autoscaling: Disable (for cost control) Step 3: Connectivity Connectivity:\nCompute resource: Don\u0026rsquo;t connect to an EC2 compute resource Network type: IPv4 Virtual private cloud (VPC): daivietblood-vpc DB subnet group: daivietblood-db-subnet-group Public access: No ⚠️ Important! VPC security group: Choose existing Existing VPC security groups: daivietblood-rds-sg Availability Zone: ap-southeast-1a Database port:\nDatabase port: 3306 Step 4: Database Authentication Database authentication: Password authentication Step 5: Additional Configuration Database options:\nInitial database name: daivietblood DB parameter group: default.mysql8.0 Option group: default:mysql-8-0 Backup:\nEnable automated backups: Yes Backup retention period: 7 days Backup window: No preference Encryption:\nEnable encryption: Yes (default) Monitoring:\nEnable Enhanced monitoring: No (to reduce cost) Maintenance:\nEnable auto minor version upgrade: Yes Maintenance window: No preference Deletion protection:\nEnable deletion protection: No (for workshop) Click Create database\nℹ️ RDS creation takes 10-15 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 6: Get RDS Endpoint After RDS is available:\nGo to RDS Console → Databases → Click daivietblood-db\nIn Connectivity \u0026amp; security tab, copy:\nEndpoint: daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com Port: 3306 Save these values for Lambda configuration:\nDB_HOST=daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com DB_PORT=3306 DB_NAME=daivietblood DB_USER=admin DB_PASSWORD=YourSecurePassword123! Step 7: Create Database Schema Since RDS is in Private Subnet, you need to connect via a bastion host or use Lambda to initialize the schema.\nOption A: Using Lambda to Initialize (Recommended)\nCreate a one-time Lambda function to initialize the database:\n// init-db.js const mysql = require(\u0026#39;mysql2/promise\u0026#39;); exports.handler = async (event) =\u0026gt; { const connection = await mysql.createConnection({ host: process.env.DB_HOST, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); // Create tables const createUsersTable = ` CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, email VARCHAR(255) UNIQUE NOT NULL, name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, phone VARCHAR(20), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; const createDonationsTable = ` CREATE TABLE IF NOT EXISTS donations ( id INT AUTO_INCREMENT PRIMARY KEY, user_id INT NOT NULL, donation_date DATE NOT NULL, location VARCHAR(255), status ENUM(\u0026#39;scheduled\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;scheduled\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (user_id) REFERENCES users(id) ) `; const createEmergencyRequestsTable = ` CREATE TABLE IF NOT EXISTS emergency_requests ( id INT AUTO_INCREMENT PRIMARY KEY, requester_name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, units_needed INT NOT NULL, hospital VARCHAR(255) NOT NULL, urgency ENUM(\u0026#39;critical\u0026#39;, \u0026#39;urgent\u0026#39;, \u0026#39;normal\u0026#39;) DEFAULT \u0026#39;normal\u0026#39;, status ENUM(\u0026#39;open\u0026#39;, \u0026#39;fulfilled\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;open\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; await connection.execute(createUsersTable); await connection.execute(createDonationsTable); await connection.execute(createEmergencyRequestsTable); await connection.end(); return { statusCode: 200, body: JSON.stringify({ message: \u0026#39;Database initialized successfully\u0026#39; }) }; }; Verification Checklist RDS instance created and status is \u0026ldquo;Available\u0026rdquo; RDS is in Private Subnet (Public access: No) RDS Security Group only allows access from Lambda SG Endpoint and credentials saved securely Initial database daivietblood created Database schema initialized (tables created) Troubleshooting Issue Solution Cannot connect to RDS Verify Security Group allows inbound from Lambda SG RDS creation failed Check Service Quotas for RDS instances Connection timeout Ensure Lambda is in same VPC and has NAT Gateway access "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create API Gateway","tags":[],"description":"","content":"Step 1: Create REST API Go to API Gateway Console → Create API\nChoose API type:\nREST API → Build Create new API:\nProtocol: REST Create new API: New API API name: daivietblood-api Description: REST API for DaiVietBlood system Endpoint Type: Regional Click Create API\nStep 2: Create Resources 2.1. Create /users Resource\nSelect root / → Actions → Create Resource\nConfigure:\nResource Name: users Resource Path: users Enable API Gateway CORS: ✅ Check Click Create Resource\n2.2. Create /emergency-requests Resource\nSelect root / → Actions → Create Resource\nConfigure:\nResource Name: emergency-requests Resource Path: emergency-requests Enable API Gateway CORS: ✅ Check Click Create Resource\nStep 3: Create Methods for /users 3.1. GET /users\nSelect /users → Actions → Create Method → GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-get-users Click Save → OK (to add permission)\n3.2. POST /users\nSelect /users → Actions → Create Method → POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-create-user Click Save → OK\nStep 4: Create Methods for /emergency-requests 4.1. GET /emergency-requests\nSelect /emergency-requests → Actions → Create Method → GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Function: daivietblood-emergency-requests Click Save → OK\n4.2. POST /emergency-requests\nSelect /emergency-requests → Actions → Create Method → POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Function: daivietblood-emergency-requests Click Save → OK\nStep 5: Enable CORS For each resource (/users, /emergency-requests):\nSelect resource → Actions → Enable CORS\nConfigure:\nAccess-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Amz-Date, Authorization, X-Api-Key Access-Control-Allow-Origin: * Click Enable CORS and replace existing CORS headers\nClick Yes, replace existing values\nStep 6: Deploy API Actions → Deploy API\nDeployment stage:\nDeployment stage: [New Stage] Stage name: prod Stage description: Production stage Click Deploy\nCopy the Invoke URL:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod ℹ️ Save this URL. You will need it for frontend configuration.\nStep 7: API Structure Summary After completing, your API structure should look like:\ndaivietblood-api │ ├── /users │ ├── GET → daivietblood-get-users │ ├── POST → daivietblood-create-user │ └── OPTIONS (CORS) │ └── /emergency-requests ├── GET → daivietblood-emergency-requests ├── POST → daivietblood-emergency-requests └── OPTIONS (CORS) Verification Checklist REST API created /users resource created with GET and POST methods /emergency-requests resource created with GET and POST methods CORS enabled for all resources API deployed to prod stage Invoke URL saved "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.2-prerequiste/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have:\n1. AWS Account\nActive AWS Account with Administrator access Recommended: Use IAM User instead of Root account Region: Asia Pacific (Singapore) - ap-southeast-1 2. Local Development Tools\nTool Version Purpose Node.js \u0026gt;= 18.x Run Lambda functions locally npm/yarn Latest Package management AWS CLI \u0026gt;= 2.x Interact with AWS services Git Latest Version control 3. Knowledge Requirements\nBasic understanding of AWS services (VPC, EC2, S3) Familiarity with REST APIs Basic Node.js/JavaScript or Python Basic React knowledge Step 1: Configure AWS CLI Install AWS CLI from AWS CLI Installation Guide\nConfigure credentials:\naws configure Enter your credentials: AWS Access Key ID: [Your Access Key] AWS Secret Access Key: [Your Secret Key] Default region name: ap-southeast-1 Default output format: json Verify configuration: aws sts get-caller-identity Step 2: Create IAM User for Workshop Go to IAM Console → Users → Create user\nUser details:\nUser name: workshop-admin Select: Provide user access to the AWS Management Console Set permissions:\nSelect: Attach policies directly Search and select: AdministratorAccess Create user and save credentials securely\n⚠️ Security Note: After completing the workshop, delete this IAM user or remove AdministratorAccess policy.\nStep 3: Verify Service Quotas Ensure your account has sufficient quotas for:\nService Resource Minimum Required VPC VPCs per Region 1 VPC Subnets per VPC 4 VPC NAT Gateways per AZ 1 RDS DB Instances 1 Lambda Concurrent Executions 10 API Gateway REST APIs 1 S3 Buckets 2 Check quotas at: Service Quotas Console → Select service → View quotas\nStep 4: Prepare Source Code Clone the sample repository: git clone https://github.com/your-repo/daivietblood-workshop.git cd daivietblood-workshop Project structure: daivietblood-workshop/ ├── frontend/ # React application │ ├── src/ │ └── package.json ├── backend/ # Lambda functions │ ├── functions/ │ └── package.json ├── infrastructure/ # CloudFormation templates │ └── templates/ └── README.md Install dependencies: # Frontend cd frontend \u0026amp;\u0026amp; npm install # Backend cd ../backend \u0026amp;\u0026amp; npm install Step 5: Cost Estimation Service Configuration Est. Cost/Day NAT Gateway 1 NAT Gateway ~$1.08 RDS db.t3.micro ~$0.52 Lambda Free Tier $0.00 API Gateway Free Tier $0.00 S3 \u0026lt; 5GB ~$0.01 CloudFront \u0026lt; 1GB transfer ~$0.01 Amplify Build \u0026amp; Host ~$0.50 Total estimated: ~$2-3/day\n💡 Tip: Complete the workshop in 1-2 days and clean up resources immediately to minimize costs.\nChecklist Before Starting AWS Account ready with Administrator access AWS CLI installed and configured Node.js \u0026gt;= 18.x installed Git installed Source code cloned Region set to ap-southeast-1 "},{"uri":"https://bbitmi.github.io/fcj-internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Dai Viet Blood Donation \u0026amp; Emergency System (DaiVietBlood) Implemented by: Skyline Team – FPT University Ho Chi Minh City\nDate: December 7, 2025\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION 1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations ACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN IMPLEMENTATION TEAM RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background: The DaiVietBlood system is designed to serve the community, including voluntary blood donors, patients in need of emergency blood, and healthcare professionals in Vietnam. The primary customers are blood donors, patient families, and medical staff responsible for managing blood inventory and donation schedules. They require a centralized, reliable platform to optimize the matching process between donors and recipients and improve communication during emergencies. In the context of digital health transformation, DaiVietBlood provides a secure, accessible solution to address localized blood shortages.\nBusiness and Technical Objectives: Migrating the DaiVietBlood system from a local/on-premise environment to AWS offers superior advantages:\nBusiness: AWS allows the application to scale flexibly according to the user base, reduces hardware infrastructure operational costs, and ensures consistent performance nationwide. Technical: AWS provides High Availability and medical data security. Adopting a Serverless Architecture (AWS Lambda, API Gateway, Cognito, RDS) simplifies backend management, accelerates development, and reduces maintenance costs. The system integrates comprehensive monitoring (CloudWatch) and adheres to strict security standards. Summary of Key Use Cases:\nRole Key Function Short Description Guest Access Public Information View donation guidelines, blood compatibility charts, and educational articles without logging in. Member Register/Login, Profile Management Create accounts, update personal information and blood type. Book Blood Donation Select time slots and locations for donation. Submit Emergency Request Submit urgent blood requests; the system automatically finds suitable donors. Staff Manage Requests \u0026amp; Inventory Approve emergency requests, confirm donation schedules, update blood stock. Administrator (Admin) System Administration Manage user accounts, configure donation slots, view overview reports. Summary of Partner’s Professional Services: The Skyline Team will provide comprehensive digital transformation services, including assessing the current local application, designing a Cloud-native architecture, and executing the migration of the system to an AWS Serverless environment. We commit to delivering a secure, scalable system accompanied by automated CI/CD pipelines and detailed operational documentation.\n1.2 PROJECT SUCCESS CRITERIA Functionality: 100% of core functions (registration, scheduling, emergency requests, administration) operate stably on AWS with no regression errors. Availability: System achieves Uptime ≥ 99.9%, ensuring continuous 24/7 access. Performance: Application response time improves by at least 30% compared to the local version. Emergency request processing time is reduced by 40%. Cost: Infrastructure costs are optimized by at least 20% thanks to the Serverless model and Auto-scaling. User Experience: UAT acceptance rate reaches a minimum of 95% for all user roles. Security: Full compliance with data encryption, access management (IAM), and API security requirements. Operations: CI/CD pipeline is fully automated with deployment time \u0026lt; 10 minutes. Monitoring system covers 100% of critical services. 1.3 ASSUMPTIONS Technical \u0026amp; Architectural Assumptions:\nSource Code: The current Local application (Frontend \u0026amp; Backend) is functionally complete. The project focuses on Refactoring for the Cloud (Serverless), excluding the development of new features. AWS Region: The entire infrastructure is deployed in Singapore (ap-southeast-1) to optimize latency for users in Vietnam. Note: During the testing phase, due to limited VPC configurations and Free Tier/Student resources, latency may fluctuate (estimated ~3.5s/request). Service Limits: The AWS account uses default limits (Soft limits). Increasing limits to reduce latency will be approved by the Customer when necessary. Third-party Integration: The system uses the Gemini API for AI support features. Access Rights: The Skyline Team is granted Admin access (IAM Role) to provision resources. Operational \u0026amp; Financial Assumptions:\nDomain: The Customer owns the domain name (e.g., daivietblood.com) and DNS configuration rights. Cost: The cost estimate is based on an assumption of approximately 50,000 API requests/month. Actual costs depend on usage levels (Pay-as-you-go). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The DaiVietBlood system utilizes a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nKey Components:\nNetwork Infrastructure (VPC): Public Subnet: Contains Internet Gateway and NAT Gateway. Private Subnet: Contains AWS Lambda and Amazon RDS to isolate and secure data, preventing direct Internet access. Application \u0026amp; Data: Frontend: Hosted on AWS Amplify, distributed via Amazon CloudFront (CDN), and assets stored on S3. Authentication: Amazon Cognito manages identity and issues JWT tokens. API \u0026amp; Compute: Amazon API Gateway receives requests and routes them to AWS Lambda for business logic processing. Database: Amazon RDS stores structured data, located in the Private Subnet. DevOps \u0026amp; Monitoring: CI/CD: Uses AWS CodePipeline, CodeBuild, CodeDeploy to automate the deployment process. Monitoring: Amazon CloudWatch centrally collects logs and metrics. 2.2 TECHNICAL PLAN The technical implementation process follows the Infrastructure-as-Code (IaC) methodology:\nInfrastructure Automation: Use AWS CloudFormation to provision VPC, Lambda, RDS, and API Gateway, ensuring consistency across environments (Dev/Staging/Prod). Application Development: Refactor backend into modular Lambda functions (NodeJS/Python). Environment variables and sensitive information (DB credentials) are securely encrypted. CI/CD Process: Source (GitHub) -\u0026gt; Build (CodeBuild) -\u0026gt; Deploy (CloudFormation/CodeDeploy). Includes a Manual Approval step before deploying to the Production environment. Testing Strategy: Unit Tests for Lambda, Integration Tests for API, and Load Tests to ensure capacity. 2.3 PROJECT PLAN The project applies the Agile Scrum model over 8 weeks (4 Sprints):\nSprint 1 (Foundation): Set up AWS Account, VPC, RDS. Sprint 2 (Backend Core): Develop Lambda, API Gateway, Cognito. Sprint 3 (Integration): Deploy Frontend (Amplify), finalize CI/CD Pipeline. Sprint 4 (Stabilization): UAT, Performance Optimization, Handover. 2.4 SECURITY CONSIDERATIONS Access Management: Use Cognito for user authentication and IAM Roles for service authorization (Least Privilege). Network Isolation: Database and Lambda are located in the Private Subnet, accessing the Internet only via NAT Gateway. Data Protection: Data encryption At-rest (on RDS/S3) and In-transit (via HTTPS). Security Monitoring: CloudWatch Logs record all activities for auditing and intrusion detection. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Phase Timeline Key Activities Deliverables Estimate (Man-days) Analysis \u0026amp; Design Week 1 Assess Local state, design Cloud architecture, plan migration. SRS Document, Architecture Diagram, API Specs. 5 Local Development Week 2-3 Build backend logic, database schema, local unit tests. Backend Prototype, Database Schema. 10 Frontend \u0026amp; Integration Week 4-5 Develop Frontend, integrate local APIs, prepare code for refactoring. Completed Local Application. 10 AWS Infrastructure Setup Week 6 Write CloudFormation scripts, provision VPC, RDS, IAM. IaC Templates, Secure VPC Environment. 5 Refactor \u0026amp; Deploy Backend Week 7-8 Convert to Lambda, configure API Gateway, Cognito. Serverless Backend active on AWS. 10 Deploy Frontend \u0026amp; CI/CD Week 9-10 Host Frontend on Amplify, set up automated Pipeline. Production URL, CI/CD Pipeline. 10 Testing \u0026amp; Go-live Week 11 UAT, Security Testing, Performance Optimization. UAT Report, Security Report. 5 Handover \u0026amp; Training Week 12 Transfer accounts, operations training, handover documentation. Operations Manual, Acceptance. 5 3.2 OUT OF SCOPE (MVP PHASE) Due to time and resource limitations of the MVP phase, the following items are not included:\nOptimal user search algorithm based on real-time Geo-location (currently using simplified logic). Complex Auto-scaling for the Database layer (currently using basic RDS). Deep Latency Optimization for regions outside Singapore. Advanced Security Compliance standards such as HIPAA/PCI-DSS. 3.3 PATH TO PRODUCTION To upgrade from the current MVP to a large-scale Production system, the following are required:\nEnvironment Strategy: Strictly separate Dev/Staging/Prod environments across different AWS accounts (Multi-account strategy). Database Scaling: Migrate to Amazon Aurora Serverless or use Read Replicas to increase read/write capacity. Enhanced Monitoring: Integrate AWS X-Ray to trace requests and identify performance bottlenecks. Strengthened Security: Deploy AWS WAF with rules to block DDoS and automated bots; use Amazon Inspector for periodic vulnerability scanning. 4. EXPECTED AWS COST BREAKDOWN Region: Asia Pacific (Singapore)\nCategory Service Estimated Configuration Monthly Cost (USD) Network NAT Gateway 1 NAT Gateway (Required for Private Subnet) + Data Processing ~$43.13 VPC Subnets, Security Groups ~$13.14 CloudFront 5GB Data Transfer (Utilizing Free Tier) ~$3.00 Compute Lambda 1,000 requests, 512MB RAM (Free Tier) ~$0.00 API Gateway 1,000 requests ~$0.00 Database RDS db.t3.micro, 20GB Storage ~$21.74 Storage S3 5GB Storage, 200 requests ~$0.14 Hosting Amplify Build \u0026amp; Hosting, WAF enabled ~$16.77 Ops CloudWatch Logs, Metrics, Alarms ~$9.41 CI/CD CodePipeline 1 Active Pipeline ~$1.05 Total ~$108.38 / Month 5. IMPLEMENTATION TEAM Mentor (AWS FCJ): Nguyen Gia Hung - Head of Solutions Architect. Project Manager (PM): Nguyen Duc Lan - Coordination, schedule management, cost optimization, and UAT strategy. Technical Lead: Nguyen Cong Minh - In charge of CI/CD, Infrastructure (CDK), Security, and Lambda. Solution Architect: Do Khang - Serverless architecture design, AI Chatbot integration, Service Policies. Fullstack Developer: Le Hoang Anh - API development, Frontend UI/UX, and application security. Data Engineer: Nguyen Quach Lam Giang - RDS administration, VPC/Subnet design, and CloudWatch monitoring. 6. RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS Time Allocation (Man-hours): The project mobilizes a total of 750 man-hours distributed equally among 5 members across phases: Foundation, Core Development, Data Analysis, Testing, and Handover.\nCost Allocation:\nPersonnel: $0 (Performed by students as part of an internship/capstone project, counted towards academic credits). AWS Infrastructure: ~$15 (Actual costs incurred during dev/test after deducting Credits). Total Project Cost: Highly optimized, relying mainly on internal resources and support from the AWS FCJ program. 7. ACCEPTANCE 7.1 Submission of Deliverables: Upon completion of the \u0026ldquo;Handover\u0026rdquo; phase, the Skyline Team will submit all Source Code, Architecture Documentation, Admin Accounts, and Operations Manuals to the Customer/Mentor.\n7.2 Acceptance Period \u0026amp; Process: The Customer has 05 business days to review and perform UAT. If the product meets the Success Criteria (Section 1.2), the Customer will sign the acceptance confirmation.\n7.3 Defect Remediation: If critical errors arise or features are missing compared to the committed scope, the Skyline Team is responsible for fixing and resubmitting for acceptance as soon as possible.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Report: “AI/ML \u0026amp; Generative AI on AWS” 1. Event Objectives The event provides an overview of AI/ML/GenAI capabilities on AWS, including how to leverage Foundation Models, Prompt Engineering, RAG, and AWS prebuilt AI services to address real-world use cases. It also introduces Amazon Bedrock AgentCore — a new platform for building and operating AI Agents at production scale.\n2. Speakers Lâm Tuấn Kiệt – Sr DevOps Engineer, FPT Software Danh Hoàng Hiếu Nghị – AI Engineer, Renova Cloud Đinh Lê Hoàng Anh – Cloud Engineer Trainee, First Cloud AI Journey Văn Hoàng Kha – Community Builder 3. Key Highlights 3.1 Generative AI on Amazon Bedrock Foundation Models (FMs):\nAWS provides a managed library of large models from providers such as Anthropic, Meta, and OpenAI, enabling users to customize models without training from scratch.\nPrompt Engineering:\nCommon techniques introduced:\nZero-shot: model receives only a task description Few-shot: model learns through a few provided examples Chain-of-Thought: model is guided to explain reasoning step-by-step RAG – Retrieval Augmented Generation:\nA technique that enhances GenAI accuracy by adding external knowledge:\nR – Retrieval: fetch relevant information A – Augmentation: inject retrieved data into the prompt G – Generation: model generates more grounded and accurate responses Applications: contextual chatbots, enterprise search, real-time summarization.\nAmazon Titan Embeddings:\nEmbedding model that converts text into vectors for similarity search and RAG workflows, supporting multilingual scenarios.\nAWS AI Services:\nRekognition, Translate, Transcribe, Textract, Comprehend, Polly, Kendra, Personalize, Lookout, and others reduce development time by offering prebuilt AI capabilities.\nDemo:\nThe AMZPhoto facial recognition app demonstrated practical AI integration into real products.\n3.2 Amazon Bedrock AgentCore – Building Large-Scale AI Agents A new framework supporting the development and operation of AI agents:\nAutomated and scalable workflows Long-term memory and session management Detailed access control and identity management Integrations with Browser Tool, Code Interpreter, Memory Store Supports major frameworks: CrewAI, LangGraph, LlamaIndex, OpenAI Agents SDK Enhanced observability, tracking, and testing 4. Key Takeaways Bedrock is the GenAI hub: centralized access to multiple foundation models Prompt + RAG increases accuracy: contextual data is key to high‑quality output Embeddings enhance search intelligence: Titan Embeddings optimize retrieval AI Services accelerate development: no need to build models from scratch AgentCore simplifies GenAI operations: reduces complexity in scaling, memory, and monitoring 5. Practical Applications Apply RAG for context‑aware GenAI projects. Use AWS AI services to accelerate product development cycles. Improve model performance using learned Prompt Engineering techniques. Consider AgentCore for projects requiring scalable, production-ready AI agents. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives: Finalize the project dataset and add required attributes to support training and user authentication via AWS Cognito. Use LocalStack to simulate AWS services for safe, cost-efficient testing and faster development. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Continue refining the dataset and adding additional features for AWS Cognito 10/11/2025 12/11/2025 5 Test system functionalities using LocalStack to simulate the AWS environment 13/11/2025 14/11/2025 Week 10 Achievements: Testing AWS using LocalStack LocalStack simulates AWS services locally without incurring cost, supporting many key AWS components.\nInstalling and configuring LocalStack\nUse Docker to run LocalStack. Configure AWS CLI to point to LocalStack endpoints instead of real AWS services. Testing system functionalities\nCreate local DynamoDB tables Create test S3 buckets Test backend APIs without deploying to AWS Reduce risk, save deployment time, and minimize testing costs "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Refine the complete dataset for the project to standardize input for backend services. Understand the data flow from Cognito into the database and evaluate suitable storage options (S3 vs RDS vs DynamoDB). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 Refine and generate the complete dataset for the project 18/11/2025 20/11/2025 6 - Understand how Cognito writes data to the database in real time - Consider switching to S3 or RDS for data storage 21/11/2025 21/11/2025 Week 11 Achievements: Cognito User Pool does not automatically write data to a database → However, real-time data syncing can be enabled through:\nPost Confirmation Trigger → Lambda → write to DynamoDB / RDS. Pre Token Generation Trigger → add custom claims. Sync user information via Cognito Identity Pool (if used). Comparison: S3 vs DynamoDB vs RDS Service Advantages Disadvantages Best use cases S3 Low cost, stores large files/datasets, easy backup Not suitable for fast queries, non-transactional Storing logs, history records, datasets RDS Powerful SQL queries, supports complex relationships Higher cost Relational information, reporting systems DynamoDB Fast, highly scalable, event-driven architecture No joins, flat schema Real-time user profiles, session data "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Deploy the recommendation model of AWS Personalize using the project’s real dataset. Perform data migration between DynamoDB and RDS MySQL to support analytics and dashboard development. Assist with data statistics and build the UI dashboard for the frontend. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up and train data with Amazon Personalize 24/11/2025 24/11/2025 3 Migrate data from DynamoDB to RDS MySQL 25/11/2025 25/11/2025 4 Support data analytics and build the UI dashboard for FE 26/11/2025 28/11/2025 Week 12 Achievements: Set up \u0026amp; Train data with Amazon Personalize\nImported datasets via S3. Selected appropriate recipes and trained the model. Monitored training metrics. Data migration to RDS MySQL\nExported data from SQL Server → MySQL → uploaded to RDS. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/1-worklog/1.12-week13/","title":"Worklog Week 13","tags":[],"description":"","content":"Week 13 Objectives: Complete the process of running the website in the local environment and deploying it to the cloud. Discuss and revise the proposal to align with the updated project scope and architecture. Begin configuring the core infrastructure on AWS: VPC, EC2, and RDS for backend and database operations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Set up and successfully run the website locally, proceed to deploy it to the cloud 01/12/2025 01/12/2025 3 Discuss and rewrite the updated proposal 02/12/2025 03/12/2025 4 Configure VPC, EC2, and create an RDS instance for the database 03/12/2025 04/12/2025 Week 13 Achievements: Successfully ran the project locally, purchased a domain, and deployed the website to the cloud. Tested the website and fixed UI/CSS issues. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test API Endpoints","tags":[],"description":"","content":"Step 1: Test from API Gateway Console 1.1. Test GET /users\nGo to API Gateway Console → Select daivietblood-api Select /users → GET Click Test Click Test button Expected response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;[]\u0026#34; } Step 2: Test with cURL Replace YOUR_API_URL with your actual Invoke URL.\n2.1. Create a User (POST /users)\ncurl -X POST https://YOUR_API_URL/prod/users \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; } 2.2. Get All Users (GET /users)\ncurl https://YOUR_API_URL/prod/users Expected response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-12-09T10:00:00.000Z\u0026#34; } ] 2.3. Create Emergency Request (POST /emergency-requests)\ncurl -X POST https://YOUR_API_URL/prod/emergency-requests \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;requester_name\u0026#34;: \u0026#34;Benh vien Cho Ray\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;AB-\u0026#34;, \u0026#34;units_needed\u0026#34;: 5, \u0026#34;hospital\u0026#34;: \u0026#34;Cho Ray Hospital\u0026#34;, \u0026#34;urgency\u0026#34;: \u0026#34;critical\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;message\u0026#34;: \u0026#34;Emergency request created\u0026#34; } 2.4. Get Emergency Requests (GET /emergency-requests)\ncurl https://YOUR_API_URL/prod/emergency-requests Step 3: Test with Postman Open Postman Create new Collection: DaiVietBlood API Add requests: Request Name Method URL Get Users GET {{baseUrl}}/users Create User POST {{baseUrl}}/users Get Emergency Requests GET {{baseUrl}}/emergency-requests Create Emergency Request POST {{baseUrl}}/emergency-requests Set Collection variable: baseUrl: https://YOUR_API_URL/prod Step 4: Verify Lambda Logs Go to CloudWatch Console → Log groups\nFind log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Check recent log streams for:\nSuccessful invocations Any errors or exceptions Database connection logs Common Issues \u0026amp; Solutions Issue Cause Solution 502 Bad Gateway Lambda error Check CloudWatch logs for details Timeout Lambda cannot reach RDS Verify VPC, Subnets, Security Groups CORS error CORS not configured Enable CORS on API Gateway 500 Internal Server Error Database connection failed Check DB credentials in environment variables Step 5: Performance Check Note the response time for each API call First call may be slow (Lambda cold start) Subsequent calls should be faster Expected performance:\nEndpoint Cold Start Warm GET /users ~3-5s ~200-500ms POST /users ~3-5s ~200-500ms GET /emergency-requests ~3-5s ~200-500ms 💡 Tip: Lambda cold start in VPC can be slow. Consider using Provisioned Concurrency for production workloads.\nVerification Checklist GET /users returns empty array or user list POST /users creates new user successfully GET /emergency-requests returns requests list POST /emergency-requests creates new request No CORS errors in browser console CloudWatch logs show successful invocations "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.3-s3-vpc/","title":"VPC &amp; Amazon RDS","tags":[],"description":"","content":"In this section, you will create the network infrastructure (VPC) and database (RDS) for the DaiVietBlood system.\nArchitecture Overview Content Create VPC Create Amazon RDS "},{"uri":"https://bbitmi.github.io/fcj-internship-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report: \u0026ldquo;Data Science on AWS\u0026rdquo; Event Objectives Share essential services for data processing (sentiment analysis, comment classification, etc.). List of Speakers Van Hoang Kha - Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights Below is a summary of the event content focusing on the listed services, presented in a professional report style without icons as requested.\nAWS TRAINING PROGRAM SUMMARY REPORT: AI \u0026amp; MACHINE LEARNING\n1. Overview of Technology Concepts\nTo begin the program, we systematized important foundational concepts in the field of intelligent technology:\nAI (Artificial Intelligence): An overarching concept regarding the creation of intelligent systems. ML (Machine Learning): A subset of AI that allows computers to learn from data. DL (Deep Learning): Uses complex neural networks to model patterns in data. GenAI (Generative AI): Focuses on creating new content and data. 2. AWS as a Service Provider\nThe next section introduced AWS as a comprehensive service provider. AWS offers Managed Services that help businesses apply AI quickly without investing heavily in building infrastructure from scratch.\n3. Details of Introduced AWS Services\nThe training focused deeply on specific tools designed to solve real-world business problems:\nAmazon Comprehend (Natural Language Processing Service - NLP) This service was discussed in the most detail, featuring powerful multi-language text processing capabilities:\nSentiment Analysis: Automatically classifies customer reviews and comments based on positive, negative, or neutral nuances. Text Summarization: Condenses content from long documents. Large-scale Information Processing: Supports bulk email processing and classification. Information Security: Capable of identifying, classifying, and protecting sensitive Personally Identifiable Information (PII) within text. Other Language and Text Processing Services\nAmazon Translate: Automated language translation service. Amazon Textract: A tool for extracting data from scanned documents and papers, including handwriting and complex forms. Amazon Transcribe: A service for converting speech (audio) into written text. Image and Computer Vision Services\nAmazon Rekognition: A Deep Learning-based service specialized for analyzing images and videos (object detection, facial recognition, content moderation). Customer Experience Services\nAmazon Personalize: A solution to enhance customer experience through personalization. This service records and analyzes user behavior, thereby providing product or content recommendations best suited to individual preferences. Technical Infrastructure\nSageMaker Instance: Provides the server environment and tools necessary for developers to self-build, train, and deploy custom machine learning models according to specific needs. Some photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Building geolocation verification for iGaming and sports betting on AWS This blog describes why the iGaming industry requires geolocation verification to meet regulatory requirements and prevent fraud such as VPN spoofing or proxy betting. It introduces several AWS-based methods including Route 53 geolocation routing, Amazon Location Service, and CloudFront geo-blocking, detailing the strengths and limitations of each. You will understand how to choose the right approach to block unauthorized traffic at the edge, enhance security, and reduce infrastructure costs.\nBlog 2 - How to run AI model inference with GPUs on Amazon EKS Auto Mode This blog explains how EKS Auto Mode simplifies GPU operations for AI inference by automatically handling node provisioning, drivers, scaling, and security patching. It walks you through creating a GPU NodePool, deploying an LLM with vLLM, and using Karpenter to scale compute resources on demand. You will see why Auto Mode allows engineering teams to focus on optimizing AI models instead of managing Kubernetes infrastructure.\nBlog 3 - Enforcing organization-wide Amazon S3 bucket-tagging policies This blog explains how organizations can automate and standardize resource tagging for Amazon S3 to improve governance, cost allocation, and security compliance. It illustrates how AWS Config, EventBridge, and Lambda work together to automatically block or allow object uploads based on tagging compliance. You also learn how to deploy a hub-and-spoke architecture using CloudFormation StackSets to enforce tagging policies across all AWS accounts in an organization.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configure CORS &amp; Security","tags":[],"description":"","content":"Understanding CORS CORS (Cross-Origin Resource Sharing) is a security feature that restricts web pages from making requests to a different domain than the one serving the web page.\nWhen your React frontend (hosted on Amplify) calls your API Gateway, the browser checks CORS headers to determine if the request is allowed.\nStep 1: Configure CORS Headers in Lambda Ensure all Lambda functions return proper CORS headers:\nconst corsHeaders = { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, // Or specific domain \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;GET,POST,PUT,DELETE,OPTIONS\u0026#39; }; // In your handler response: return { statusCode: 200, headers: corsHeaders, body: JSON.stringify(data) }; Step 2: Configure API Gateway CORS Method 1: Using Console\nGo to API Gateway Console → Select your API For each resource: Select resource → Actions → Enable CORS Configure allowed origins, methods, headers Click Enable CORS and replace existing CORS headers Method 2: Using OPTIONS Method\nCreate OPTIONS method for each resource Integration type: Mock Add Method Response with status 200 Add Integration Response with headers: Access-Control-Allow-Headers: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#39; Access-Control-Allow-Methods: \u0026#39;GET,POST,OPTIONS\u0026#39; Access-Control-Allow-Origin: \u0026#39;*\u0026#39; Step 3: API Gateway Security Best Practices 3.1. Enable API Key (Optional)\nGo to API Gateway → API Keys → Create API Key Name: daivietblood-api-key Go to Usage Plans → Create Configure throttling and quota Associate API Key with Usage Plan For each method, set API Key Required: true 3.2. Enable Request Validation\nGo to API Gateway → Models → Create Create model for request body: { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-04/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CreateUserModel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;blood_type\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;blood_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;A+\u0026#34;, \u0026#34;A-\u0026#34;, \u0026#34;B+\u0026#34;, \u0026#34;B-\u0026#34;, \u0026#34;AB+\u0026#34;, \u0026#34;AB-\u0026#34;, \u0026#34;O+\u0026#34;, \u0026#34;O-\u0026#34;] }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } Apply model to POST method: Select method → Method Request Request Validator: Validate body Request Body: Add model 3.3. Enable Throttling\nGo to Stages → Select prod Stage Settings → Default Method Throttling Configure: Rate: 100 requests/second Burst: 200 requests Step 4: Lambda Security Best Practices 4.1. Use AWS Secrets Manager for Credentials\nInstead of storing DB credentials in environment variables:\nGo to Secrets Manager → Store a new secret\nSecret type: Other type of secret\nKey/value pairs:\nDB_HOST: daivietblood-db.xxxx.rds.amazonaws.com DB_USER: admin DB_PASSWORD: YourSecurePassword123! DB_NAME: daivietblood Secret name: daivietblood/db-credentials\nUpdate Lambda to retrieve secrets:\nconst { SecretsManagerClient, GetSecretValueCommand } = require(\u0026#39;@aws-sdk/client-secrets-manager\u0026#39;); const client = new SecretsManagerClient({ region: \u0026#39;ap-southeast-1\u0026#39; }); const getDbCredentials = async () =\u0026gt; { const command = new GetSecretValueCommand({ SecretId: \u0026#39;daivietblood/db-credentials\u0026#39; }); const response = await client.send(command); return JSON.parse(response.SecretString); }; Add IAM permission to Lambda role: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:ap-southeast-1:*:secret:daivietblood/*\u0026#34; } 4.2. Input Validation\nAlways validate input in Lambda:\nconst validateUser = (body) =\u0026gt; { const errors = []; if (!body.email || !isValidEmail(body.email)) { errors.push(\u0026#39;Invalid email\u0026#39;); } if (!body.name || body.name.length \u0026lt; 1) { errors.push(\u0026#39;Name is required\u0026#39;); } const validBloodTypes = [\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;]; if (!validBloodTypes.includes(body.blood_type)) { errors.push(\u0026#39;Invalid blood type\u0026#39;); } return errors; }; Step 5: Redeploy API After making changes:\nActions → Deploy API Select prod stage Click Deploy Security Checklist CORS configured correctly Lambda returns proper CORS headers API Key enabled (optional but recommended) Request validation enabled Throttling configured DB credentials stored in Secrets Manager (recommended) Input validation in Lambda functions API redeployed after changes "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.4-s3-onprem/","title":"Lambda &amp; API Gateway","tags":[],"description":"","content":"In this section, you will create AWS Lambda functions and expose them via Amazon API Gateway to build the serverless backend for DaiVietBlood.\nArchitecture Overview API Endpoints Method Endpoint Description GET /users Get all users POST /users Create new user GET /users/{id} Get user by ID GET /donations Get all donations POST /donations Create donation appointment GET /emergency-requests Get emergency requests POST /emergency-requests Create emergency request Content Create Lambda Functions Create API Gateway Test API Endpoints Configure CORS \u0026amp; Security "},{"uri":"https://bbitmi.github.io/fcj-internship-report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #3 – Security Pillar Workshop” 1. Event Objectives This workshop focused on the Security Pillar of the AWS Well-Architected Framework, covering five core domains:\nIdentity \u0026amp; Access Management (IAM) Detection \u0026amp; Continuous Monitoring Infrastructure Protection Data Protection Incident Response The event also introduced AWS Cloud Clubs, a community initiative supporting students in developing cloud skills.\n2. Speakers Lê Vũ Xuân An - AWS Cloud Club Captain HCMUTE\nTrần Đức Anh - AWS Cloud Club Captain SGU\nTrần Đoàn Công Lý - AWS Cloud Club Captain PTIT\nDanh Hoàng Hiếu Nghị - AWS Cloud Club Captain HUFLIT\nHuỳnh Hoàng Long - AWS Community Builders\nĐinh Lê Hoàng Anh - AWS Community Builders\nNguyễn Tuấn Thịnh - Cloud Engineer Trainee\nNguyễn Đỗ Thành Đạt - Cloud Engineer Trainee\nVăn Hoàng Kha - Cloud Security Engineer, AWS Community Builder\nThịnh Lâm - FCJ Member\nViệt Nguyễn - FCJ Member\nMendel Grabski (Long) - Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect\nTrịnh Trương - Platform Engineer tại TymeX, AWS Community Builder\n3. Key Highlights 3.1 AWS Cloud Club Supports students in learning cloud technologies through real-world scenarios. Provides mentorship and career development opportunities. Strengthens professional networking among participants. 3.2 IAM – Core Focus Area Apply Least Privilege. Remove root access keys and avoid wildcard (*). Use AWS SSO for multi-account access control. Implement SCPs, Permission Boundaries, and MFA (TOTP \u0026amp; FIDO2). Use Secrets Manager with a credential rotation workflow. 3.3 Detection \u0026amp; Monitoring Multi-layer observability: management events, data events, VPC Flow Logs. EventBridge used for automation and alert routing. Detection-as-Code implemented via CloudTrail Lake. 3.4 GuardDuty Detects threats using CloudTrail logs, VPC Flow Logs, and DNS queries. Advanced features: malware scanning, EKS audit logs, RDS anomaly detection, Lambda runtime monitoring. Aligns with AWS Security Best Practices and CIS Benchmarks. 3.5 Network Security Differentiates SG (stateful) vs NACL (stateless). Uses Route 53 Resolver for hybrid environments. AWS Network Firewall integrates GuardDuty threat intelligence. 3.6 Data Protection Uses KMS key hierarchy and IAM conditions for encryption control. ACM provides free TLS certificates. Enforces encryption for S3, DynamoDB, and RDS. Uses Secrets Manager rotation. 3.7 Incident Response Best practices: temporary credentials, avoid public S3, private subnet segmentation. Five-step IR workflow: Prepare → Detect \u0026amp; Analyze → Contain → Recover → Lessons Learned. 4. Application to Project The workshop improved the team\u0026rsquo;s AI Chatbot project by:\nStrengthening IAM for backend and admin access. Enhancing logging and monitoring to prevent misconfigurations. Establishing a consistent incident response process. 5. Event Experience The workshop was well-organized, and complex concepts were explained clearly. The Q\u0026amp;A sessions helped reinforce understanding and resolve participant questions.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AWS Cloud Day Vietnam – AI Edition 2025\nDate \u0026amp; Time: 18-09-2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML \u0026amp; Generative AI on AWS\nDate \u0026amp; Time: 15-11-2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Data Science on AWS\nDate \u0026amp; Time: 16-10-2025\nLocation: FPT University\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series#3 – Security Pillar Workshop\nDate \u0026amp; Time: 29-11-2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: Building Agentic AI – Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 05-12-2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.5-policy/","title":"S3, CloudFront &amp; Amplify","tags":[],"description":"","content":"In this section, you will set up Amazon S3 for static assets, CloudFront for content distribution, and AWS Amplify to host the React frontend application.\nArchitecture Overview Part 1: Amazon S3 Setup Step 1: Create S3 Bucket for Assets Go to S3 Console → Create bucket\nGeneral configuration:\nBucket name: daivietblood-assets-{your-account-id} AWS Region: Asia Pacific (Singapore) ap-southeast-1 Object Ownership:\nACLs disabled (recommended) Block Public Access settings:\nBlock all public access: ✅ (We\u0026rsquo;ll use CloudFront) Bucket Versioning:\nEnable (recommended for production) Default encryption:\nServer-side encryption: Enable Encryption type: Amazon S3 managed keys (SSE-S3) Click Create bucket\nStep 2: Create Bucket Policy for CloudFront After creating CloudFront distribution (Part 2), update bucket policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::daivietblood-assets-{your-account-id}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::{account-id}:distribution/{distribution-id}\u0026#34; } } } ] } Step 3: Upload Sample Assets Create folder structure:\n/images /blood-types /icons /banners /documents Upload sample images for the application\nPart 2: CloudFront Setup Step 1: Create CloudFront Distribution Go to CloudFront Console → Create distribution\nOrigin settings:\nOrigin domain: Select your S3 bucket Origin path: Leave empty Name: daivietblood-s3-origin Origin access: Origin access control settings (recommended) Create new OAC: Click Create control setting Name: daivietblood-oac Signing behavior: Sign requests Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD Cache policy: CachingOptimized Settings:\nPrice class: Use only North America and Europe (or All edge locations) Default root object: index.html Click Create distribution\nImportant: Copy the bucket policy provided and update your S3 bucket policy\nStep 2: Get CloudFront Domain After distribution is deployed (takes 5-10 minutes):\nCopy the Distribution domain name:\nhttps://d1234567890.cloudfront.net Test accessing an asset:\nhttps://d1234567890.cloudfront.net/images/logo.png Part 3: AWS Amplify Setup Step 1: Prepare React Application Create React app (if not exists): npx create-react-app daivietblood-frontend cd daivietblood-frontend Install dependencies: npm install axios react-router-dom Create .env file: REACT_APP_API_URL=https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod REACT_APP_ASSETS_URL=https://d1234567890.cloudfront.net Sample API service (src/services/api.js): import axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL; export const getUsers = async () =\u0026gt; { const response = await axios.get(`${API_URL}/users`); return response.data; }; export const createUser = async (userData) =\u0026gt; { const response = await axios.post(`${API_URL}/users`, userData); return response.data; }; export const getEmergencyRequests = async () =\u0026gt; { const response = await axios.get(`${API_URL}/emergency-requests`); return response.data; }; export const createEmergencyRequest = async (requestData) =\u0026gt; { const response = await axios.post(`${API_URL}/emergency-requests`, requestData); return response.data; }; Push to GitHub repository Step 2: Deploy with Amplify Go to AWS Amplify Console → Create new app\nChoose source:\nGitHub → Continue Authorize AWS Amplify to access your GitHub Add repository branch:\nRepository: Select your repository Branch: main Configure build settings:\nApp name: daivietblood-frontend Build and test settings: Auto-detected for React Build settings (amplify.yml):\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: build files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* Environment variables:\nAdd REACT_APP_API_URL and REACT_APP_ASSETS_URL Click Save and deploy\nStep 3: Configure Custom Domain (Optional) Go to App settings → Domain management Click Add domain Enter your domain name Configure DNS records as instructed Part 4: Verify Deployment Access Amplify URL:\nhttps://main.d1234567890.amplifyapp.com Test functionality:\nHomepage loads correctly API calls work (check Network tab) Images load from CloudFront No CORS errors Verification Checklist S3 bucket created with proper settings CloudFront distribution deployed S3 bucket policy updated for CloudFront access Assets accessible via CloudFront URL React app deployed to Amplify Environment variables configured Frontend can call API Gateway Images load from CloudFront "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building Serverless System on AWS - DaiVietBlood Overview This workshop guides you through building a Serverless Blood Donation \u0026amp; Emergency System (DaiVietBlood) on AWS. You will learn how to set up and configure the core AWS services used in the project architecture.\nAWS Services Used Service Purpose Amazon VPC Create virtual private network with Public/Private Subnets NAT Gateway Allow resources in Private Subnet to access Internet Amazon RDS MySQL database for the application AWS Lambda Serverless business logic processing Amazon API Gateway Manage and expose REST APIs Amazon S3 Store static assets (images, files) Amazon CloudFront CDN for global content distribution AWS Amplify Host Frontend application (React) AWS CodePipeline CI/CD automation Amazon CloudWatch Monitoring and logging What You Will Learn Design and deploy Serverless-First architecture on AWS Configure VPC with Public/Private Subnets for security Create RDS MySQL in Private Subnet Build Lambda Functions and connect with API Gateway Store and distribute content with S3 and CloudFront Deploy React application with AWS Amplify Set up automated CI/CD Pipeline Monitor application with CloudWatch Prerequisites AWS Account with Administrator access Basic knowledge of AWS services Familiarity with Node.js and React AWS CLI installed and configured Estimated Cost This workshop uses resources within AWS Free Tier when possible. Estimated cost is approximately ~$15-20 if completed within 1-2 days and resources are cleaned up immediately after.\nContent Workshop Overview Preparation VPC \u0026amp; Amazon RDS Lambda \u0026amp; API Gateway S3, CloudFront \u0026amp; Amplify CI/CD, CloudWatch \u0026amp; Cleanup "},{"uri":"https://bbitmi.github.io/fcj-internship-report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Event Report: “Building Agentic AI – Context Optimization with Amazon Bedrock” Event Date: 05/12/2025\nLocation: 26th Floor, Bitexco Financial Tower\n1. Event Objectives The event focused on how to build Agentic AI (multi-task, multi-agent AI systems) through:\nContext Optimization when designing AI agents. Applying Amazon Bedrock for multi-agent workflows. Enterprise automation solutions from Diaflow and CloudThinker. Sharing practical experience in agent deployment, tool design, and workflow construction. 2. Speakers Nguyen Gia Hung — Head of Solutions Architect, AWS\nKien Nguyen — Solutions Architect, AWS\nViet Pham — Founder \u0026amp; CEO, Diaflow\nKha Van — Community Leader, AWS\nThang Ton — Co-Founder\nHenry Bui — Head of Engineering\n3. Key Content Highlights Session 1 – Agent Core (AWS) React Agent – executing tasks sequentially. Designing Tools in an Agent System. Optimizing evaluation and monitoring from day one (Eval \u0026amp; Observability). Multi-agent \u0026amp; Multi-session: each task/department operates its own agent flow, coordinated via a supervisor agent. Session 2 – Diaflow Business Problems\nAccording to Diaflow’s presentation:\n90% of businesses waste ~20 hours/week on repetitive tasks. The main concern: AI may leak sensitive data during processing. Diaflow’s Solution\nDiaflow addresses this with an AI model running entirely on the enterprise’s own infrastructure:\nEnterprises retain full control over data flows through MCP.\nSupports integration with multiple Google and AWS services.\nInternal automation applications:\nDatabase Automation Document processing Knowledge base Building internal tools/apps Benefits\nProcess Efficiency: Automates up to 80% of repetitive tasks. Cost-saving: Can be deployed in minutes, replacing many disconnected tools. Usability: Simple interface, enabling anyone to build automated workflows. Integration with AWS Bedrock\nCombines multiple models (multi-model). Optimizes data security at an enterprise level. Enables cost optimization at scale. Session 3 – CloudThinker Enterprise Cloud Challenges\nInfrastructure costs can easily skyrocket (Cost Explosion). Cloud systems are complex and hard to monitor. It is difficult to react quickly when incidents occur. CloudThinker’s Solution\nA comprehensive “all-in-one” platform including:\nCode Review Automation Incident Response Agent Operations \u0026amp; Optimization Security \u0026amp; Compliance Automation Built on a multi-agent framework, this solution helps:\nAutomate operations Reduce costs Continuously optimize systems Improve incident response capability Context Optimization\nHighlighted as the key factor determining agent system performance:\nUser input for a chatbot is only about ~10% of the input required for an agent system. Context overload leads to degraded performance. Context optimization techniques:\nPrompt Caching Context Compaction Tool Consolidation (grouping tools into MCI) Parallel Tool Calling Cross-Region Inference Multi-agent Architecture\nA Supervisor Agent orchestrates the system. Specialist Agents handle specialized tasks. Delegation architecture reduces latency and distributes workloads. Challenges \u0026amp; Implementation Experience\nThe hardest part is the Agent Starter – building the first agent and the tool-calling model. Tools must be properly designed from the outset. A robust Eval \u0026amp; Observability system is mandatory to avoid “AI out of control.” 4. Key Takeaways The knowledge from this workshop is highly applicable to ongoing AI/Cloud projects:\nBuilding multi-agent systems so AI chatbots can analyze, plan, and call complex APIs. Applying context optimization to reduce costs and increase inference performance. Using Diaflow for internal workflow automation, especially for data extraction and RAG. Adopting CloudThinker’s mindset for monitoring, operations, and incident response in AI/ML systems. 5. Overall Evaluation The event was professionally organized, with cutting-edge content aligned to real enterprise needs. Practical examples from Diaflow and CloudThinker helped participants clearly visualize how to implement agents in real-world scenarios. AWS’s sessions provided a solid technical foundation for agentic AI, emphasizing data flows and tool design. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/5-workshop/5.6-cleanup/","title":"CI/CD, CloudWatch &amp; Cleanup","tags":[],"description":"","content":"In this final section, you will set up CI/CD Pipeline, configure CloudWatch monitoring, and clean up all resources after completing the workshop.\nPart 1: CI/CD Pipeline with CodePipeline Step 1: Create CodeBuild Project Go to CodeBuild Console → Create build project\nProject configuration:\nProject name: daivietblood-backend-build Description: Build project for Lambda functions Source:\nSource provider: GitHub Repository: Select your repository Branch: main Environment:\nEnvironment image: Managed image Operating system: Amazon Linux 2 Runtime: Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Service role: New service role Buildspec:\nBuild specifications: Use a buildspec file Create buildspec.yml file in your repository: version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - echo Installing dependencies... - cd backend \u0026amp;\u0026amp; npm ci pre_build: commands: - echo Running tests... - npm test || true build: commands: - echo Building Lambda packages... - mkdir -p dist - zip -r dist/get-users.zip functions/get-users/ - zip -r dist/create-user.zip functions/create-user/ - zip -r dist/emergency-requests.zip functions/emergency-requests/ post_build: commands: - echo Updating Lambda functions... - aws lambda update-function-code --function-name daivietblood-get-users --zip-file fileb://dist/get-users.zip - aws lambda update-function-code --function-name daivietblood-create-user --zip-file fileb://dist/create-user.zip - aws lambda update-function-code --function-name daivietblood-emergency-requests --zip-file fileb://dist/emergency-requests.zip artifacts: files: - dist/**/* Click Create build project Step 2: Create CodePipeline Go to CodePipeline Console → Create pipeline\nPipeline settings:\nPipeline name: daivietblood-pipeline Service role: New service role Source stage:\nSource provider: GitHub (Version 2) Connection: Create new connection or select existing Repository name: Select your repository Branch name: main Output artifact format: CodePipeline default Build stage:\nBuild provider: AWS CodeBuild Project name: daivietblood-backend-build Deploy stage:\nSkip deploy stage (Lambda is updated in build stage) Click Create pipeline\nStep 3: Add IAM Permissions for CodeBuild Go to IAM Console → Roles Find role codebuild-daivietblood-backend-build-service-role Add inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:*:function:daivietblood-*\u0026#34; } ] } Part 2: CloudWatch Monitoring Step 1: Create CloudWatch Dashboard Go to CloudWatch Console → Dashboards → Create dashboard\nDashboard name: DaiVietBlood-Monitoring\nAdd widgets:\nWidget 1: Lambda Invocations\nWidget type: Line Metrics: Lambda → By Function Name → Invocations Select all daivietblood functions Widget 2: Lambda Errors\nWidget type: Number Metrics: Lambda → By Function Name → Errors Statistic: Sum Widget 3: Lambda Duration\nWidget type: Line Metrics: Lambda → By Function Name → Duration Statistic: Average Widget 4: API Gateway Requests\nWidget type: Line Metrics: ApiGateway → By Api Name → Count Widget 5: RDS Connections\nWidget type: Line Metrics: RDS → Per-Database Metrics → DatabaseConnections Step 2: Create CloudWatch Alarms Alarm 1: Lambda Errors\nGo to CloudWatch → Alarms → Create alarm Select metric: Lambda → By Function Name → Errors Conditions: Threshold type: Static Whenever Errors is: Greater than 5 Period: 5 minutes Notification: Create new SNS topic: daivietblood-alerts Email: your-email@example.com Alarm name: DaiVietBlood-Lambda-Errors Alarm 2: RDS CPU High\nCreate alarm Select metric: RDS → Per-Database Metrics → CPUUtilization Conditions: Threshold: Greater than 80% Period: 5 minutes Notification: Use existing SNS topic Alarm name: DaiVietBlood-RDS-CPU-High Alarm 3: API Gateway 5XX Errors\nCreate alarm Select metric: ApiGateway → By Api Name → 5XXError Conditions: Threshold: Greater than 10 Period: 5 minutes Alarm name: DaiVietBlood-API-5XX-Errors Step 3: Configure Log Insights Go to CloudWatch → Logs → Logs Insights\nSelect log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Sample query - Find errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 50 Sample query - Duration statistics: fields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) by bin(1h) Part 3: Resource Cleanup ⚠️ Important: Follow these steps to avoid unexpected charges.\nCleanup Order (Important!) Clean up in the following order to avoid dependency errors:\nStep 1: Delete Amplify App Go to Amplify Console Select daivietblood-frontend Actions → Delete app Confirm deletion Step 2: Delete CloudFront Distribution Go to CloudFront Console Select distribution → Disable Wait for status to change to \u0026ldquo;Deployed\u0026rdquo; Select distribution → Delete Step 3: Delete S3 Buckets Go to S3 Console Select bucket daivietblood-assets-* Empty bucket first Then Delete bucket Step 4: Delete API Gateway Go to API Gateway Console Select daivietblood-api Actions → Delete Step 5: Delete Lambda Functions Go to Lambda Console Delete each function: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests Delete Lambda Layer: mysql2-layer Step 6: Delete RDS Instance Go to RDS Console → Databases Select daivietblood-db Actions → Delete Uncheck \u0026ldquo;Create final snapshot\u0026rdquo; Check \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; Type delete me to confirm Step 7: Delete VPC Resources Go to VPC Console\nDelete NAT Gateway:\nNAT Gateways → Select NAT Gateway → Delete Wait for status \u0026ldquo;Deleted\u0026rdquo; Release Elastic IP:\nElastic IPs → Select EIP → Release Delete VPC Endpoints (if any):\nEndpoints → Select endpoints → Delete Delete Security Groups (except default):\nSecurity Groups → Delete daivietblood-lambda-sg, daivietblood-rds-sg Delete DB Subnet Group:\nRDS Console → Subnet groups → Delete daivietblood-db-subnet-group Delete VPC:\nYour VPCs → Select daivietblood-vpc → Delete VPC This will delete subnets, route tables, internet gateway Step 8: Delete CI/CD Resources CodePipeline Console → Delete daivietblood-pipeline CodeBuild Console → Delete daivietblood-backend-build Step 9: Delete CloudWatch Resources CloudWatch → Dashboards → Delete DaiVietBlood-Monitoring CloudWatch → Alarms → Delete all related alarms CloudWatch → Log groups → Delete log groups /aws/lambda/daivietblood-* Step 10: Delete IAM Resources IAM Console → Roles Delete roles: daivietblood-lambda-role codebuild-daivietblood-* codepipeline-daivietblood-* Cleanup Checklist Amplify app deleted CloudFront distribution deleted S3 buckets emptied and deleted API Gateway deleted Lambda functions and layers deleted RDS instance deleted NAT Gateway deleted Elastic IP released VPC and all components deleted CodePipeline and CodeBuild deleted CloudWatch dashboards, alarms, log groups deleted IAM roles deleted Verify No Remaining Charges Go to AWS Cost Explorer Verify no resources are running Go to Billing Console → Bills to confirm 💡 Tip: Set up Budget Alert in AWS Budgets to receive notifications when costs exceed threshold.\nWorkshop Conclusion Congratulations! 🎉 You have completed the workshop on building a Serverless system on AWS.\nWhat You Learned: ✅ Design and deploy VPC with Public/Private Subnets ✅ Create RDS MySQL in a secure environment ✅ Build Lambda functions and expose via API Gateway ✅ Configure S3 and CloudFront for static assets ✅ Deploy React app with AWS Amplify ✅ Set up automated CI/CD Pipeline ✅ Monitor application with CloudWatch Next Steps: Learn more about AWS Well-Architected Framework Explore advanced features like X-Ray tracing Experiment with Aurora Serverless for database Implement authentication with Amazon Cognito "},{"uri":"https://bbitmi.github.io/fcj-internship-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey (FCJ) - AWS Study Group from September 8, 2025 to December 9, 2025 I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in building a website supporting blood donation, through which I made friends, learned about Cloud Service and improve soft-skill for myself.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improve discipline by organizing tasks more effectively and scientifically. Learn to communicate better in a work environment and connect with more colleagues and peers. "},{"uri":"https://bbitmi.github.io/fcj-internship-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"General Evaluation 1. Working Environment\nThe working environment is very friendly and open. The mentors and FCJ members always support me promptly and wholeheartedly. Everyone is approachable and kind, helping point out mistakes and guiding me to improve.\n2. Support from Mentor / Team Admin\nThe guidance and teamwork throughout the program were excellent. The team was always ready to discuss technical challenges, consider my proposals, and guide me through complex AWS concepts. What I appreciate most is that the mentor encouraged me to research and propose solutions independently before offering support.\n3. Alignment Between Work and Academic Background\nJoining FCJ allowed me to learn new Cloud skills and services, which gave me broader insights into how various industries operate. These skills also strengthen my soft skills and will be valuable for my future work.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as project management, planning and task allocation, self-discipline, and improving communication in a professional environment.\n5. Culture \u0026amp; Team Spirit\nThe company culture is very positive. Even though it was my first time meeting the FCJ members, everyone was friendly and open, making me feel comfortable rather than out of place. People are very serious during work but fun and kind during breaks.\nAdditional Questions What are you most satisfied with during the internship?\nA flexible working schedule, a positive environment, and friendly colleagues who offered enthusiastic support whenever I needed help.\nWhat do you think the company should improve for future interns?\nHaving more team-building events or activities so members have more opportunities to connect with each other.\nWould you recommend your friends to intern here? Why?\nAbsolutely yes. FCJ offers a dynamic and friendly environment, and it is a great opportunity to develop both technical and personal skills.\n"},{"uri":"https://bbitmi.github.io/fcj-internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://bbitmi.github.io/fcj-internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]